# === ./testthetastar.py ===
from math import radians
import time
from typing import List
import heapq
import math
from typing import List, Tuple

from matplotlib import pyplot as plt
import numpy as np

from src.path_finding.LidarPlot import LidarPlot
from src.path_finding.Position import Position
import math
import heapq
from typing import List

RESOLUTION = 50
PAD_ROUNDS = 4
DIRECTIONS = [
    (0, 0),
    (0, 1),
    (0, -1),
    (1, 0),
    (1, 1),
    (1, -1),
    (-1, 0),
    (-1, 1),
    (-1, -1)
]


def test_plt(padded_obstacles,obstacles,  goal_cart, path):
    plt.scatter(
        [y[0] for y in padded_obstacles], [y[1] for y in padded_obstacles], color="r"
    )
    plt.scatter(
        [y[0] for y in obstacles], [y[1] for y in obstacles], color="black"
    )
    plt.scatter([y[0] for y in path], [y[1] for y in path], color="b")
    plt.scatter([goal_cart[0]], [goal_cart[1]], color="g")
    plt.show()


def polar_to_cartesian(pos: Position) -> tuple:
    if pos.distance == 0:
        return (0.0, 0.0)
    angle_rad = pos.angle
    x = pos.distance * math.cos(angle_rad)
    y = pos.distance * math.sin(angle_rad)
    return (x, y)


def cartesian_to_polar(x: float, y: float) -> Position:
    distance = math.hypot(x, y)
    if distance == 0:
        return Position(distance=0.0, angle=0.0)
    angle_rad = math.atan2(y, x)
    angle_deg = angle_rad
    return Position(angle=angle_deg, distance=distance*RESOLUTION)


def snap(pos: Position):
    cart = polar_to_cartesian(pos)
    return (int(cart[0] / RESOLUTION),int(cart[1] / RESOLUTION))


def remove_duplicates(coords):
    seen = set()
    unique_coords = []
    
    for coord in coords:
        if coord not in seen:
            seen.add(coord)
            unique_coords.append(coord)
    
    return unique_coords


def pad_obstacles(obstacles):
    padded = []
    for x, y in obstacles:
        pads = [(x_p+x, y_p+y) for x_p, y_p in DIRECTIONS]
        padded.extend(pads)
    return remove_duplicates(padded)        


def pre_process_obstacles(obstacles: List[Position]):
    obstacles = remove_duplicates([snap(obs) for obs in obstacles])
    for _ in range(PAD_ROUNDS):
        obstacles = pad_obstacles(obstacles)
    return obstacles


def a_star(obstacles: List[Position], goal: Position):
    padded_obstacles = pre_process_obstacles(obstacles)
    goal = snap(goal)


    start = (0, 0)
    obstacles_set = set(padded_obstacles)
    
    open_set = []  # Priority queue
    heapq.heappush(open_set, (0, start))
    came_from = {}
    g_score = {start: 0}
    f_score = {start: np.linalg.norm(np.array(start) - np.array(goal), ord=1)}
    
    while open_set:
        _, current = heapq.heappop(open_set)
        
        if current == goal:
            path = []
            while current in came_from:
                path.append(current)
                current = came_from[current]
            path.append(start)
            path.reverse()
            # test_plt(padded_obstacles, [snap(obs) for obs in obstacles],  goal, path)
            return [cartesian_to_polar(step[0], step[1]) for step in path], (padded_obstacles, [polar_to_cartesian(obs) for obs in obstacles], goal, path)
        
        for dx, dy in DIRECTIONS:
            neighbor = (current[0] + dx, current[1] + dy)
            if neighbor in obstacles_set:
                continue
            
            tentative_g_score = g_score[current] + 1
            if neighbor not in g_score or tentative_g_score < g_score[neighbor]:
                came_from[neighbor] = current
                g_score[neighbor] = tentative_g_score
                f_score[neighbor] = tentative_g_score + np.linalg.norm(np.array(neighbor) - np.array(goal), ord=1)
                heapq.heappush(open_set, (f_score[neighbor], neighbor))
    
    return []  # No path found


import numpy as np
import heapq
from math import hypot

positions = []
for angle in range(200, 360):
    positions.append(Position(radians(angle), 1000))

for angle in range(0, 100):
    positions.append(Position(radians(angle), 1000))


for distance in range(1000, 1500):
    positions.append(Position(radians(100), distance))


for distance in range(1000, 1600):
    positions.append(Position(radians(200), distance))

    

for distance in range(1700, 3000):
    positions.append(Position(radians(60), distance))


for distance in range(1700, 3000):
    positions.append(Position(radians(270), distance))

    
for distance in range(1000, 3000):
    positions.append(Position(radians(30), distance))


    
for angle in range(160, 200):
    positions.append(Position(radians(angle), 1000))

goal = Position(0, 1500)
plot = LidarPlot(range_mm=4000, debug_algo_path=True)

while True:
    path, (padded_obstacles_debug, original_obs_debug, goal_debug, path_debug) = a_star( positions, goal)
    
    plot.plot_path(path)
    plot.plot_path_debug(path_debug)
    plot.plot_destination_debug(goal_debug)
    plot.plot_obstacles_debug(padded_obstacles_debug,original_obs_debug)

    plot.plot_obstacles(positions)
    plot.plot_destination(goal)
    plot.update_plot()
    time.sleep(1)


# === ./Jetson_Motoron_GPIO_test.py ===
import Jetson.GPIO as GPIO
import time
import motoron
# Pin Definitions
output_pin = 7  # Jetson Board Pin 7
mc = motoron.MotoronI2C(bus=7)
mc.reinitialize()  # Bytes: 0x96 0x74
mc.disable_crc()   # Bytes: 0x8B 0x04 0x7B 0x43
# Clear the reset flag, which is set after the controller reinitializes and
# counts as an error.
mc.clear_reset_flag()  # Bytes: 0xA9 0x00 0x04
mc.disable_command_timeout()
mc.set_max_acceleration(1, 200)
mc.set_max_deceleration(1, 10)
mc.set_max_acceleration(2, 200)
mc.set_max_deceleration(2, 10)

def main():
    # Pin Setup:
    GPIO.setmode(GPIO.BOARD)  # Jetson board numbering scheme
    # set pin as an output pin with optional initial state of HIGH
    GPIO.setup(output_pin, GPIO.OUT, initial=GPIO.HIGH)

    print("Starting demo now! Press CTRL+C to exit")
    curr_value = GPIO.HIGH
    GPIO.output(output_pin, curr_value)
    print("Outputting {} to pin {}".format(curr_value, output_pin))
    
    try:
        while True:
            time.sleep(2)
            mc.set_speed(1, 300)
            mc.set_speed(2, 300)
            time.sleep(3)
            mc.set_speed(1,0)
            mc.set_speed(2,0)
            time.sleep(1)
            mc.set_speed(1, -300)
            mc.set_speed(2, -300)
            time.sleep(3)
            mc.set_speed(1,0)
            mc.set_speed(2,0)
            print("Outputting {} to pin {}".format(curr_value, output_pin))
            time.sleep(1)
    finally:
        mc.set_speed(1, 0)
        mc.set_speed(2, 0)
        time.sleep(1)
        GPIO.output(output_pin, 0)
        GPIO.cleanup()

if __name__ == '__main__':
    main()


# === ./new_main.py ===
import py_trees
from py_trees.trees import BehaviourTree
from src.behaviors.game_mode import make_game_sub_tree
from src.behaviors.diagnostic_mode import make_diagnostic_sub_tree
from src.behaviors.lidarchase import make_lidar_chase_sub_tree
from src.multithreading import graceful_thread_exit
from src.behaviors.MaxineBehavior import ROBOT_BLACKBOARD, ROBOT_KEY
from src.behaviors.utils import make_mode_sub_tree
from src.behaviors.idle_mode import make_idle_mode_sub_tree
from src.behaviors.keyboard_mode import make_keyboard_mode_sub_tree
from src.behaviors.chase_mode import make_chase_mode_sub_tree
from src.behaviors.discovery_mode import make_discovery_sub_tree
from src.types.RobotModes import RobotMode
from src.robot.Robot import Robot
from src.robot.RobotFactory import RobotFactory

CONFIG_FILE = "src/configs/config_maxine.yaml"


def build_robot() -> Robot:
    robot_factory = RobotFactory(CONFIG_FILE)
    return robot_factory.build_robot()


def build_robot_behavior_tree() -> BehaviourTree:
    """
    Builds the behavior tree for the robot.
    The tree is composed of sub trees for each mode.
    """
    # mapping of mode to sub tree
    mode_to_sub_tree = {
        RobotMode.IDLE: make_idle_mode_sub_tree(),
        RobotMode.KEYBOARD_CONTROL: make_keyboard_mode_sub_tree(),
        RobotMode.CHASE: make_chase_mode_sub_tree(),
        RobotMode.LIDARCHASE: make_lidar_chase_sub_tree(),
        RobotMode.DISCOVERY: make_discovery_sub_tree(),
        RobotMode.DIAGNOSTIC: make_diagnostic_sub_tree(),
        RobotMode.PLAYGAME: make_game_sub_tree()
    }

    # wrap each sub tree with a condition on current robot mode
    sub_trees = []
    for robot_mode, sub_tree in mode_to_sub_tree.items():
        wrapped_sub_tree = make_mode_sub_tree(robot_mode, sub_tree)
        sub_trees.append(wrapped_sub_tree)

    # root node is a selector (will run first passing sub tree)
    # ie will run sub tree of the curent mode
    robot_root = py_trees.composites.Selector("root", memory=True, children=sub_trees)

    # add blackbaord to root (so that stop condition can acess robot)
    robot_root.blackboard = robot_root.attach_blackboard_client(name=ROBOT_BLACKBOARD)
    robot_root.blackboard.register_key(ROBOT_KEY, access=py_trees.common.Access.WRITE)
    robot_root.setup_with_descendants()
    return BehaviourTree(robot_root)


def initialise_black_board(robot):
    """
    Initilaises the black board which gives all nodes in the tree acess to the robot class

    arguments:
        - robot: the robot
    """
    configuration = py_trees.blackboard.Client(name=ROBOT_BLACKBOARD)
    configuration.register_key(ROBOT_KEY, access=py_trees.common.Access.WRITE)
    configuration.set(ROBOT_KEY, robot)


@graceful_thread_exit
def run():
    robot = build_robot()

    # build tree
    initialise_black_board(robot)
    behavior_tree = build_robot_behavior_tree()

    # exits the program if the robot is in exit mode
    def stop_condition(tree: BehaviourTree):
        robot = tree.root.blackboard.get(ROBOT_KEY)
        if robot.current_mode == RobotMode.EXIT:
            exit()

    try:
        # continuously tick the behavior tree, checking robot is not in exit mode each time
        behavior_tree.tick_tock(50, post_tick_handler=stop_condition)
    finally:
        robot.shutdown()
        raise

if __name__ == "__main__":
    run()


# === ./pyParallax-master/demo/parallax/parallax.py ===
# -*- coding: utf-8 -*-

#    Copyright (C) , 2012 Åke Forslund (ake.forslund@gmail.com)
#
#    Permission to use, copy, modify, and/or distribute this software for any
#    purpose with or without fee is hereby granted, provided that the above
#    copyright notice and this permission notice appear in all copies.
#
#    THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
#    WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
#    MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
#    ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#    WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
#    ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
#    OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
#

'''A simple parallax rendering module'''

import pygame

class _subsurface(object):
    '''Container class for subsurface'''
    def __init__(self, surface, factor):
        self.scroll = 0
        self.factor = factor
        self.surface = surface

class ParallaxSurface(object):
    '''Class handling parallax scrolling of a series of surfaces'''
    def __init__(self, size, colorkey_flags=0):
        self.colorkey_flags = colorkey_flags
        self.scroller = 0
        self.levels = []
        self.levels_id = {}
        self.size = size
        self.orientation = 'horizontal'

    def chg_size(self, size):
        ''' Changes the parallax surface's size. '''
        self.size = size

    def update(self, image_path, scroll_factor, size=(0, 0)):
        ''' Updates the parallax level identified by image_path and
            redefines the layer's scroll_factor and the size of the entire
            parallax surface. '''
        self.rem(image_path)
        self.add(image_path, scroll_factor, size)

    def rem(self, image_path):
        ''' Removes the parallax level created from the image_path.
            If no matching level is found nothing is removed. '''
        if image_path in self.levels_id:
            elem_id = self.levels_id[image_path]
            del self.levels[elem_id]
            del self.levels_id[image_path]

    def add(self, image_path, scroll_factor, size=None):
        ''' Adds a parallax level, first added level is the
            deepest level, i.e. furthest back into the \"screen\".

            image_path is the path to the image to be used
            scroll_factor is the slowdown factor for this parallax level. '''
        try:
            image = (pygame.image.load(image_path))
        except:
            message = "couldn't open image:" + image_path
            raise SystemExit(message)
        if ".png" in image_path:
            image = image.convert_alpha()
        else:
            image = image.convert()
        if len(self.levels) > 0:
            image.set_colorkey((0xff, 0x00, 0xea), self.colorkey_flags)
        if size is not None:
            image = pygame.transform.scale(image, size) # Change the image size
            self.chg_size(size) # Update the size
        # Track the current image by it id
        self.levels_id[image_path] = len(self.levels)
        self.levels.append(_subsurface(image, scroll_factor))

    def add_colorkeyed_surface(self, surface, scroll_factor,
                               color_key=(0xff, 0x00, 0xea)):
        ''' Adds a colorkeyed surface created elsewhere. '''
        surface = surface.convert()
        if len(self.levels) > 0:
            surface.set_colorkey(color_key, self.colorkey_flags)
        self.levels.append(_subsurface(surface, scroll_factor))

    def add_surface(self, surface, scroll_factor):
        ''' Adds a surface created elsewhere. '''
        surface = surface.convert_alpha()
        if len(self.levels) > 0:
            surface.set_colorkey((0xff, 0x00, 0xea), self.colorkey_flags)
        self.levels.append(_subsurface(surface, scroll_factor))

    def draw(self, surface):
        ''' This draws all parallax levels to the surface
            provided as argument. '''
        s_width = self.size[0]
        s_height = self.size[1]
        for lvl in self.levels:
            if self.orientation == 'vertical':
                surface.blit(lvl.surface, (0, 0),
                             (0, -lvl.scroll, s_width, s_height))
                surface.blit(lvl.surface,
                             (0, lvl.scroll - lvl.surface.get_height()))
            else:
                surface.blit(lvl.surface, (0, 0),
                             (lvl.scroll, 0, s_width, s_height))
                surface.blit(lvl.surface,
                             (lvl.surface.get_width() - lvl.scroll, 0),
                             (0, 0, lvl.scroll, s_height))

    def scroll(self, offset, orientation=None):
        '''scroll moves each surface _offset_ pixels / assigned factor'''
        if orientation is not None:
            self.orientation = orientation

        self.scroller = (self.scroller + offset)
        for lvl in self.levels:
            if self.orientation == 'vertical':
                lvl.scroll = (self.scroller / lvl.factor) \
                             % lvl.surface.get_height()
            else:
                lvl.scroll = (self.scroller / lvl.factor) \
                             % lvl.surface.get_width()

class VerticalParallaxSurface(ParallaxSurface):
    ''' Class implementing vertical scrolling parallax surface. '''
    def __init__(self, size, colorkey_flags=0):
        ParallaxSurface.__init__(self, size, colorkey_flags)
        self.orientation = 'vertical'


# === ./pyParallax-master/demo/parallax/__init__.py ===
from parallax import *


# === ./pyParallax-master/demo/demo.py ===
# -*- coding: utf-8 -*-

#    Copyright (C) , 2012 Åke Forslund (ake.forslund@gmail.com)
#
#    Permission to use, copy, modify, and/or distribute this software for any
#    purpose with or without fee is hereby granted, provided that the above
#    copyright notice and this permission notice appear in all copies.
#
#    THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
#    WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
#    MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
#    ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#    WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
#    ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
#    OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
#


import os, sys
import pygame
from pygame.locals import *
import time

sys.path.append ("../")
from parallax import parallax

pygame.init()
screen = pygame.display.set_mode((1024, 768), pygame.DOUBLEBUF)
pygame.display.set_caption('Parallax-test')
pygame.mouse.set_visible(0)

orientation = 'vertical'

bg = parallax.ParallaxSurface((1024, 768), pygame.RLEACCEL)
bg.add('bkgd_0.png', 6)
bg.add('bkgd_1.png', 5)
bg.add('bkgd_2.png', 4)
bg.add('bkgd_4.png', 3)
bg.add('bkgd_6.png', 2)
bg.add('bkgd_7.png', 1)

run = True
speed = 0
t_ref = 0
while run:
    for event in pygame.event.get():
        if event.type == QUIT:
            run = False
        if event.type == KEYDOWN and event.key == K_RIGHT:
            speed += 4
        if event.type == KEYUP and event.key == K_RIGHT:
            speed -= 4
        if event.type == KEYDOWN and event.key == K_LEFT:
            speed -= 4
        if event.type == KEYUP and event.key == K_LEFT:
            speed += 4
        if event.type == KEYDOWN and event.key == K_UP:
            orientation = 'vertical'
        if event.type == KEYDOWN and event.key == K_DOWN:
            orientation = 'horizontal'
        
    bg.scroll(speed, orientation)
    t = pygame.time.get_ticks()
    if (t - t_ref) > 60:
        bg.draw(screen)
        pygame.display.flip()
        time.sleep(0.01)



# === ./pyParallax-master/parallax/parallax.py ===
# -*- coding: utf-8 -*-

#    Copyright (C) , 2012 Åke Forslund (ake.forslund@gmail.com)
#
#    Permission to use, copy, modify, and/or distribute this software for any
#    purpose with or without fee is hereby granted, provided that the above
#    copyright notice and this permission notice appear in all copies.
#
#    THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
#    WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
#    MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
#    ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#    WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
#    ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
#    OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
#

'''A simple parallax rendering module'''

import pygame

class _subsurface(object):
    '''Container class for subsurface'''
    def __init__(self, surface, factor):
        self.scroll = 0
        self.factor = factor
        self.surface = surface

class ParallaxSurface(object):
    '''Class handling parallax scrolling of a series of surfaces'''
    def __init__(self, size, colorkey_flags=0):
        self.colorkey_flags = colorkey_flags
        self.scroller = 0
        self.levels = []
        self.levels_id = {}
        self.size = size
        self.orientation = 'horizontal'

    def chg_size(self, size):
        ''' Changes the parallax surface's size. '''
        self.size = size

    def update(self, image_path, scroll_factor, size=(0, 0)):
        ''' Updates the parallax level identified by image_path and
            redefines the layer's scroll_factor and the size of the entire
            parallax surface. '''
        self.rem(image_path)
        self.add(image_path, scroll_factor, size)

    def rem(self, image_path):
        ''' Removes the parallax level created from the image_path.
            If no matching level is found nothing is removed. '''
        if image_path in self.levels_id:
            elem_id = self.levels_id[image_path]
            del self.levels[elem_id]
            del self.levels_id[image_path]

    def add(self, image_path, scroll_factor, size=None):
        ''' Adds a parallax level, first added level is the
            deepest level, i.e. furthest back into the \"screen\".

            image_path is the path to the image to be used
            scroll_factor is the slowdown factor for this parallax level. '''
        try:
            image = (pygame.image.load(image_path))
        except:
            message = "couldn't open image:" + image_path
            raise SystemExit(message)
        if ".png" in image_path:
            image = image.convert_alpha()
        else:
            image = image.convert()
        if len(self.levels) > 0:
            image.set_colorkey((0xff, 0x00, 0xea), self.colorkey_flags)
        if size is not None:
            image = pygame.transform.scale(image, size) # Change the image size
            self.chg_size(size) # Update the size
        # Track the current image by it id
        self.levels_id[image_path] = len(self.levels)
        self.levels.append(_subsurface(image, scroll_factor))

    def add_colorkeyed_surface(self, surface, scroll_factor,
                               color_key=(0xff, 0x00, 0xea)):
        ''' Adds a colorkeyed surface created elsewhere. '''
        surface = surface.convert()
        if len(self.levels) > 0:
            surface.set_colorkey(color_key, self.colorkey_flags)
        self.levels.append(_subsurface(surface, scroll_factor))

    def add_surface(self, surface, scroll_factor):
        ''' Adds a surface created elsewhere. '''
        surface = surface.convert_alpha()
        if len(self.levels) > 0:
            surface.set_colorkey((0xff, 0x00, 0xea), self.colorkey_flags)
        self.levels.append(_subsurface(surface, scroll_factor))

    def draw(self, surface):
        ''' This draws all parallax levels to the surface
            provided as argument. '''
        s_width = self.size[0]
        s_height = self.size[1]
        for lvl in self.levels:
            if self.orientation == 'vertical':
                surface.blit(lvl.surface, (0, 0),
                             (0, -lvl.scroll, s_width, s_height))
                surface.blit(lvl.surface,
                             (0, lvl.scroll - lvl.surface.get_height()))
            else:
                surface.blit(lvl.surface, (0, 0),
                             (lvl.scroll, 0, s_width, s_height))
                surface.blit(lvl.surface,
                             (lvl.surface.get_width() - lvl.scroll, 0),
                             (0, 0, lvl.scroll, s_height))

    def scroll(self, offset, orientation=None):
        '''scroll moves each surface _offset_ pixels / assigned factor'''
        if orientation is not None:
            self.orientation = orientation

        self.scroller = (self.scroller + offset)
        for lvl in self.levels:
            if self.orientation == 'vertical':
                lvl.scroll = (self.scroller / lvl.factor) \
                             % lvl.surface.get_height()
            else:
                lvl.scroll = (self.scroller / lvl.factor) \
                             % lvl.surface.get_width()

class VerticalParallaxSurface(ParallaxSurface):
    ''' Class implementing vertical scrolling parallax surface. '''
    def __init__(self, size, colorkey_flags=0):
        ParallaxSurface.__init__(self, size, colorkey_flags)
        self.orientation = 'vertical'


# === ./pyParallax-master/parallax/__init__.py ===
from parallax import *


# === ./slamtec.py ===
# from pyrplidar import PyRPlidar
# import time
# import numpy as np
# import matplotlib.pyplot as plt
# from matplotlib.animation import FuncAnimation
# from math import radians
# import numpy as np
# from math import radians, sin, cos
# import numpy as np
# from scipy.ndimage import binary_dilation


# class Search:
#     def __init__(self, angles, distances, world_size=200) -> None:
#         self.size = world_size

#         self.robot_x = self.size / 2
#         self.robot_y = self.size / 2

#         self.grid = self.build_grid(angles, distances)
#         self.goal = None
#         self.start = (self.robot_x, self.robot_y)

#     def build_grid(self, angles, distances):
#         grid = np.zeros((self.size, self.size))
#         for a, d in zip(angles, distances):
#             coord = self.lidar_to_grid(a, d)
#             if coord is None:
#                 continue
#             x, y = coord
#             grid[x, y] = 1

#         # plt.imshow(grid, cmap="gray", origin="lower")
#         # plt.show()

#         structure = np.ones((7, 7))

#         # Perform binary dilation to expand the obstacles
#         dilated_grid = binary_dilation(grid, structure=structure)
#         dilated_grid = dilated_grid.astype(int)

#         # plt.imshow(dilated_grid, cmap="gray", origin="lower")
#         # plt.show()

#         return dilated_grid

#     def lidar_to_grid(self, angle, distance):
#         # Ignore points with zero distance (no reading)
#         if distance == 0:
#             return None

#         # Calculate the world coordinates of the point
#         point_x = self.robot_x + (distance / 10) * cos(angle)
#         point_y = self.robot_y + (distance / 10) * sin(angle)

#         # Convert world coordinates to grid coordinates
#         grid_x = int(point_x)
#         grid_y = int(point_y)

#         # Ensure coordinates are within grid bounds
#         if 0 <= grid_x < self.size and 0 <= grid_y < self.size:
#             return grid_x, grid_y
#         else:
#             return None

#     def heuristic(self, a):
#         return np.linalg.norm(np.array(a) - np.array(self.goal))  # Euclidean distance

#     def a_star_search(self):
#         frontier = PriorityQueue()
#         frontier.put(self.start, 0)

#         came_from = {}  # Dictionary to store the path
#         cost_so_far = {}  # Dictionary to store the cost
#         came_from[self.start] = None
#         cost_so_far[self.start] = 0

#         while not frontier.empty():
#             current = frontier.get()

#             if current == self.goal:
#                 print("foudn goal")
#                 break  # Exit once the goal is reached

#             # Explore neighbors (up, down, left, right)
#             for dx, dy in [
#                 (-1, 0),
#                 (1, 0),
#                 (0, -1),
#                 (0, 1),
#                 (1, 1),
#                 (-1, -1),
#                 (1, -1),
#                 (-1, 1),
#             ]:  # Optionally add diagonals
#                 neighbor = (int(current[0] + dx), int(current[1] + dy))

#                 # Check if the neighbor is within bounds and not an obstacle
#                 if (
#                     0 <= neighbor[0] < self.grid.shape[0]
#                     and 0 <= neighbor[1] < self.grid.shape[1]
#                 ):
#                     if self.grid[neighbor[0], neighbor[1]] == 0:  # Free space
#                         new_cost = (
#                             cost_so_far[current] + 1
#                         )  # Cost of moving to a neighbor

#                         if (
#                             neighbor not in cost_so_far
#                             or new_cost < cost_so_far[neighbor]
#                         ):
#                             cost_so_far[neighbor] = new_cost
#                             priority = new_cost + self.heuristic(neighbor)
#                             frontier.put(neighbor, priority)
#                             came_from[neighbor] = current

#         return came_from, cost_so_far

#     def reconstruct_path(self, came_from):
#         current = self.goal
#         path = []

#         while current != self.start:
#             path.append(current)
#             current = came_from[current]

#         path.append(self.start)  # Optional: include start in the path
#         path.reverse()  # Reverse the path to get it from start to goal
#         return path

#     def find_path(self, goal):
#         self.goal = goal
#         came_from, cost_so_far = self.a_star_search()
#         path = self.reconstruct_path(came_from)
#         self.path = np.array([self.cartesian_to_polar(*coord) for coord in path])

#     def cartesian_to_polar(self, x, y):
#         # Translate coordinates so that the origin is the robot's position
#         dx = x - self.robot_x
#         dy = y - self.robot_y
#         dx *= 10
#         dy *= 10

#         # Calculate r (distance) and theta (angle in radians)
#         r = np.sqrt(dx**2 + dy**2)
#         theta = np.arctan2(dy, dx)  # Gives angle in radians

#         return theta, r


# class LidarPlot:
#     def __init__(self) -> None:
#         self.scatter, self.goal, self.path = self.init_polar()

#     def init_polar(self):
#         # Initialize the plot
#         fig, ax = plt.subplots(subplot_kw={"projection": "polar"})
#         ax.set_theta_zero_location("N")
#         ax.set_theta_direction(-1)
#         ax.set_rmax(1000)  # Set the maximum range for the LIDAR
#         ax.set_rticks([250, 500, 750, 1000])  # Example of range ticks
#         ax.grid(True)

#         # Mark every 45 degrees
#         ax.set_xticks(np.radians(np.arange(0, 360, 45)))
#         ax.set_yticks([250, 500, 750, 1000])  # Inner circles for range

#         scatter = ax.scatter([], [], s=10, color="blue")
#         goal = ax.scatter([], [], 100, marker="*", color="red")
#         (path,) = ax.plot([], [], "r-")

#         plt.ion()
#         plt.show()

#         return scatter, goal, path

#     def update_points(self, angles, distances):
#         self.scatter.set_offsets(np.c_[angles, distances])

#     def update_plot(self):
#         plt.draw()
#         plt.pause(0.01)

#     def plot_search(self, search: Search):
#         # plot goal star
#         goal = search.goal[0], search.goal[1]
#         angle, distance = search.cartesian_to_polar(*goal)
#         self.goal.set_offsets(np.c_[[angle], [distance]])

#         # plot path
#         angles = search.path[:, 0]
#         distances = search.path[:, 1]
#         gg = np.c_[angles, distances]
#         self.path.set_data(gg[:, 0], gg[:, 1])


# def plot_lidar_loop(scan_generator):
#     plot = LidarPlot()

#     while True:
#         angles = []
#         distances = []

#         for count, scan in enumerate(scan_generator()):
#             if scan.quality > 0 and scan.distance > 0:
#                 angle = radians(scan.angle)  # Convert to radians for polar plot
#                 distance = scan.distance

#                 angles.append(angle)
#                 distances.append(distance)

#             # Limit the number of points to avoid overloading the plot
#             if count > 750:
#                 break

#         plot.update_points(angles, distances)

#         search = Search(angles, distances)
#         search.find_path((100 + 20, 100 - 80))
#         plot.plot_search(search)

#         plot.update_plot()


# def plot_occupancy_loop(scan_generator, robot_x, robot_y):
#     occupancy_grid = np.zeros((GRID_SIZE, GRID_SIZE))
#     plt.imshow(occupancy_grid, cmap="gray", origin="lower")
#     plt.ion()
#     plt.show()

#     while True:
#         occupancy_grid = np.zeros((GRID_SIZE, GRID_SIZE))

#         # Fetch next scan data
#         for count, scan in enumerate(scan_generator()):
#             if scan.quality > 0 and scan.distance > 0:
#                 angle = radians(scan.angle)  # Convert to radians for polar plot
#                 distance = scan.distance / 1000
#                 # print(distance)

#                 grid_coords = lidar_to_grid(angle, distance, robot_x, robot_y)
#                 if grid_coords is None:
#                     continue

#                 # print(grid_coords, angle, distance)
#                 grid_x, grid_y = grid_coords
#                 occupancy_grid[grid_x, grid_y] = 1

#             # Limit the number of points to avoid overloading the plot
#             if count > 700:
#                 break

#         print(occupancy_grid.sum())
#         plt.imshow(occupancy_grid, cmap="gray", origin="lower")
#         # plt.title("Occupancy Grid")
#         # plt.show()
#         plt.draw()
#         plt.pause(0.01)


# lidar = PyRPlidar()
# lidar.connect(port="COM9", baudrate=256000, timeout=5)
# lidar.set_motor_pwm(500)
# time.sleep(2)

# try:

#     scan_generator = lidar.force_scan()
#     plot_lidar_loop(scan_generator)
#     # plot_occupancy_loop(scan_generator, int(WORLD_SIZE / 2), int(WORLD_SIZE / 2))


# finally:
#     lidar.stop()
#     lidar.set_motor_pwm(0)
#     lidar.disconnect()
import time
from typing import List

from matplotlib import pyplot as plt
from src.path_finding.LidarPlot import LidarPlot
from src.path_finding.OccupancyGrid import OccupancyGrid
from src.path_finding.Position import Position
from src.multithreading import graceful_thread_exit
from src.multithreading.LidarSensorThread import LidarSensorThread
from src.path_finding.AStarSearch import AStarSearch


@graceful_thread_exit
def start():
    positions = []

    thread = LidarSensorThread(positions)
    thread.start()

    range = 1000
    resolution = 25
    scale_fact = resolution / range

    def scale(positions: List[Position]):
        scaled = []
        for pos in positions:
            clone = pos.copy()
            clone.dilate(scale_fact)
            clone.move(resolution, resolution)
            scaled.append(clone)
        return scaled

    def unscale(positions: List[Position]):
        unscaled = []
        for pos in positions:
            clone = pos.copy()
            clone.move(-resolution, -resolution)
            clone.dilate(1 / scale_fact)
            unscaled.append(clone)
            # print(pos.x, pos.y, pos.angle, pos.distance)
            # print(clone.x, clone.y, clone.angle, clone.distance)
            # print("\n\n")
        return unscaled

    plot = LidarPlot(range_mm=range)
    goal = Position(x=200, y=-800)
    time.sleep(2)

    count = 0
    while True:
        if count % 50 == 0:
            grid = OccupancyGrid(scale(positions), resolution * 2)

            # # plt.imshow(grid.grid, cmap="gray", origin="lower")
            # # plt.show()
            search = AStarSearch(
                grid,
                Position(x=resolution, y=resolution),
                scale([goal])[0],
            )
            path = search.find_path()
            path = unscale(path)
            plot.plot_path(path)
            
            pass
        plot.plot_obstacles(positions)
        plot.plot_destination(goal)
        plot.update_plot()
        time.sleep(0.1)
        count += 1


if __name__ == "__main__":
    start()


# === ./UltraBorg.py ===
#!/usr/bin/env python3
# coding: utf-8
"""
This module is designed to communicate with the UltraBorg

Use by creating an instance of the class, call the Init function, then command as desired, e.g.
import UltraBorg
UB = UltraBorg.UltraBorg()
UB.Init()
# User code here, use UB to control the board

Multiple boards can be used when configured with different I²C addresses by creating multiple instances, e.g.
import UltraBorg
UB1 = UltraBorg.UltraBorg()
UB2 = UltraBorg.UltraBorg()
UB1.i2cAddress = 0x44
UB2.i2cAddress = 0x45
UB1.Init()
UB2.Init()
# User code here, use UB1 and UB2 to control each board separately

For explanations of the functions available call the Help function, e.g.
import UltraBorg
UB = UltraBorg.UltraBorg()
UB.Help()
See the website at www.piborg.org/ultraborg for more details
"""

# Import the libraries we need
import io
import fcntl
import types
import time

# Constant values
I2C_SLAVE               = 0x0703
I2C_MAX_LEN             = 4
USM_US_TO_MM            = 0.171500
PWM_MIN                 = 2000  # Should be a 1 ms burst, typical servo minimum
PWM_MAX                 = 4000  # Should be a 2 ms burst, typical servo maximum
DELAY_AFTER_EEPROM      = 0.01  # Time to wait after updating an EEPROM value before reading
PWM_UNSET               = 0xFFFF

I2C_ID_SERVO_USM        = 0x36

COMMAND_GET_TIME_USM1   = 1     # Get the time measured by ultrasonic #1 in us (0 for no detection)
COMMAND_GET_TIME_USM2   = 2     # Get the time measured by ultrasonic #2 in us (0 for no detection)
COMMAND_GET_TIME_USM3   = 3     # Get the time measured by ultrasonic #3 in us (0 for no detection)
COMMAND_GET_TIME_USM4   = 4     # Get the time measured by ultrasonic #4 in us (0 for no detection)
COMMAND_SET_PWM1        = 5     # Set the PWM duty cycle for drive #1 (16 bit)
COMMAND_GET_PWM1        = 6     # Get the PWM duty cycle for drive #1 (16 bit)
COMMAND_SET_PWM2        = 7     # Set the PWM duty cycle for drive #2 (16 bit)
COMMAND_GET_PWM2        = 8     # Get the PWM duty cycle for drive #2 (16 bit)
COMMAND_SET_PWM3        = 9     # Set the PWM duty cycle for drive #3 (16 bit)
COMMAND_GET_PWM3        = 10    # Get the PWM duty cycle for drive #3 (16 bit)
COMMAND_SET_PWM4        = 11    # Set the PWM duty cycle for drive #4 (16 bit)
COMMAND_GET_PWM4        = 12    # Get the PWM duty cycle for drive #4 (16 bit)
COMMAND_CALIBRATE_PWM1  = 13    # Set the PWM duty cycle for drive #1 (16 bit, ignores limit checks)
COMMAND_CALIBRATE_PWM2  = 14    # Set the PWM duty cycle for drive #2 (16 bit, ignores limit checks)
COMMAND_CALIBRATE_PWM3  = 15    # Set the PWM duty cycle for drive #3 (16 bit, ignores limit checks)
COMMAND_CALIBRATE_PWM4  = 16    # Set the PWM duty cycle for drive #4 (16 bit, ignores limit checks)
COMMAND_GET_PWM_MIN_1   = 17    # Get the minimum allowed PWM duty cycle for drive #1
COMMAND_GET_PWM_MAX_1   = 18    # Get the maximum allowed PWM duty cycle for drive #1
COMMAND_GET_PWM_BOOT_1  = 19    # Get the startup PWM duty cycle for drive #1
COMMAND_GET_PWM_MIN_2   = 20    # Get the minimum allowed PWM duty cycle for drive #2
COMMAND_GET_PWM_MAX_2   = 21    # Get the maximum allowed PWM duty cycle for drive #2
COMMAND_GET_PWM_BOOT_2  = 22    # Get the startup PWM duty cycle for drive #2
COMMAND_GET_PWM_MIN_3   = 23    # Get the minimum allowed PWM duty cycle for drive #3
COMMAND_GET_PWM_MAX_3   = 24    # Get the maximum allowed PWM duty cycle for drive #3
COMMAND_GET_PWM_BOOT_3  = 25    # Get the startup PWM duty cycle for drive #3
COMMAND_GET_PWM_MIN_4   = 26    # Get the minimum allowed PWM duty cycle for drive #4
COMMAND_GET_PWM_MAX_4   = 27    # Get the maximum allowed PWM duty cycle for drive #4
COMMAND_GET_PWM_BOOT_4  = 28    # Get the startup PWM duty cycle for drive #4
COMMAND_SET_PWM_MIN_1   = 29    # Set the minimum allowed PWM duty cycle for drive #1
COMMAND_SET_PWM_MAX_1   = 30    # Set the maximum allowed PWM duty cycle for drive #1
COMMAND_SET_PWM_BOOT_1  = 31    # Set the startup PWM duty cycle for drive #1
COMMAND_SET_PWM_MIN_2   = 32    # Set the minimum allowed PWM duty cycle for drive #2
COMMAND_SET_PWM_MAX_2   = 33    # Set the maximum allowed PWM duty cycle for drive #2
COMMAND_SET_PWM_BOOT_2  = 34    # Set the startup PWM duty cycle for drive #2
COMMAND_SET_PWM_MIN_3   = 35    # Set the minimum allowed PWM duty cycle for drive #3
COMMAND_SET_PWM_MAX_3   = 36    # Set the maximum allowed PWM duty cycle for drive #3
COMMAND_SET_PWM_BOOT_3  = 37    # Set the startup PWM duty cycle for drive #3
COMMAND_SET_PWM_MIN_4   = 38    # Set the minimum allowed PWM duty cycle for drive #4
COMMAND_SET_PWM_MAX_4   = 39    # Set the maximum allowed PWM duty cycle for drive #4
COMMAND_SET_PWM_BOOT_4  = 40    # Set the startup PWM duty cycle for drive #4
COMMAND_GET_FILTER_USM1 = 41    # Get the filtered time measured by ultrasonic #1 in us (0 for no detection)
COMMAND_GET_FILTER_USM2 = 42    # Get the filtered time measured by ultrasonic #2 in us (0 for no detection)
COMMAND_GET_FILTER_USM3 = 43    # Get the filtered time measured by ultrasonic #3 in us (0 for no detection)
COMMAND_GET_FILTER_USM4 = 44    # Get the filtered time measured by ultrasonic #4 in us (0 for no detection)
COMMAND_GET_ID          = 0x99  # Get the board identifier
COMMAND_SET_I2C_ADD     = 0xAA  # Set a new I2C address

COMMAND_VALUE_FWD       = 1     # I2C value representing forward
COMMAND_VALUE_REV       = 2     # I2C value representing reverse

COMMAND_VALUE_ON        = 1     # I2C value representing on
COMMAND_VALUE_OFF       = 0     # I2C value representing off


def ScanForUltraBorg(busNumber = 7):
    """
ScanForUltraBorg([busNumber])

Scans the I²C bus for a UltraBorg boards and returns a list of all usable addresses
The busNumber if supplied is which I²C bus to scan, 0 for Rev 1 boards, 1 for Rev 2 boards, if not supplied the default is 1
    """
    found = []
    print('Scanning I²C bus #%d' % (busNumber))
    bus = UltraBorg()
    for address in range(0x03, 0x78, 1):
        try:
            bus.InitBusOnly(busNumber, address)
            i2cRecv = bus.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
            if len(i2cRecv) == I2C_MAX_LEN:
                if i2cRecv[1] == I2C_ID_SERVO_USM:
                    print('Found UltraBorg at %02X' % (address))
                    found.append(address)
                else:
                    pass
            else:
                pass
        except KeyboardInterrupt:
            raise
        except:
            pass
    if len(found) == 0:
        print('No UltraBorg boards found, is bus #%d correct (should be 0 for Rev 1, 1 for Rev 2)' % (busNumber))
    elif len(found) == 1:
        print('1 UltraBorg board found')
    else:
        print('%d UltraBorg boards found' % (len(found)))
    return found


def SetNewAddress(newAddress, oldAddress = -1, busNumber = 7):
    """
SetNewAddress(newAddress, [oldAddress], [busNumber])

Scans the I²C bus for the first UltraBorg and sets it to a new I2C address
If oldAddress is supplied it will change the address of the board at that address rather than scanning the bus
The busNumber if supplied is which I²C bus to scan, 0 for Rev 1 boards, 1 for Rev 2 boards, if not supplied the default is 1
Warning, this new I²C address will still be used after resetting the power on the device
    """
    if newAddress < 0x03:
        print('Error, I²C addresses below 3 (0x03) are reserved, use an address between 3 (0x03) and 119 (0x77)')
        return
    elif newAddress > 0x77:
        print('Error, I²C addresses above 119 (0x77) are reserved, use an address between 3 (0x03) and 119 (0x77)')
        return
    if oldAddress < 0x0:
        found = ScanForUltraBorg(busNumber)
        if len(found) < 1:
            print('No UltraBorg boards found, cannot set a new I²C address!')
            return
        else:
            oldAddress = found[0]
    print('Changing I²C address from %02X to %02X (bus #%d)' % (oldAddress, newAddress, busNumber))
    bus = UltraBorg()
    bus.InitBusOnly(busNumber, oldAddress)
    try:
        i2cRecv = bus.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
        if len(i2cRecv) == I2C_MAX_LEN:
            if i2cRecv[1] == I2C_ID_SERVO_USM:
                foundChip = True
                print('Found UltraBorg at %02X' % (oldAddress))
            else:
                foundChip = False
                print('Found a device at %02X, but it is not a UltraBorg (ID %02X instead of %02X)' % (oldAddress, i2cRecv[1], I2C_ID_SERVO_USM))
        else:
            foundChip = False
            print('Missing UltraBorg at %02X' % (oldAddress))
    except KeyboardInterrupt:
        raise
    except:
        foundChip = False
        print('Missing UltraBorg at %02X' % (oldAddress))
    if foundChip:
        bus.RawWrite(COMMAND_SET_I2C_ADD, [newAddress])
        time.sleep(0.1)
        print('Address changed to %02X, attempting to talk with the new address' % (newAddress))
        try:
            bus.InitBusOnly(busNumber, newAddress)
            i2cRecv = bus.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
            if len(i2cRecv) == I2C_MAX_LEN:
                if i2cRecv[1] == I2C_ID_SERVO_USM:
                    foundChip = True
                    print('Found UltraBorg at %02X' % (newAddress))
                else:
                    foundChip = False
                    print('Found a device at %02X, but it is not a UltraBorg (ID %02X instead of %02X)' % (newAddress, i2cRecv[1], I2C_ID_SERVO_USM))
            else:
                foundChip = False
                print('Missing UltraBorg at %02X' % (newAddress))
        except KeyboardInterrupt:
            raise
        except:
            foundChip = False
            print('Missing UltraBorg at %02X' % (newAddress))
    if foundChip:
        print('New I²C address of %02X set successfully' % (newAddress))
    else:
        print('Failed to set new I²C address...')


# Class used to control UltraBorg
class UltraBorg:
    """
This module is designed to communicate with the UltraBorg

busNumber               I²C bus on which the UltraBorg is attached (Rev 1 is bus 0, Rev 2 is bus 1)
bus                     the smbus object used to talk to the I²C bus
i2cAddress              The I²C address of the UltraBorg chip to control
foundChip               True if the UltraBorg chip can be seen, False otherwise
printFunction           Function reference to call when printing text, if None "print" is used
    """

    # Shared values used by this class
    busNumber               = 7                # Check here for Rev 1 vs Rev 2 and select the correct bus
    i2cAddress              = I2C_ID_SERVO_USM  # I²C address, override for a different address
    foundChip               = False
    printFunction           = None
    i2cWrite                = None
    i2cRead                 = None

    # Default calibration adjustments to standard values
    PWM_MIN_1               = PWM_MIN
    PWM_MAX_1               = PWM_MAX
    PWM_MIN_2               = PWM_MIN
    PWM_MAX_2               = PWM_MAX
    PWM_MIN_3               = PWM_MIN
    PWM_MAX_3               = PWM_MAX
    PWM_MIN_4               = PWM_MIN
    PWM_MAX_4               = PWM_MAX

    def RawWrite(self, command, data):
        """
RawWrite(command, data)

Sends a raw command on the I2C bus to the UltraBorg
Command codes can be found at the top of UltraBorg.py, data is a list of 0 or more byte values

Under most circumstances you should use the appropriate function instead of RawWrite
        """
        rawOutput = [command]
        rawOutput.extend(data)
        rawOutput = bytes(rawOutput)
        self.i2cWrite.write(rawOutput)


    def RawRead(self, command, length, retryCount = 5):
        """
RawRead(command, length, [retryCount])

Reads data back from the UltraBorg after sending a GET command
Command codes can be found at the top of UltraBorg.py, length is the number of bytes to read back

The function checks that the first byte read back matches the requested command
If it does not it will retry the request until retryCount is exhausted (default is 3 times)

Under most circumstances you should use the appropriate function instead of RawRead
        """
        while retryCount > 0:
            try:
                self.RawWrite(command, [])
            except:
                # Delay on failed bus write
                retryCount -= 1
                time.sleep(0.1)
                continue
            time.sleep(0.000001)
            try:
                rawReply = self.i2cRead.read(length)
            except:
                # Delay on failed bus read
                retryCount -= 1
                time.sleep(0.1)
                continue
            reply = []
            for singleByte in rawReply:
                reply.append(singleByte)
            if command == reply[0]:
                # Reply successful
                break
            else:
                retryCount -= 1
        if retryCount > 0:
            return reply
        else:
            raise IOError('I2C read for command %d failed' % (command))


    def InitBusOnly(self, busNumber, address):
        """
InitBusOnly(busNumber, address)

Prepare the I2C driver for talking to a UltraBorg on the specified bus and I2C address
This call does not check the board is present or working, under most circumstances use Init() instead
        """
        self.busNumber = busNumber
        self.i2cAddress = address
        self.i2cRead = io.open("/dev/i2c-" + str(self.busNumber), "rb", buffering = 0)
        fcntl.ioctl(self.i2cRead, I2C_SLAVE, self.i2cAddress)
        self.i2cWrite = io.open("/dev/i2c-" + str(self.busNumber), "wb", buffering = 0)
        fcntl.ioctl(self.i2cWrite, I2C_SLAVE, self.i2cAddress)


    def Print(self, message):
        """
Print(message)

Wrapper used by the UltraBorg instance to print messages, will call printFunction if set, print otherwise
        """
        if self.printFunction == None:
            print(message)
        else:
            self.printFunction(message)


    def NoPrint(self, message):
        """
NoPrint(message)

Does nothing, intended for disabling diagnostic printout by using:
UB = UltraBorg.UltraBorg()
UB.printFunction = UB.NoPrint
        """
        pass


    def Init(self, tryOtherBus = False):
        """
Init([tryOtherBus])

Prepare the I2C driver for talking to the UltraBorg

If tryOtherBus is True, this function will attempt to use the other bus if the ThunderBorg devices can not be found on the current busNumber
    This is only really useful for early Raspberry Pi models!
        """
        self.Print('Loading UltraBorg on bus %d, address %02X' % (self.busNumber, self.i2cAddress))

        # Open the bus
        self.i2cRead = io.open("/dev/i2c-" + str(self.busNumber), "rb", buffering = 0)
        fcntl.ioctl(self.i2cRead, I2C_SLAVE, self.i2cAddress)
        self.i2cWrite = io.open("/dev/i2c-" + str(self.busNumber), "wb", buffering = 0)
        fcntl.ioctl(self.i2cWrite, I2C_SLAVE, self.i2cAddress)

        # Check for UltraBorg
        try:
            i2cRecv = self.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
            if len(i2cRecv) == I2C_MAX_LEN:
                if i2cRecv[1] == I2C_ID_SERVO_USM:
                    self.foundChip = True
                    self.Print('Found UltraBorg at %02X' % (self.i2cAddress))
                else:
                    self.foundChip = False
                    self.Print('Found a device at %02X, but it is not a UltraBorg (ID %02X instead of %02X)' % (self.i2cAddress, i2cRecv[1], I2C_ID_SERVO_USM))
            else:
                self.foundChip = False
                self.Print('Missing UltraBorg at %02X' % (self.i2cAddress))
        except KeyboardInterrupt:
            raise
        except:
            self.foundChip = False
            self.Print('Missing UltraBorg at %02X' % (self.i2cAddress))

        # See if we are missing chips
        if not self.foundChip:
            self.Print('UltraBorg was not found')
            if tryOtherBus:
                if self.busNumber == 1:
                    self.busNumber = 7
                else:
                    self.busNumber = 1
                self.Print('Trying bus %d instead' % (self.busNumber))
                self.Init(False)
            else:
                self.Print('Are you sure your UltraBorg is properly attached, the correct address is used, and the I2C drivers are running?')
                self.bus = None
        else:
            self.Print('UltraBorg loaded on bus %d' % (self.busNumber))

        # Read the calibration settings from the UltraBorg
        self.PWM_MIN_1 = self.GetWithRetry(self.GetServoMinimum1, 5)
        self.PWM_MAX_1 = self.GetWithRetry(self.GetServoMaximum1, 5)
        self.PWM_MIN_2 = self.GetWithRetry(self.GetServoMinimum2, 5)
        self.PWM_MAX_2 = self.GetWithRetry(self.GetServoMaximum2, 5)
        self.PWM_MIN_3 = self.GetWithRetry(self.GetServoMinimum3, 5)
        self.PWM_MAX_3 = self.GetWithRetry(self.GetServoMaximum3, 5)
        self.PWM_MIN_4 = self.GetWithRetry(self.GetServoMinimum4, 5)
        self.PWM_MAX_4 = self.GetWithRetry(self.GetServoMaximum4, 5)


    def GetWithRetry(self, function, count):
        """
value = GetWithRetry(function, count)

Attempts to read a value multiple times before giving up
Pass a get function with no parameters
e.g.
distance = GetWithRetry(UB.GetDistance1, 5)
Will try UB.GetDistance1() upto 5 times, returning when it gets a value
Useful for ensuring a read is successful
        """
        value = None
        for i in range(count):
            okay = True
            try:
                value = function()
            except KeyboardInterrupt:
                raise
            except:
                okay = False
            if okay:
                break
        return value


    def SetWithRetry(self, setFunction, getFunction, value, count):
        """
worked = SetWithRetry(setFunction, getFunction, value, count)

Attempts to write a value multiple times before giving up
Pass a set function with one parameter, and a get function no parameters
The get function will be used to check if the set worked, if not it will be repeated
e.g.
worked = SetWithRetry(UB.SetServoMinimum1, UB.GetServoMinimum1, 2000, 5)
Will try UB.SetServoMinimum1(2000) upto 5 times, returning when UB.GetServoMinimum1 returns 2000.
Useful for ensuring a write is successful
        """
        for i in range(count):
            okay = True
            try:
                setFunction(value)
                readValue = getFunction()
            except KeyboardInterrupt:
                raise
            except:
                okay = False
            if okay:
                if readValue == value:
                    break
                else:
                    okay = False
        return okay


    def GetDistance1(self):
        """
distance = GetDistance1()

Gets the filtered distance for ultrasonic module #1 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
If you need a faster response try GetRawDistance1 instead (no filtering)
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #1 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetDistance2(self):
        """
distance = GetDistance2()

Gets the filtered distance for ultrasonic module #2 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
If you need a faster response try GetRawDistance2 instead (no filtering)
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #2 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetDistance3(self):
        """
distance = GetDistance3()

Gets the filtered distance for ultrasonic module #3 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
If you need a faster response try GetRawDistance3 instead (no filtering)
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #3 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetDistance4(self):
        """
distance = GetDistance4()

Gets the filtered distance for ultrasonic module #4 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
If you need a faster response try GetRawDistance4 instead (no filtering)
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #4 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM



    def GetRawDistance1(self):
        """
distance = GetRawDistance1()

Gets the raw distance for ultrasonic module #1 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
For a filtered (less jumpy) reading use GetDistance1
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #1 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetRawDistance2(self):
        """
distance = GetRawDistance2()

Gets the raw distance for ultrasonic module #2 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
For a filtered (less jumpy) reading use GetDistance2
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #2 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetRawDistance3(self):
        """
distance = GetRawDistance3()

Gets the raw distance for ultrasonic module #3 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
For a filtered (less jumpy) reading use GetDistance3
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #3 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetRawDistance4(self):
        """
distance = GetRawDistance4()

Gets the distance for ultrasonic module #4 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
For a filtered (less jumpy) reading use GetDistance4
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #4 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetServoPosition1(self):
        """
position = GetServoPosition1()

Gets the drive position for servo output #1
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo output #1!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_1) / (self.PWM_MAX_1 - self.PWM_MIN_1)
        return (2.0 * powerOut) - 1.0


    def GetServoPosition2(self):
        """
position = GetServoPosition2()

Gets the drive position for servo output #2
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo output #2!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_2) / (self.PWM_MAX_2 - self.PWM_MIN_2)
        return (2.0 * powerOut) - 1.0


    def GetServoPosition3(self):
        """
position = GetServoPosition3()

Gets the drive position for servo output #3
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo output #3!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_3) / (self.PWM_MAX_3 - self.PWM_MIN_3)
        return (2.0 * powerOut) - 1.0


    def GetServoPosition4(self):
        """
position = GetServoPosition4()

Gets the drive position for servo output #4
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo output #4!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_4) / (self.PWM_MAX_4 - self.PWM_MIN_4)
        return (2.0 * powerOut) - 1.0


    def SetServoPosition1(self, position):
        """
SetServoPosition1(position)

Sets the drive position for servo output #1
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_1 - self.PWM_MIN_1)) + self.PWM_MIN_1)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo output #1!')


    def SetServoPosition2(self, position):
        """
SetServoPosition2(position)

Sets the drive position for servo output #2
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_2 - self.PWM_MIN_2)) + self.PWM_MIN_2)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo output #2!')


    def SetServoPosition3(self, position):
        """
SetServoPosition3(position)

Sets the drive position for servo output #3
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_3 - self.PWM_MIN_3)) + self.PWM_MIN_3)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo output #3!')


    def SetServoPosition4(self, position):
        """
SetServoPosition4(position)

Sets the drive position for servo output #4
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_4 - self.PWM_MIN_4)) + self.PWM_MIN_4)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo output #1!')


    def GetServoMinimum1(self):
        """
pwmLevel = GetServoMinimum1()

Gets the minimum PWM level for servo output #1
This corresponds to position -1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #1 minimum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMinimum2(self):
        """
pwmLevel = GetServoMinimum2()

Gets the minimum PWM level for servo output #2
This corresponds to position -1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #2 minimum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMinimum3(self):
        """
pwmLevel = GetServoMinimum3()

Gets the minimum PWM level for servo output #3
This corresponds to position -1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #3 minimum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMinimum4(self):
        """
pwmLevel = GetServoMinimum4()

Gets the minimum PWM level for servo output #4
This corresponds to position -1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #4 minimum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMaximum1(self):
        """
pwmLevel = GetServoMaximum1()

Gets the maximum PWM level for servo output #1
This corresponds to position +1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #1 maximum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMaximum2(self):
        """
pwmLevel = GetServoMaximum2()

Gets the maximum PWM level for servo output #2
This corresponds to position +1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #2 maximum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMaximum3(self):
        """
pwmLevel = GetServoMaximum3()

Gets the maximum PWM level for servo output #3
This corresponds to position +1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #3 maximum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMaximum4(self):
        """
pwmLevel = GetServoMaximum4()

Gets the maximum PWM level for servo output #4
This corresponds to position +1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #4 maximum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoStartup1(self):
        """
pwmLevel = GetServoStartup1()

Gets the startup PWM level for servo output #1
This can be anywhere in the minimum to maximum range
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #1 startup burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoStartup2(self):
        """
pwmLevel = GetServoStartup2()

Gets the startup PWM level for servo output #2
This can be anywhere in the minimum to maximum range
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #2 startup burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoStartup3(self):
        """
pwmLevel = GetServoStartup3()

Gets the startup PWM level for servo output #3
This can be anywhere in the minimum to maximum range
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #3 startup burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoStartup4(self):
        """
pwmLevel = GetServoStartup4()

Gets the startup PWM level for servo output #4
This can be anywhere in the minimum to maximum range
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre, 
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #4 startup burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def CalibrateServoPosition1(self, pwmLevel):
        """
CalibrateServoPosition1(pwmLevel)

Sets the raw PWM level for servo output #1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending calibration servo output #1!')


    def CalibrateServoPosition2(self, pwmLevel):
        """
CalibrateServoPosition2(pwmLevel)

Sets the raw PWM level for servo output #2
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending calibration servo output #2!')


    def CalibrateServoPosition3(self, pwmLevel):
        """
CalibrateServoPosition3(pwmLevel)

Sets the raw PWM level for servo output #3
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending calibration servo output #3!')


    def CalibrateServoPosition4(self, pwmLevel):
        """
CalibrateServoPosition4(pwmLevel)

Sets the raw PWM level for servo output #4
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending calibration servo output #4!')


    def GetRawServoPosition1(self):
        """
pwmLevel = GetRawServoPosition1()

Gets the raw PWM level for servo output #1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

This value requires interpreting into an actual servo position, this is already done by GetServoPosition1
We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading raw servo output #1!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty


    def GetRawServoPosition2(self):
        """
pwmLevel = GetRawServoPosition2()

Gets the raw PWM level for servo output #2
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

This value requires interpreting into an actual servo position, this is already done by GetServoPosition2
We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading raw servo output #2!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty


    def GetRawServoPosition3(self):
        """
pwmLevel = GetRawServoPosition3()

Gets the raw PWM level for servo output #3
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

This value requires interpreting into an actual servo position, this is already done by GetServoPosition3
We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading raw servo output #3!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty


    def GetRawServoPosition4(self):
        """
pwmLevel = GetRawServoPosition4()

Gets the raw PWM level for servo output #4
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

This value requires interpreting into an actual servo position, this is already done by GetServoPosition4
We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading raw servo output #4!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty


    def SetServoMinimum1(self, pwmLevel):
        """
SetServoMinimum1(pwmLevel)

Sets the minimum PWM level for servo output #1
This corresponds to position -1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo minimum limit #1!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_1 = self.GetServoMinimum1()


    def SetServoMinimum2(self, pwmLevel):
        """
SetServoMinimum2(pwmLevel)

Sets the minimum PWM level for servo output #2
This corresponds to position -1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo minimum limit #2!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_2 = self.GetServoMinimum2()


    def SetServoMinimum3(self, pwmLevel):
        """
SetServoMinimum3(pwmLevel)

Sets the minimum PWM level for servo output #3
This corresponds to position -1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo minimum limit #3!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_3 = self.GetServoMinimum3()


    def SetServoMinimum4(self, pwmLevel):
        """
SetServoMinimum4(pwmLevel)

Sets the minimum PWM level for servo output #4
This corresponds to position -1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo minimum limit #4!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_4 = self.GetServoMinimum4()


    def SetServoMaximum1(self, pwmLevel):
        """
SetServoMaximum1(pwmLevel)

Sets the maximum PWM level for servo output #1
This corresponds to position +1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo maximum limit #1!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_1 = self.GetServoMaximum1()


    def SetServoMaximum2(self, pwmLevel):
        """
SetServoMaximum2(pwmLevel)

Sets the maximum PWM level for servo output #2
This corresponds to position +1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo maximum limit #2!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_2 = self.GetServoMaximum2()


    def SetServoMaximum3(self, pwmLevel):
        """
SetServoMaximum3(pwmLevel)

Sets the maximum PWM level for servo output #3
This corresponds to position +1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo maximum limit #3!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_3 = self.GetServoMaximum3()


    def SetServoMaximum4(self, pwmLevel):
        """
SetServoMaximum4(pwmLevel)

Sets the maximum PWM level for servo output #4
This corresponds to position +1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo maximum limit #4!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_4 = self.GetServoMaximum4()


    def SetServoStartup1(self, pwmLevel):
        """
SetServoStartup1(pwmLevel)

Sets the startup PWM level for servo output #1
This can be anywhere in the minimum to maximum range

We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1
This value is checked against the current servo limits before setting

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_1 < self.PWM_MAX_1:
            # Normal direction
            if pwmLevel < self.PWM_MIN_1:
                inRange = False
            elif pwmLevel > self.PWM_MAX_1:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_1:
                inRange = False
            elif pwmLevel < self.PWM_MAX_1:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print('Servo #1 startup position %d is outside the limits of %d to %d' % (pwmLevel, self.PWM_MIN_1, self.PWM_MAX_1))
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo startup position #1!')
        time.sleep(DELAY_AFTER_EEPROM)


    def SetServoStartup2(self, pwmLevel):
        """
SetServoStartup2(pwmLevel)

Sets the startup PWM level for servo output #2
This can be anywhere in the minimum to maximum range

We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2
This value is checked against the current servo limits before setting

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_2 < self.PWM_MAX_2:
            # Normal direction
            if pwmLevel < self.PWM_MIN_2:
                inRange = False
            elif pwmLevel > self.PWM_MAX_2:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_2:
                inRange = False
            elif pwmLevel < self.PWM_MAX_2:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print('Servo #2 startup position %d is outside the limits of %d to %d' % (pwmLevel, self.PWM_MIN_2, self.PWM_MAX_2))
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo startup position #2!')
        time.sleep(DELAY_AFTER_EEPROM)


    def SetServoStartup3(self, pwmLevel):
        """
SetServoStartup3(pwmLevel)

Sets the startup PWM level for servo output #3
This can be anywhere in the minimum to maximum range

We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3
This value is checked against the current servo limits before setting

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_3 < self.PWM_MAX_3:
            # Normal direction
            if pwmLevel < self.PWM_MIN_3:
                inRange = False
            elif pwmLevel > self.PWM_MAX_3:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_3:
                inRange = False
            elif pwmLevel < self.PWM_MAX_3:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print('Servo #3 startup position %d is outside the limits of %d to %d' % (pwmLevel, self.PWM_MIN_3, self.PWM_MAX_3))
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo startup position #3!')
        time.sleep(DELAY_AFTER_EEPROM)


    def SetServoStartup4(self, pwmLevel):
        """
SetServoStartup4(pwmLevel)

Sets the startup PWM level for servo output #4
This can be anywhere in the minimum to maximum range

We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4
This value is checked against the current servo limits before setting

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_4 < self.PWM_MAX_4:
            # Normal direction
            if pwmLevel < self.PWM_MIN_4:
                inRange = False
            elif pwmLevel > self.PWM_MAX_4:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_4:
                inRange = False
            elif pwmLevel < self.PWM_MAX_4:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print('Servo #4 startup position %d is outside the limits of %d to %d' % (pwmLevel, self.PWM_MIN_4, self.PWM_MAX_4))
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo startup position #4!')
        time.sleep(DELAY_AFTER_EEPROM)


    def Help(self):
        """
Help()

Displays the names and descriptions of the various functions and settings provided
        """
        funcList = [UltraBorg.__dict__.get(a) for a in dir(UltraBorg) if isinstance(UltraBorg.__dict__.get(a), types.FunctionType)]
        funcListSorted = sorted(funcList, key = lambda x: x.func_code.co_firstlineno)

        print(self.__doc__)
        print
        for func in funcListSorted:
            print('=== %s === %s' % (func.func_name, func.func_doc))




# === ./parallax/parallax.py ===
# -*- coding: utf-8 -*-

#    Copyright (C) , 2012 Åke Forslund (ake.forslund@gmail.com)
#
#    Permission to use, copy, modify, and/or distribute this software for any
#    purpose with or without fee is hereby granted, provided that the above
#    copyright notice and this permission notice appear in all copies.
#
#    THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
#    WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
#    MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
#    ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#    WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
#    ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
#    OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
#

'''A simple parallax rendering module'''

import pygame

class _subsurface(object):
    '''Container class for subsurface'''
    def __init__(self, surface, factor):
        self.scroll = 0
        self.factor = factor
        self.surface = surface

class ParallaxSurface(object):
    '''Class handling parallax scrolling of a series of surfaces'''
    def __init__(self, size, colorkey_flags=0):
        self.colorkey_flags = colorkey_flags
        self.scroller = 0
        self.levels = []
        self.levels_id = {}
        self.size = size
        self.orientation = 'horizontal'

    def chg_size(self, size):
        ''' Changes the parallax surface's size. '''
        self.size = size

    def update(self, image_path, scroll_factor, size=(0, 0)):
        ''' Updates the parallax level identified by image_path and
            redefines the layer's scroll_factor and the size of the entire
            parallax surface. '''
        self.rem(image_path)
        self.add(image_path, scroll_factor, size)

    def rem(self, image_path):
        ''' Removes the parallax level created from the image_path.
            If no matching level is found nothing is removed. '''
        if image_path in self.levels_id:
            elem_id = self.levels_id[image_path]
            del self.levels[elem_id]
            del self.levels_id[image_path]

    def add(self, image_path, scroll_factor, size=None):
        ''' Adds a parallax level, first added level is the
            deepest level, i.e. furthest back into the \"screen\".

            image_path is the path to the image to be used
            scroll_factor is the slowdown factor for this parallax level. '''
        try:
            image = (pygame.image.load(image_path))
        except:
            message = "couldn't open image:" + image_path
            raise SystemExit(message)
        if ".png" in image_path:
            image = image.convert_alpha()
        else:
            image = image.convert()
        if len(self.levels) > 0:
            image.set_colorkey((0xff, 0x00, 0xea), self.colorkey_flags)
        if size is not None:
            image = pygame.transform.scale(image, size) # Change the image size
            self.chg_size(size) # Update the size
        # Track the current image by it id
        self.levels_id[image_path] = len(self.levels)
        self.levels.append(_subsurface(image, scroll_factor))

    def add_colorkeyed_surface(self, surface, scroll_factor,
                               color_key=(0xff, 0x00, 0xea)):
        ''' Adds a colorkeyed surface created elsewhere. '''
        surface = surface.convert()
        if len(self.levels) > 0:
            surface.set_colorkey(color_key, self.colorkey_flags)
        self.levels.append(_subsurface(surface, scroll_factor))

    def add_surface(self, surface, scroll_factor):
        ''' Adds a surface created elsewhere. '''
        surface = surface.convert_alpha()
        if len(self.levels) > 0:
            surface.set_colorkey((0xff, 0x00, 0xea), self.colorkey_flags)
        self.levels.append(_subsurface(surface, scroll_factor))

    def draw(self, surface):
        ''' This draws all parallax levels to the surface
            provided as argument. '''
        s_width = self.size[0]
        s_height = self.size[1]
        for lvl in self.levels:
            if self.orientation == 'vertical':
                surface.blit(lvl.surface, (0, 0),
                             (0, -lvl.scroll, s_width, s_height))
                surface.blit(lvl.surface,
                             (0, lvl.scroll - lvl.surface.get_height()))
            else:
                surface.blit(lvl.surface, (0, 0),
                             (lvl.scroll, 0, s_width, s_height))
                surface.blit(lvl.surface,
                             (lvl.surface.get_width() - lvl.scroll, 0),
                             (0, 0, lvl.scroll, s_height))

    def scroll(self, offset, orientation=None):
        '''scroll moves each surface _offset_ pixels / assigned factor'''
        if orientation is not None:
            self.orientation = orientation

        self.scroller = (self.scroller + offset)
        for lvl in self.levels:
            if self.orientation == 'vertical':
                lvl.scroll = (self.scroller / lvl.factor) \
                             % lvl.surface.get_height()
            else:
                lvl.scroll = (self.scroller / lvl.factor) \
                             % lvl.surface.get_width()

class VerticalParallaxSurface(ParallaxSurface):
    ''' Class implementing vertical scrolling parallax surface. '''
    def __init__(self, size, colorkey_flags=0):
        ParallaxSurface.__init__(self, size, colorkey_flags)
        self.orientation = 'vertical'


# === ./parallax/__init__.py ===
from . import *


# === ./a_star_optimise.py ===

import numpy as np
import heapq
from math import hypot, radians

from src.path_finding.LidarPlot import LidarPlot
from src.path_finding.new_a_star import a_star
from src.path_finding.Position import Position

positions = []
for angle in range(200, 360):
    positions.append(Position(radians(angle), 1000))

for angle in range(0, 100):
    positions.append(Position(radians(angle), 1000))


for distance in range(1000, 1500):
    positions.append(Position(radians(100), distance))


for distance in range(1000, 1600):
    positions.append(Position(radians(200), distance))

    

for distance in range(1700, 3000):
    positions.append(Position(radians(60), distance))


for distance in range(1700, 3000):
    positions.append(Position(radians(270), distance))

    
for distance in range(1000, 3000):
    positions.append(Position(radians(30), distance))


    
for angle in range(160, 200):
    positions.append(Position(radians(angle), 1000))
import pstats
import io
import cProfile




goal = Position(0, 1500)
plot = LidarPlot(range_mm=4000, debug_algo_path=True)
profiler = cProfile.Profile()
profiler.enable()


path, (padded_obstacles_debug, original_obs_debug, goal_debug, path_debug) = a_star( positions, goal)

profiler.disable()
s = io.StringIO()
sortby = 'cumulative'  # Sort by cumulative time
ps = pstats.Stats(profiler, stream=s).sort_stats(sortby)
ps.print_stats()
print(s.getvalue())  # Display profiling results


plot.plot_path(path)
plot.plot_path_debug(path_debug)
plot.plot_destination_debug(goal_debug)
plot.plot_obstacles_debug(padded_obstacles_debug,original_obs_debug)

plot.plot_obstacles(positions)
plot.plot_destination(goal)
plot.update_plot()

while True:
    continue

# === ./motoron_protocol.py ===
## \file motoron_protocol.py
##
## This file defines the arbitrary constants needed to communicate with a
## Motoron.  We do not recommend importing this package directly because
## all of the constants defined in it are available through the `motoron`
## package.
##
## \cond

CMD_GET_FIRMWARE_VERSION = 0x87
CMD_SET_PROTOCOL_OPTIONS = 0x8B
CMD_READ_EEPROM = 0x93
CMD_WRITE_EEPROM = 0x95
CMD_REINITIALIZE = 0x96
CMD_RESET = 0x99
CMD_GET_VARIABLES = 0x9A
CMD_SET_VARIABLE = 0x9C
CMD_COAST_NOW = 0xA5
CMD_CLEAR_MOTOR_FAULT = 0xA6
CMD_CLEAR_LATCHED_STATUS_FLAGS = 0xA9
CMD_SET_LATCHED_STATUS_FLAGS = 0xAC
CMD_SET_BRAKING = 0xB1
CMD_SET_BRAKING_NOW = 0xB2
CMD_SET_SPEED = 0xD1
CMD_SET_SPEED_NOW = 0xD2
CMD_SET_BUFFERED_SPEED = 0xD4
CMD_SET_ALL_SPEEDS = 0xE1
CMD_SET_ALL_SPEEDS_NOW = 0xE2
CMD_SET_ALL_BUFFERED_SPEEDS = 0xE4
CMD_SET_ALL_SPEEDS_USING_BUFFERS = 0xF0
CMD_SET_ALL_SPEEDS_NOW_USING_BUFFERS = 0xF3
CMD_RESET_COMMAND_TIMEOUT = 0xF5
CMD_MULTI_DEVICE_ERROR_CHECK = 0xF9
CMD_MULTI_DEVICE_WRITE = 0xFA

SETTING_FACTORY_RESET_CODE = 0
SETTING_DEVICE_NUMBER = 1
SETTING_ALTERNATIVE_DEVICE_NUMBER = 3
SETTING_COMMUNICATION_OPTIONS = 5
SETTING_BAUD_DIVIDER = 6
SETTING_RESPONSE_DELAY = 8

VAR_PROTOCOL_OPTIONS = 0
VAR_STATUS_FLAGS = 1
VAR_VIN_VOLTAGE = 3
VAR_COMMAND_TIMEOUT = 5
VAR_ERROR_RESPONSE = 7
VAR_ERROR_MASK = 8
VAR_JUMPER_STATE = 10
VAR_UART_FAULTS = 11

MVAR_PWM_MODE = 1
MVAR_TARGET_SPEED = 2
MVAR_TARGET_BRAKE_AMOUNT = 4
MVAR_CURRENT_SPEED = 6
MVAR_BUFFERED_SPEED = 8
MVAR_MAX_ACCEL_FORWARD = 10
MVAR_MAX_ACCEL_REVERSE = 12
MVAR_MAX_DECEL_FORWARD = 14
MVAR_MAX_DECEL_REVERSE = 16
MVAR_STARTING_SPEED_FORWARD = 18
MVAR_STARTING_SPEED_REVERSE = 20
MVAR_DIRECTION_CHANGE_DELAY_FORWARD = 22
MVAR_DIRECTION_CHANGE_DELAY_REVERSE = 23
MVAR_MAX_DECEL_TMP = 24
MVAR_CURRENT_LIMIT = 26
MVAR_CURRENT_SENSE_RAW = 28
MVAR_CURRENT_SENSE_SPEED = 30
MVAR_CURRENT_SENSE_PROCESSED = 32
MVAR_CURRENT_SENSE_OFFSET = 34
MVAR_CURRENT_SENSE_MINIMUM_DIVISOR = 35

PROTOCOL_OPTION_CRC_FOR_COMMANDS = 0
PROTOCOL_OPTION_CRC_FOR_RESPONSES = 1
PROTOCOL_OPTION_I2C_GENERAL_CALL = 2

COMMUNICATION_OPTION_7BIT_RESPONSES = 0
COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER = 1
COMMUNICATION_OPTION_ERR_IS_DE = 2

STATUS_FLAG_PROTOCOL_ERROR = 0
STATUS_FLAG_CRC_ERROR = 1
STATUS_FLAG_COMMAND_TIMEOUT_LATCHED = 2
STATUS_FLAG_MOTOR_FAULT_LATCHED = 3
STATUS_FLAG_NO_POWER_LATCHED = 4
STATUS_FLAG_UART_ERROR = 5
STATUS_FLAG_RESET = 9
STATUS_FLAG_COMMAND_TIMEOUT = 10
STATUS_FLAG_MOTOR_FAULTING = 11
STATUS_FLAG_NO_POWER = 12
STATUS_FLAG_ERROR_ACTIVE = 13
STATUS_FLAG_MOTOR_OUTPUT_ENABLED = 14
STATUS_FLAG_MOTOR_DRIVING = 15

UART_FAULT_FRAMING = 0
UART_FAULT_NOISE = 1
UART_FAULT_HARDWARE_OVERRUN = 2
UART_FAULT_SOFTWARE_OVERRUN = 3

ERROR_RESPONSE_COAST = 0
ERROR_RESPONSE_BRAKE = 1
ERROR_RESPONSE_COAST_NOW = 2
ERROR_RESPONSE_BRAKE_NOW = 3

PWM_MODE_DEFAULT = 0
PWM_MODE_1_KHZ = 1
PWM_MODE_2_KHZ = 2
PWM_MODE_4_KHZ = 3
PWM_MODE_5_KHZ = 4
PWM_MODE_10_KHZ = 5
PWM_MODE_20_KHZ = 6
PWM_MODE_40_KHZ = 7
PWM_MODE_80_KHZ = 8

JMP1_INSTALLED = 0
JMP1_NOT_INSTALLED = 1

CLEAR_MOTOR_FAULT_UNCONDITIONAL = 0

ERROR_CHECK_CONTINUE = 0x3C
ERROR_CHECK_DONE = 0x00

MAX_SPEED = 800
MAX_ACCEL = 6400
MAX_DIRECTION_CHANGE_DELAY = 250

LATCHED_STATUS_FLAGS = 0x03FF
MAX_ERROR_MASK = 0x07FF

MAX_COMMAND_TIMEOUT = 16250

MIN_BAUD_RATE = 245
MAX_BAUD_RATE = 1000000

CRC_TABLE = [
  0x00, 0x41, 0x13, 0x52, 0x26, 0x67, 0x35, 0x74,
  0x4c, 0x0d, 0x5f, 0x1e, 0x6a, 0x2b, 0x79, 0x38,
  0x09, 0x48, 0x1a, 0x5b, 0x2f, 0x6e, 0x3c, 0x7d,
  0x45, 0x04, 0x56, 0x17, 0x63, 0x22, 0x70, 0x31,
  0x12, 0x53, 0x01, 0x40, 0x34, 0x75, 0x27, 0x66,
  0x5e, 0x1f, 0x4d, 0x0c, 0x78, 0x39, 0x6b, 0x2a,
  0x1b, 0x5a, 0x08, 0x49, 0x3d, 0x7c, 0x2e, 0x6f,
  0x57, 0x16, 0x44, 0x05, 0x71, 0x30, 0x62, 0x23,
  0x24, 0x65, 0x37, 0x76, 0x02, 0x43, 0x11, 0x50,
  0x68, 0x29, 0x7b, 0x3a, 0x4e, 0x0f, 0x5d, 0x1c,
  0x2d, 0x6c, 0x3e, 0x7f, 0x0b, 0x4a, 0x18, 0x59,
  0x61, 0x20, 0x72, 0x33, 0x47, 0x06, 0x54, 0x15,
  0x36, 0x77, 0x25, 0x64, 0x10, 0x51, 0x03, 0x42,
  0x7a, 0x3b, 0x69, 0x28, 0x5c, 0x1d, 0x4f, 0x0e,
  0x3f, 0x7e, 0x2c, 0x6d, 0x19, 0x58, 0x0a, 0x4b,
  0x73, 0x32, 0x60, 0x21, 0x55, 0x14, 0x46, 0x07,
  0x48, 0x09, 0x5b, 0x1a, 0x6e, 0x2f, 0x7d, 0x3c,
  0x04, 0x45, 0x17, 0x56, 0x22, 0x63, 0x31, 0x70,
  0x41, 0x00, 0x52, 0x13, 0x67, 0x26, 0x74, 0x35,
  0x0d, 0x4c, 0x1e, 0x5f, 0x2b, 0x6a, 0x38, 0x79,
  0x5a, 0x1b, 0x49, 0x08, 0x7c, 0x3d, 0x6f, 0x2e,
  0x16, 0x57, 0x05, 0x44, 0x30, 0x71, 0x23, 0x62,
  0x53, 0x12, 0x40, 0x01, 0x75, 0x34, 0x66, 0x27,
  0x1f, 0x5e, 0x0c, 0x4d, 0x39, 0x78, 0x2a, 0x6b,
  0x6c, 0x2d, 0x7f, 0x3e, 0x4a, 0x0b, 0x59, 0x18,
  0x20, 0x61, 0x33, 0x72, 0x06, 0x47, 0x15, 0x54,
  0x65, 0x24, 0x76, 0x37, 0x43, 0x02, 0x50, 0x11,
  0x29, 0x68, 0x3a, 0x7b, 0x0f, 0x4e, 0x1c, 0x5d,
  0x7e, 0x3f, 0x6d, 0x2c, 0x58, 0x19, 0x4b, 0x0a,
  0x32, 0x73, 0x21, 0x60, 0x14, 0x55, 0x07, 0x46,
  0x77, 0x36, 0x64, 0x25, 0x51, 0x10, 0x42, 0x03,
  0x3b, 0x7a, 0x28, 0x69, 0x1d, 0x5c, 0x0e, 0x4f,
]

## \endcond

def calculate_crc(buffer):
  """
  This method calculates the 7-bit CRC needed for a Motoron
  command or response.  Most users will not need to use this, since most
  methods in this library automatically append a CRC byte or check
  received CRC bytes when appropriate.
  """
  crc = 0
  for byte in buffer:
    crc = CRC_TABLE[crc ^ byte]
  return crc



# === ./tile sets/SpaceGame.py ===
# Import libraries
import pygame, sys
from pygame.locals import *
import random, time
from os import path
sys.path.append ("../")
from parallax import parallax

# Initializing
pygame.mixer.pre_init(48000, -16, 8, 8192)# initialise the music and sound mixer
pygame.init() # initialise pygame
pygame.mouse.set_visible(0)
WIDTH = 1024
HEIGHT = 768
entity=None

# Create Graphics Window of Width and Height call it screen
screen = pygame.display.set_mode((WIDTH, HEIGHT), pygame.DOUBLEBUF)

music_volume=0.5
pygame.mixer.music.set_volume(music_volume)
clock = pygame.time.Clock()
explosion_finished=False
shield_kill=False

POWERUP_TIME =6000
Boss_died_anim_finished=False
boss_explode_inprogress=False
Boss_level_triggered=False
explosion_time=0

orientation = 'vertical'
speed=4

bg = parallax.ParallaxSurface((1024, 768), pygame.RLEACCEL)
bg.add('bkgd_0.png', 6)
bg.add('bkgd_1.png', 5)
bg.add('bkgd_2.png', 4)
bg.add('bkgd_4.png', 3)
bg.add('bkgd_6.png', 2)
bg.add('bkgd_7.png', 1)

# Creating colors
BLUE  = (0, 0, 255)
RED   = (255, 0, 0)
GREEN = (0, 255, 0)
BLACK = (0, 0, 0)
WHITE = (255, 255, 255)

# Initialise variables for zoom_player function
x=0
y=0
zoom_speed=8
t_ref = 0
col1 =(255,244,11)
col2 =(0,255,3)
col3 =(255,153,33)
col4 =(58,0,239)
col5 =(175,0,231)
cols = [col1,col2,col3,col4,col5]

# Define sound objects
bullet_sound = pygame.mixer.Sound("Sound\\Laser.wav")
crash_sound = pygame.mixer.Sound("Sound\\Explosion.wav")  
boss_hit_sound = pygame.mixer.Sound("Sound\\Boss_hit_sound.wav")
enemy_hit = pygame.mixer.Sound("Sound\\Hit_Enemy.wav")
player_hit = pygame.mixer.Sound("Sound\\expl6.wav")
boss_bullet_sound = pygame.mixer.Sound("Sound\\Boss_bullet.wav")
game_over_message = pygame.mixer.Sound("Sound\\Game_over.wav")
powerup_shields = pygame.mixer.Sound("Sound\\Powerup_shields.wav")
bolt_powerup = pygame.mixer.Sound("Sound\\Bolt_powerup.wav")
boss_explode_sound = pygame.mixer.Sound("Sound\\BIG_explosion.wav")
chris_getready = pygame.mixer.Sound("Sound\\Chris_getready.wav")
owwyeah = pygame.mixer.Sound("Sound\\Oh_yeah.wav")
evil_laugh = pygame.mixer.Sound("Sound\\Evil_laugh.wav")

# Define graphic objects
player_ship = pygame.image.load("Player\\Ship2.png")
enemy_ships = pygame.image.load("Enemy\\Zorg.png")
boss_monster = pygame.image.load("Boss\\Boss1.png")
player_shield = pygame.image.load("Player\\shield.png")
sheet = pygame.image.load('Explosion\\Explosion.png')
Background = pygame.image.load('bkgd_0.png')

img_dir=("Powerups")
powerup_images = {}
powerup_images['shield'] = pygame.image.load(path.join(img_dir, 'shield_powerup.png'))
powerup_images['power'] = pygame.image.load(path.join(img_dir, 'bolt.png'))
player_mini_img = pygame.transform.scale(player_ship, (35, 29))


# -----------------------------------------------------------------------------------------------------------------------------
# --------------------------------- Load Animated sprites into lists ----------------------------------------------------------
# -----------------------------------------------------------------------------------------------------------------------------

# loading the explosion images into lists using filename of images with 0-8 on the end

img_dir = ("Explosion") 
explosion_anim = {}
explosion_anim['lg'] = []
explosion_anim['sm'] = []
explosion_anim['player'] = []
for i in range(9):
    filename = 'regularExplosion0{}.png'.format(i)
    img = pygame.image.load(path.join(img_dir, filename)).convert()
    img.set_colorkey(BLACK)
    img_lg = pygame.transform.scale(img, (75, 75))
    explosion_anim['lg'].append(img_lg)
    img_sm = pygame.transform.scale(img, (32, 32))
    explosion_anim['sm'].append(img_sm)
    filename = 'sonicExplosion0{}.png'.format(i)
    img = pygame.image.load(path.join(img_dir, filename)).convert()
    img.set_colorkey(BLACK)
    explosion_anim['player'].append(img)

# loading the BOSS images into lists using filename of images with 0-7 on the end

boss_dir = ("Boss")  
eye_anim = {}
eye_anim['boss'] = []
for i in range(8):
    filename = 'Boss{}.png'.format(i)
    img = pygame.image.load(path.join(boss_dir, filename))
    eye_anim['boss'].append(img)

# loading BIG BOSS EXPLOSION into lists

size = 192,192
pos = 0,0
len_sprt_x,len_sprt_y = size                                         #sprite size
sprt_rect_x,sprt_rect_y = pos                                        #where to find first sprite on sheet
sheet = pygame.image.load('Explosion\\Explosion.png').convert_alpha() #Load the sheet
sheet_rect = sheet.get_rect()
msprites = []   

for i in range(0,22):#columns
    sheet.set_clip(pygame.Rect(sprt_rect_x, sprt_rect_y, len_sprt_x, len_sprt_y)) #find sprite you want
    sprite = sheet.subsurface(sheet.get_clip())                                   #grab the sprite you want
    msprites.append(sprite)
    sprt_rect_x += len_sprt_x
bexplosion = msprites

#Setting up Frames Per Second 
FPS = 60
FramePerSec = pygame.time.Clock()
 
# Other Variables for use in the program
score = 0
enemy_speedx = 1.2
enemy_speedy = 1
boss_speedx = 1
boss_speedy = 1
direction = 1
scroll_speed = 2

# Bullet settings
bullet_speed = -20
boss_bullet_speed = 12
bullet_width = 4
bullet_height = 16
bullet_color = (255, 255, 255)
bullets_allowed = 3

#Setting up Fonts
font = pygame.font.SysFont("Verdana", 60)
font_small = pygame.font.SysFont("Verdana", 20)
game_over_text = font.render("Game Over", True, BLACK)

# Set screen caption
pygame.display.set_caption("Chris's Crazy Space Game")

# Setting up the background for vertical scrolling space effect so it looks like your moving through space! TRICKY TRICKY!
# bg = pygame.image.load(os.path.join("Background",'bg2.png')).convert()
# bg_rect = bg.get_rect()
# bgY = 0 # bgY cordinate is at the top of screen
# bgY2 = bg.get_height() * -1 # the bgY2 coordinate is a whole screen above the visable area

# -----------------------------------------------------------------------------------------------------------------------------
# ------------------------------------ Class Definitions for all the major sprites --------------------------------------------
# -----------------------------------------------------------------------------------------------------------------------------

class Powerups(pygame.sprite.Sprite):
    def __init__(self, center):
        pygame.sprite.Sprite.__init__(self)
        self.type = random.choice(['shield', 'power'])
        self.image = powerup_images[self.type]
        self.rect = self.image.get_rect()
        self.rect.center = center
        self.speedy = 5

    def move(self):
        self.rect.y += self.speedy
        # kill if it moves off the bottom of the screen
        if self.rect.top > HEIGHT:
            self.kill()    
    
# Class function to draw 1st BOSS bullet and move them
class Boss_Bullet1(pygame.sprite.Sprite):
    def __init__(self, x, y):
        pygame.sprite.Sprite.__init__(self)
        self.image = pygame.Surface((bullet_width, bullet_height))
        self.image.fill(bullet_color)
        self.rect = self.image.get_rect()
        self.rect.bottom = y
        self.rect.centerx = x 
        self.speedy = boss_bullet_speed # boss bullet speed is positive int so bullet moves DOWN the screen

    def move(self):
        self.rect.y += self.speedy
        # kill if it moves off the BOTTOM of the screen
        if self.rect.bottom > HEIGHT:
            self.kill()
            
# Class function to draw 2nd BOSS bullet and move them
class Boss_Bullet2(pygame.sprite.Sprite):
    def __init__(self, x, y):
        pygame.sprite.Sprite.__init__(self)
        self.image = pygame.Surface((bullet_width, bullet_height))
        self.image.fill(bullet_color)
        self.rect = self.image.get_rect()
        self.rect.bottom = y + 64   # move start position of second boss bullet to the LEFT WING of the BOSS
        self.rect.centerx = x -116  # move start position of second boss bullet to the LEFT WING of the BOSS
        self.speedy = boss_bullet_speed # boss bullet speed is positive int so bullet moves DOWN the screen

    def move(self):
        self.rect.y += self.speedy
        # kill if it moves off the BOTTOM of the screen
        if self.rect.bottom > HEIGHT:
            self.kill()            
            
# Class function to draw 3rd BOSS bullet and move them
class Boss_Bullet3(pygame.sprite.Sprite):
    def __init__(self, x, y):
        pygame.sprite.Sprite.__init__(self)
        self.image = pygame.Surface((bullet_width, bullet_height))
        self.image.fill(bullet_color)
        self.rect = self.image.get_rect()
        self.rect.bottom = y + 64   # move start position of third boss bullet to the RIGHT WING of the BOSS
        self.rect.centerx = x +116  # move start position of third boss bullet to the RIGHT WING of the BOSS
        self.speedy = boss_bullet_speed # boss bullet speed is positive int so bullet moves DOWN the screen

    def move(self):
        self.rect.y += self.speedy
        # kill if it moves off the BOTTOM of the screen
        if self.rect.bottom > HEIGHT:
            self.kill()                        
 
# Class function to draw bullets and move them
class Bullet(pygame.sprite.Sprite):
    def __init__(self, x, y):
        pygame.sprite.Sprite.__init__(self)
        self.image = pygame.Surface((bullet_width, bullet_height))
        self.image.fill(bullet_color)
        self.rect = self.image.get_rect()
        self.rect.bottom = y
        self.rect.centerx = x 
        self.speedy = bullet_speed  # bullet speed is negative int so bullet moves UP the screen

    def move(self):
        self.rect.y += self.speedy
        # kill if it moves off the TOP of the screen
        if self.rect.bottom < 0:
            self.kill()
            
# a class  function to draw the ememies, move them
class Enemy(pygame.sprite.Sprite):
    def __init__(self):
        pygame.sprite.Sprite.__init__(self)
        self.image = enemy_ships
        self.rect = self.image.get_rect()
        self.radius = int(self.rect.width * .75 / 2) # trying to make a hit box of correct size for enemy need HELP with this
        #pygame.draw.circle(self.image, RED, self.rect.center, self.radius)
        self.rect.x = random.randrange(WIDTH - self.rect.width)
        self.rect.y = random.randrange(-100, -40)
        self.speedy = random.randrange(1, 8)
        self.speedx = random.randrange(-3, 3)
        self.last_update = pygame.time.get_ticks()

    def move(self): # moves the ememies down and randomly to the left or right
        self.rect.x += self.speedx
        self.rect.y += self.speedy + enemy_speedx
        if self.rect.top > HEIGHT + 10 or self.rect.left < -25 or self.rect.right > WIDTH + 20:
            self.rect.x = random.randrange(WIDTH - self.rect.width)
            self.rect.y = random.randrange(-100, -40)
            self.speedy = random.randrange(1, 8)

# a class  function to draw the BOSS MONSTER, move it
class Boss(pygame.sprite.Sprite):
    def __init__(self, center, size):     
        pygame.sprite.Sprite.__init__(self)
        self.size = size
        self.image = eye_anim[self.size][0]
        self.rect = self.image.get_rect()
        self.rect.center = center
        self.frame = 0
        self.radius = int(self.rect.width * .68 / 2) # trying to make a hit box of correct size for enemy need HELP with this
        #pygame.draw.circle(self.image, RED, self.rect.center, self.radius)
        self.rect.x = random.randrange(WIDTH - self.rect.width)
        self.rect.y = 100
        self.speedy = boss_speedy
        self.speedx = boss_speedx
        self.shoot_delay = 140
        self.last_shot = pygame.time.get_ticks()
        self.last_update = pygame.time.get_ticks()
        self.frame_rate = 25
        self.life = 100

    def move(self): # moves the ememies down and randomly to the left or right
        if not boss_explode_inprogress:
            now = pygame.time.get_ticks()
            if now - self.last_update > self.frame_rate:
                self.last_update = now
                self.frame += 1
                if self.frame == len(eye_anim[self.size]):
                    self.frame=0
                else:
                    #center = self.rect.center
                    self.image = eye_anim[self.size][self.frame]
                    #self.rect = self.image.get_rect()
                    #self.rect.center = center  
            self.rect.x += self.speedx
            self.rect.y += self.speedy 
            if self.rect.left < 0 or self.rect.right > WIDTH :
                #self.rect.x = random.randrange(WIDTH - 100)
                #self.rect.y = random.randrange(100, 200)
                #self.speedy = 0
                self.speedx = self.speedx * -1
            if self.rect.top < 14 or self.rect.bottom > HEIGHT -130:
                self.speedy = self.speedy * -1
            if self.rect.top < 14:
                self.rect.y +=1
            if self.rect.bottom > HEIGHT -130:
                self.rect.y -=1
            reverse_chance =random.randrange(1,100)
            if reverse_chance >=99:
                self.speedx = self.speedx * -1
            if reverse_chance <=1:
                self.speedy = self.speedy * -1
            bullet_chance = random.randrange(1,100)
            if bullet_chance >93:   # fire a bullet!
                boss_bullet_sound.set_volume(0.5)
                #boss_bullet_sound.play()
                Big_boss.shoot()
             
    def shoot(self):  # create a new bullet
        Bnow = pygame.time.get_ticks()
        if Bnow - self.last_shot > self.shoot_delay:
            self.last_shot = Bnow
            limit_fire=random.randrange(1,100)
            if limit_fire<31:
                boss_bullet_sound.play()
                B_bullet1 = Boss_Bullet1(self.rect.centerx, self.rect.centery)   #shoot 1st Bullet
                all_sprites.add(B_bullet1)
                boss_bullet_list.add(B_bullet1)
            if limit_fire>30 and limit_fire<61:
                boss_bullet_sound.play()
                B_bullet2 = Boss_Bullet2(self.rect.centerx, self.rect.centery)   #shoot 2nd bullet
                all_sprites.add(B_bullet2)
                boss_bullet_list.add(B_bullet2)
            if limit_fire>60 and limit_fire<93:
                boss_bullet_sound.play() 
                B_bullet3 = Boss_Bullet3(self.rect.centerx, self.rect.centery)   #shoot 3rd bullet
                all_sprites.add(B_bullet3)
                boss_bullet_list.add(B_bullet3)
            if limit_fire>93:
                boss_bullet_sound.play()
                evil_laugh.play()
                B_bullet3 = Boss_Bullet3(self.rect.centerx, self.rect.centery)   #shoot 3rd bullet
                B_bullet1 = Boss_Bullet1(self.rect.centerx, self.rect.centery)   #shoot 1st Bullet
                all_sprites.add(B_bullet1)
                boss_bullet_list.add(B_bullet1) 
                all_sprites.add(B_bullet3)
                boss_bullet_list.add(B_bullet3)
            
# a class function to draw the player and move, as well as start the shoot bullet process if the space bar is pressed

class Player(pygame.sprite.Sprite):
    def __init__(self):
        pygame.sprite.Sprite.__init__(self)
        self.image = player_ship
        self.surf = pygame.Surface((140, 98))
        self.rect = self.image.get_rect(center =((WIDTH/2)-46, HEIGHT - 40))
        self.radius = 40   # trying to make a hit box of correct size for player need HELP with this
        #pygame.draw.circle(screen, RED, self.rect.center, self.radius)
        self.speedx = 0
        self.shield = 100
        self.shoot_delay = 250
        self.last_shot = pygame.time.get_ticks()
        self.lives = 3
        self.hidden = False
        self.power = 1
        self.fire_now = pygame.time.get_ticks()
        self.power_time = pygame.time.get_ticks()
        self.hide_timer = pygame.time.get_ticks()
        
    def move(self): # move the player left and right
        # unhide if hidden
        if self.hidden and pygame.time.get_ticks() - self.hide_timer > 1000:
            self.hidden = False
            self.rect.centerx = ((WIDTH /2)-46)
            self.rect.bottom = HEIGHT
        self.speedx = 0
        pressed_keys = pygame.key.get_pressed()
            
        if self.rect.left > 0:
              if pressed_keys[K_LEFT]:
                  self.speedx = -5
        if self.rect.right < WIDTH:        
              if pressed_keys[K_RIGHT]:
                  self.speedx = 5
        if pressed_keys[K_SPACE] and (pygame.time.get_ticks() - self.fire_now) >300:   # fire a bullet!
              self.fire_now = pygame.time.get_ticks()
              P1.shoot()      
        self.rect.x += self.speedx
        # timeout for powerups
        if self.power >= 2 and pygame.time.get_ticks() - self.power_time > POWERUP_TIME:
            self.power -= 1
            self.power_time = pygame.time.get_ticks()
            
    def zoom(self):
        x=0
        y=0
        Dspeed=zoom_speed
        for i in range(0, int((HEIGHT/Dspeed)/5)):
            for i in range(len(cols)):
                color=cols[i]
                pygame.draw.line(screen, (color), ((WIDTH/2),HEIGHT), ((WIDTH/2),HEIGHT-y), 32)
                self.rect.x = int(WIDTH/2)-46
                if self.rect.y>-70:
                    screen.blit(P1.image, P1.rect)
                    time.sleep(0.0005)
                    self.rect.y -=8
                pygame.display.flip()
                bg.draw(screen)
                y=y+Dspeed
        x=0
        y=0
        Dspeed =+10
        for i in range(0, int(WIDTH/Dspeed/2)):
            pygame.draw.line(screen, (color), (512-x,HEIGHT), (512-x,0), 5)
            pygame.draw.line(screen, (color), (512+x,HEIGHT), (512+x,0), 5)
            pygame.display.flip()
            bg.draw(screen)
            x=x+Dspeed
        x=0
        y=0
        for i in range(0, int(WIDTH/Dspeed/2)):
            pygame.draw.line(screen, (color), (0+x,HEIGHT), (0+x,0), 5)
            pygame.draw.line(screen, (color), (WIDTH-x,HEIGHT), (WIDTH-x,0), 5)
            pygame.display.flip()
            bg.draw(screen)
            x=x+Dspeed
        self.rect.y = HEIGHT - 76   
        x=0
        y=0
        Dspeed=zoom_speed
        
    def shoot(self):
        now = pygame.time.get_ticks()
        if now - self.last_shot > self.shoot_delay:
            self.last_shot = now
            if self.power == 1:
                bullet = Bullet(self.rect.centerx, self.rect.top+26)
                all_sprites.add(bullet)
                bullet_list.add(bullet)
                bullet_sound.set_volume(0.7) 
                bullet_sound.play()
            if self.power >= 2:
                bullet_sound.set_volume(0.7) 
                bullet_sound.play()
                bullet1 = Bullet(self.rect.left+7, self.rect.centery+26)
                bullet2 = Bullet(self.rect.right-7, self.rect.centery+26)
                all_sprites.add(bullet1)
                all_sprites.add(bullet2)
                bullet_list.add(bullet1)
                bullet_list.add(bullet2)
                #sound.play()
            if self.power >= 3:
                bullet_sound.set_volume(0.7) 
                bullet_sound.play()
                bullet1 = Bullet(self.rect.left+7, self.rect.centery+26)
                bullet2 = Bullet(self.rect.right-7, self.rect.centery+26)
                bullet3 = Bullet(self.rect.centerx, self.rect.top+26)
                all_sprites.add(bullet1)
                all_sprites.add(bullet2)
                all_sprites.add(bullet3)
                bullet_list.add(bullet1)
                bullet_list.add(bullet2)
                bullet_list.add(bullet3)
 
    def powerup(self):
        self.power += 1
        self.power_time = pygame.time.get_ticks()

    def hide(self):
        # hide the player temporarily
        self.hidden = True
        self.hide_timer = pygame.time.get_ticks()
        self.rect.center = (WIDTH +250, HEIGHT + 250)
         
  # a class function to draw the player SHIELD and move 
class Shield(pygame.sprite.Sprite):
    def __init__(self, center):
        pygame.sprite.Sprite.__init__(self)
        self.image = player_shield
        self.rect = self.image.get_rect()
        self.rect.center = center
        self.surf = pygame.Surface((140, 68))
        self.radius = 40   # trying to make a hit box of correct size for player need HELP with this
        #pygame.draw.circle(screen, RED, self.rect.center, self.radius)
        self.speedx = 0
        self.rect.x = (P1.rect.x -10 )
       
    def move(self): # move the shield left and right
        self.rect.x = (P1.rect.x -10 )    # -------------  make sure the shield appears in the same position as the player's ship
        if shield_kill:
            all_sprites.remove(P1_shield)
            shield_sprites.remove(P1_shield)
            self.kill()

class Explosion(pygame.sprite.Sprite):
    def __init__(self, center, size):
        pygame.sprite.Sprite.__init__(self)
        self.size = size
        self.image = explosion_anim[self.size][0]      # set first frame in animation sequence
        self.rect = self.image.get_rect()
        self.rect.center = center
        self.frame = 0
        self.last_update = pygame.time.get_ticks()
        self.frame_rate = 40

    def move(self):
        now = pygame.time.get_ticks()
        if now - self.last_update > self.frame_rate:
            self.last_update = now
            self.frame += 1                                    # goto the next frame
            if self.frame == len(explosion_anim[self.size]):
                # if the animation has reached the last frame the remove sprite from all lists
                shield_kill = True
                if self.size == 'sm' or self.size == 'player' and len(shield_sprites)>0:
                    all_sprites.remove(P1_shield)
                    P1_shield.kill()
                self.kill()
            else:
                center = self.rect.center
                self.image = explosion_anim[self.size][self.frame]
                self.rect = self.image.get_rect()
                self.rect.center = center
                
class Boss_explosion(pygame.sprite.Sprite):
    def __init__(self, center):
        pygame.sprite.Sprite.__init__(self)
        self.image = bexplosion[0]      # set first frame in animation sequence
        self.rect = self.image.get_rect()
        self.rect.center = center
        self.frame = 0
        self.last_update = pygame.time.get_ticks()
        self.frame_rate = 30
        

    def move(self):
        now = pygame.time.get_ticks()
        if now - self.last_update > self.frame_rate:
            self.last_update = now
            self.frame += 1                                    # goto the next frame
            time.sleep(0.005)
            if self.frame == len(bexplosion):
                # if the animation has reached the last frame the remove sprite from all lists
                Boss_died_anim_finished=True
                
                self.kill()
                all_sprites.remove(Big_boss)
                enemy_boss.remove(Big_boss)
            else:
                center = self.rect.center
                Boss_died_anim_finished=False
                self.image = bexplosion[self.frame]
                self.rect = self.image.get_rect()
                self.rect.center = center       

# -----------------------------------------------------------------------------------------------------------------------------
# -------------------------------------------------------- END of Class Definitions -------------------------------------------
# -----------------------------------------------------------------------------------------------------------------------------

def do_big_explosion():
    boss_explode_sound.set_volume(1)
    boss_explode_sound.play()
    boss_explode_inprogress=True
    Big_boss.life = 100
    XX = hit.rect.center[0]
    YY = hit.rect.center[1]
    YY=YY-60
    ZZZ= (XX,YY)
    ZZ = (XX,YY+192)
    XX=XX-192                            # multiple explosions in middle and below 6 explosions in all
    AA= (XX,YY)
    BB= (XX,YY+192)
    XX=XX+384
    CC=(XX,YY)
    DD=(XX,YY+192)
    Huge_expl1 = Boss_explosion(ZZZ)
    Huge_expl2 = Boss_explosion(ZZ)
    Huge_expl3 = Boss_explosion(AA)
    Huge_expl4 = Boss_explosion(BB)
    Huge_expl5 = Boss_explosion(CC)
    Huge_expl6 = Boss_explosion(DD)
    all_sprites.add(Huge_expl1)
    all_sprites.add(Huge_expl2)
    all_sprites.add(Huge_expl3)
    all_sprites.add(Huge_expl4)
    all_sprites.add(Huge_expl5)
    all_sprites.add(Huge_expl6)
    myexpl.add(Huge_expl1)
    myexpl.add(Huge_expl2)
    myexpl.add(Huge_expl3)
    myexpl.add(Huge_expl4)
    myexpl.add(Huge_expl5)
    myexpl.add(Huge_expl6)
    myexpl.draw(screen)
                   

# A Function to draw text
font_name = pygame.font.match_font('arial')
def draw_text(surf, text, size, x, y, text_color):
    font = pygame.font.Font(font_name, size)
    text_surface = font.render(text, True, text_color)
    text_rect = text_surface.get_rect()
    text_rect.midtop = (x, y)
    surf.blit(text_surface, text_rect)

# function to draw 2 backgrounds one above the visible area            
#def DrawBackground(): 
#    screen.blit(bg, (0,bgY))   # draws our first bg image
#    screen.blit(bg, (0,bgY2))  # draws the seconf bg image

# function to show the start screen with instructions
def show_go_screen():
    #screen.blit(bg, bg_rect)
    pygame.mixer.music.load('Sound\Background_music.mp3')   # loads BACKGROUND MUSIC
    pygame.mixer.music.set_volume(music_volume)
    pygame.mixer.music.play(-1, 1000)   # Plays background music in continous loop
    bg.draw(screen)
    draw_text(screen, "Chris' Crazy Space Game!", 84, WIDTH / 2, HEIGHT * 0.1, WHITE)
    draw_text(screen, "Arrow keys move, space to fire", 48, WIDTH / 2, HEIGHT * 0.4, WHITE)
    draw_text(screen, "PRESS  S  KEY TO START", 32, WIDTH / 2, HEIGHT * 0.6, WHITE)
    draw_text(screen, "PRESS  Q  KEY TO QUIT", 28, WIDTH / 2, HEIGHT * 0.85, WHITE)
    pygame.display.update()
    waiting = True
    while waiting:
        clock.tick(FPS)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                sys.exit()
            if event.type == pygame.KEYUP and event.key == pygame.K_q :
                pygame.quit()
                sys.exit()
            if event.type == pygame.KEYUP and event.key == pygame.K_s :
                waiting = False
def new_game():
    game_over = False
    boss_explode_inprogress=False
    enemy_speedx = 1.2
    enemy_speedy = 1
    boss_speedx = 1
    boss_speedy = 1
    enemies = pygame.sprite.Group()      # create a group of sprites that include all the ememies
    boss_bullet_list = pygame.sprite.Group()
    bullet_list = pygame.sprite.Group()  # create a group of sprites for the bullets
    all_sprites = pygame.sprite.Group()  # create a group of sprites that include all the sprites from player, enemies and bullets
    enemy_boss = pygame.sprite.Group()
    shield_sprites = pygame.sprite.Group()
    powerups = pygame.sprite.Group()
    myexpl = pygame.sprite.Group()
    P1 = Player()
    all_sprites.add(P1)
    P1_shield = Shield(P1.rect.center)
    for i in range(8):   # create 8 new enemies
        new_enemy()
    score = 0            # reset score back to 0 
    shield = 100         # reset shields back to 100
    P1.lives = 3         # set lives back to 3
    direction = 1        # set scroll direction back to normal
    scroll_speed = 2     # set scroll speed back to normal
    speed = scroll_speed * direction
    Boss_level_triggered=False
    musical_volume=0.5

def get_ready():
    #screen.blit(bg, bg_rect)
    chris_getready.play()
    bg.draw(screen)
    pygame.display.update()
    draw_text(screen, "Get Ready", 84, WIDTH / 2, HEIGHT * 0.4, WHITE)
    waiting = True   
    pygame.display.update()
    time.sleep(1.5)
    
def super_zoom():
    #screen.blit(bg, bg_rect)
    bg.draw(screen)
    pygame.display.update()
    P1.zoom()
    #bg.scroll(speed, orientation)
    #t = pygame.time.get_ticks()
    bg.draw(screen)
    pygame.display.update()
    time.sleep(0.5)    
    
def show_boss_level():
    #screen.blit(bg, bg_rect)
    bg.draw(screen)
    draw_text(screen, "BOSS LEVEL", 84, WIDTH / 2, HEIGHT * 0.3, WHITE)
    draw_text(screen, "Get Ready", 48, WIDTH / 2, HEIGHT * 0.6, WHITE)
    waiting = True   
    pygame.display.update()
    pygame.mixer.music.load('Sound\\Boss_music2.mp3')
    pygame.mixer.music.set_volume(music_volume)
    pygame.mixer.music.play(-1, 1000)
    time.sleep(2)
    
# function to draw the shield bar at the top middle of the screen
def draw_boss_bar(surf, x, y, pct):
    if pct < 0:
        pct = 0
    BAR_LENGTH = 100
    BAR_HEIGHT = 15
    fill = (pct / 100) * BAR_LENGTH
    outline_rect = pygame.Rect(x, y, BAR_LENGTH, BAR_HEIGHT)
    fill_rect = pygame.Rect(x, y, fill, BAR_HEIGHT)
    pygame.draw.rect(surf, RED, fill_rect)
    pygame.draw.rect(surf, WHITE, outline_rect, 2)

# function to draw the shield bar at the top middle of the screen
def draw_shield_bar(surf, x, y, pct):
    if pct < 0:
        pct = 0
    BAR_LENGTH = 100
    BAR_HEIGHT = 15
    fill = (pct / 100) * BAR_LENGTH
    outline_rect = pygame.Rect(x, y, BAR_LENGTH, BAR_HEIGHT)
    fill_rect = pygame.Rect(x, y, fill, BAR_HEIGHT)
    pygame.draw.rect(surf, GREEN, fill_rect)
    pygame.draw.rect(surf, WHITE, outline_rect, 2)
    
# function to draw the player lives on the right of the screen
def draw_lives(surf, x, y, lives, img):
    for i in range(lives):
        img_rect = img.get_rect()
        img_rect.x = x + 30 * i
        img_rect.y = y
        surf.blit(img, img_rect)

# Setting up Sprites creating the player ship        
P1 = Player()  # creates a player ship

# Creating Sprites Groups
enemies = pygame.sprite.Group() # create a group of sprites that include all the ememies
boss_bullet_list = pygame.sprite.Group()
bullet_list = pygame.sprite.Group()  # create a group of sprites for the bullets
all_sprites = pygame.sprite.Group()  # create a group of sprites that include all the sprites from player, enemies and bullets
enemy_boss = pygame.sprite.Group()
shield_sprites = pygame.sprite.Group()
powerups = pygame.sprite.Group()
myexpl = pygame.sprite.Group()

all_sprites.add(P1) # adds players ship to all sprites group
P1_shield = Shield(P1.rect.center) # spawns shield so we don't have definition problems later.

# a function to create a new ememy sprite    
def new_enemy():
    e = Enemy()
    all_sprites.add(e)
    enemies.add(e)
          
for i in range(8):          # create 8 new enemies and place them in the sprite groups
    new_enemy()

# Adding a new User event 
INC_SPEED = pygame.USEREVENT + 1 # creates new EVENT called INC_SPEED and gives it a new ID 
pygame.time.set_timer(INC_SPEED, 2000)    # creates the pygame event called INC_SPEED every 2000 miliseconds this will allow us to slowly increase the speed of enemies



#  ---------------------------------------------------------------------------------------------------------------------------

# Main Game Loop
show_go_screen()
get_ready()
game_over = False        
main_loop = True
shield=100

while main_loop:
    
    if game_over:
        show_go_screen()
        game_over = False
        boss_explode_inprogress=False
        all_sprites = pygame.sprite.Group()
        enemies = pygame.sprite.Group()
        bullet_list = pygame.sprite.Group()
        powerups = pygame.sprite.Group()
        enemy_speedx = 1.2
        enemy_speedy = 1
        boss_speedx = 1
        boss_speedy = 1
        P1 = Player()
        all_sprites.add(P1)
        for i in range(8):   # create 8 new enemies
            new_enemy()
        score = 0        # reset score back to 0 
        shield = 100     # reset shields back to 100
        P1.lives = 3     # set lives back to 3
        direction = 1    # set scroll direction back to normal 
        scroll_speed = 2
        speed = scroll_speed * direction   # Set scroll speed back to normal
        Boss_level_triggered=False
        musical_volume=0.5
        get_ready()
    bg.scroll(speed, orientation)
    t = pygame.time.get_ticks()
    bg.draw(screen)
    #DrawBackground() # makes my moving background scroll he he he Tricky Tricky!
    # Cycles through all events that are occuring  
    for event in pygame.event.get():
        if event.type == INC_SPEED:
            enemy_speedx += 0.4
            enemy_speedy +=0.4
        elif event.type == QUIT:  # quit if x in window is clicked
            pygame.quit()
            sys.exit()
    #if not boss_explode_inprogress:      # STOP scrolling if BOSS DIES 
       # bgY += direction * scroll_speed  # Move both background images down
      #  bgY2 += direction * scroll_speed # Move both background images down
      #  if direction >0:
         #   if bgY > bg.get_height() * 1:  # If our 1st background has moved to the bottom of the screen then reset its position back to zero
         #       bgY = 0
         #   if bgY2 > 0:
         #       bgY2 = bg.get_height() * -1 # If our 2nd background has moved to the top of the screen then reset its position back to 1 screen above the visable area
         #  if direction <0:
         #    if bgY < 0:  
         #      bgY = bg.get_height() * 1
        # if bgY2 < bg.get_height() *-1:
        #    bgY2 = 0
#                                                check the Shield powerup AND Bolt powerup for a colission
# ----------------------------------------------------------------------------------------------------------------------------------------------------------------

    hits = pygame.sprite.spritecollide(P1, powerups, True)
    for hit in hits:
        if hit.type == 'shield':
            powerup_shields.play()
            P1.shield += random.randrange(10, 30)
            if P1.shield >= 100:
                P1.shield = 100
        if hit.type == 'power':
            bolt_powerup.play()
            P1.powerup()
#                                                Check for Boss bullet and Player collisions
# ---------------------------------------------------------------------------------------------------------------------------------------------------------------   
    if len(enemy_boss) >0:
        # Calculate mechanics for each bossbullet
        boss_hit_list = pygame.sprite.spritecollide(P1, boss_bullet_list, True)   # collision detection FOR BOSS BULLETS and PLAYER SHIP
        if len(boss_hit_list)>0 and len(shield_sprites)<=0:
                P1_shield = Shield(P1.rect.center)
                all_sprites.add(P1_shield)
                shield_sprites.add(P1_shield)
        for bullet_hit in boss_hit_list:
            player_hit.play()
            P1.shield -= 10
            
            expl = Explosion(bullet_hit.rect.center, 'sm')
            all_sprites.add(expl)
            if P1.shield <= 0: # if player has no shield left then play explosion sound & animation, subract 1 from lives and reset shield to 100
                crash_sound.play()
                death_explosion = Explosion(P1.rect.center, 'player')
                all_sprites.add(death_explosion)
                P1.hide()
                P1.lives -=1
                P1.shield = 100
# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------  
# -------------------------------------------- Test for collision between PLAYER BULLET and BOSS MONSTER! -------------------------------------------------------------
# ---------------------------------------------------------------------------------------------------------------------------------------------------------------------           
# Calculate mechanics for each bullet
    if len(enemy_boss) >0:   # ONLY CHECK IF BOSS MONSTER EXISTS
        for bullet in bullet_list:
            # See if the bullet has hit BOSS MONSTER
            enemy_hit_list = pygame.sprite.spritecollide(bullet, enemy_boss, False, pygame.sprite.collide_circle )   # collision detection using the circle hit box method
            # For each Player hit, remove the bullet and add to the score
            for hit in enemy_hit_list:
                boss_hit_sound.play()
                bullet_list.remove(bullet)
                all_sprites.remove(bullet)
                tempx = bullet.rect.x
                tempy = hit.rect.y+126
                tempxy = tempx, tempy
                expl = Explosion(tempxy, 'sm')
                all_sprites.add(expl)
                score += 5
                Big_boss.life -= 1
                if Big_boss.life<5:
                    pygame.mixer.music.fadeout(1400)
                
                if Big_boss.life<=0 and Boss_died_anim_finished==False and boss_explode_inprogress==False and len(enemy_boss)>0:
                    do_big_explosion()
                    explosion_time = pygame.time.get_ticks()
                    
     
    if len(enemy_boss)<=0 and Boss_level_triggered==True and pygame.time.get_ticks() - explosion_time >2500:
        owwyeah.play()
        time.sleep(0.9)
        Boss_died_anim_finished=False
        pygame.display.flip()
        game_over=True
# ------------------------------------------------------------------------------------------------------------------------------------------------------------------   
# -------------------------------------------------------- Test for collision between PLAYER bullet and enemies! ---------------------------------------------------
# ------------------------------------------------------------------------------------------------------------------------------------------------------------------            
    # Calculate mechanics for each bullet
    for bullet in bullet_list:
        # See if the bullet has hit an enemy
        enemy_hit_list = pygame.sprite.spritecollide(bullet, enemies, True, pygame.sprite.collide_circle )   # collision detection using the circle hit box method
        # For each enemy hit, remove the bullet and add to the score
        for hit in enemy_hit_list:
            enemy_hit.play()
            bullet_list.remove(bullet)
            all_sprites.remove(bullet)
            expl = Explosion(hit.rect.center, 'lg')
            all_sprites.add(expl)
            score += 10
            if random.random() > 0.85:
                Pups = Powerups(hit.rect.center)
                all_sprites.add(Pups)
                powerups.add(Pups)
            if score<200:
                new_enemy()
 
        # Remove the bullet if it flies up off the screen
        if bullet.rect.y < -10:
            bullet_list.remove(bullet)
            all_sprites.remove(bullet)
# ------------------------------------------------------------------------------------------------------------------------------------------------------------------
# -------------- Refreshes display and MOVES all Sprites  calls move function in all sprite classes----- VERY IMPORTANT -------------------------------------------- 
# ------------------------------------------------------------------------------------------------------------------------------------------------------------------
    for entity in all_sprites:
        screen.blit(entity.image, entity.rect)
        entity.move()
    if score >200:
        number = len(enemies)  # the number of enemies left
        if number == 0 and len(enemy_boss)==0 and Boss_level_triggered==False:     #     if there are 0 enemies left and the score is more than 150 then CHANGE background scroll direction, speed create BOSS sprite, add to sprite groups!
            super_zoom()
            Boss_level_triggered=True
            direction = -1
            scroll_speed = 6
            speed = scroll_speed * direction
            musical_volume = 0.9 # turn up music for Boss Monster level
            show_boss_level()
            Big_boss = Boss(entity.rect.center,'boss')
            all_sprites.add(Big_boss)
            enemy_boss.add(Big_boss)
            enemy_boss.draw(screen)
# ---------------------------------------------------------------------------------------------------------------------------------------- 
# To be run if collision occurs between PLAYER and Enemy
# ---------------------------------------------------------------------------------------------------------------------------------------- 
    hits = pygame.sprite.spritecollide(P1, enemies, True, pygame.sprite.collide_circle)
    if len(hits)>0 and len(shield_sprites)<=0:
        P1_shield = Shield(P1.rect.center)
        all_sprites.add(P1_shield)
        shield_sprites.add(P1_shield)
    for hit in hits:
        player_hit.play()
        P1.shield -= 20
        expl = Explosion(hit.rect.center, 'sm')
        all_sprites.add(expl)
        new_enemy()
    if P1.shield <= 0: # if player has no shield left then play explosion sound & animation, subract 1 from lives and reset shield to 100
        all_sprites.remove(P1_shield)
        shield_sprites.remove(P1_shield)
        P1_shield.kill()
        shield_kill=False
        crash_sound.play()
        death_explosion = Explosion(P1.rect.center, 'player')
        all_sprites.add(death_explosion)
        P1.hide()
        P1.lives -=1
        P1.shield = 100
# ---------------------------------------------------------------------------------------------------------------------------------------
# If the player has no more lives left and the explosion animation has finished playing then GAME OVER   
# ---------------------------------------------------------------------------------------------------------------------------------------           
    if P1.lives == 0 and not death_explosion.alive():
        all_sprites.remove(P1_shield)
        shield_sprites.remove(P1_shield) 
        P1_shield.kill()
        draw_text(screen, "GAME OVER", 94, WIDTH / 2, (HEIGHT / 2)-70, RED)
        pygame.display.update()
        game_over_message.play()
        time.sleep(3)
        enemy_speedx=1
        enemy_speedy=1
        if len(enemy_boss)>0:
            enemy_boss.remove(Big_boss)
            all_sprites.remove(Big_boss)
        all_sprites.remove(P1)
        P1.kill()
        game_over=True
# ---------------------------------------------------------------------------------------------------------------------------------------       
    draw_shield_bar(screen, WIDTH / 1.3, 7, P1.shield)
    draw_text(screen, 'SCORE ' +str(score), 22, WIDTH / 11, 3, WHITE) # display score on screen on far left
    draw_lives(screen, WIDTH - 100, 7, P1.lives, player_mini_img)
    if len(enemy_boss) >0:   # ONLY DRAW BOSS BAR IF BOSS MONSTER EXISTS
        draw_text(screen, 'BOSS HEATH', 20, WIDTH / 2.4, 3, WHITE) 
        draw_boss_bar(screen, WIDTH / 2.1, 7, Big_boss.life)    
    pygame.display.flip()
    FramePerSec.tick(FPS)
pygame.quit()

# === ./UltraBorg_old.py ===
#!/usr/bin/env python3
# coding: utf-8
"""
This module is designed to communicate with the UltraBorg

Use by creating an instance of the class, call the Init function, then command as desired, e.g.
import UltraBorg
UB = UltraBorg.UltraBorg()
UB.Init()
# User code here, use UB to control the board

Multiple boards can be used when configured with different I²C addresses by creating multiple instances, e.g.
import UltraBorg
UB1 = UltraBorg.UltraBorg()
UB2 = UltraBorg.UltraBorg()
UB1.i2cAddress = 0x44
UB2.i2cAddress = 0x45
UB1.Init()
UB2.Init()
# User code here, use UB1 and UB2 to control each board separately

For explanations of the functions available call the Help function, e.g.
import UltraBorg
UB = UltraBorg.UltraBorg()
UB.Help()
See the website at www.piborg.org/ultraborg for more details
"""

# Import the libraries we need
import io

# import fcntl
import types
import time

# Constant values
I2C_SLAVE = 0x0703
I2C_MAX_LEN = 4
USM_US_TO_MM = 0.171500
PWM_MIN = 2000  # Should be a 1 ms burst, typical servo minimum
PWM_MAX = 4000  # Should be a 2 ms burst, typical servo maximum
DELAY_AFTER_EEPROM = 0.01  # Time to wait after updating an EEPROM value before reading
PWM_UNSET = 0xFFFF

I2C_ID_SERVO_USM = 0x36

COMMAND_GET_TIME_USM1 = (
    1  # Get the time measured by ultrasonic #1 in us (0 for no detection)
)
COMMAND_GET_TIME_USM2 = (
    2  # Get the time measured by ultrasonic #2 in us (0 for no detection)
)
COMMAND_GET_TIME_USM3 = (
    3  # Get the time measured by ultrasonic #3 in us (0 for no detection)
)
COMMAND_GET_TIME_USM4 = (
    4  # Get the time measured by ultrasonic #4 in us (0 for no detection)
)
COMMAND_SET_PWM1 = 5  # Set the PWM duty cycle for drive #1 (16 bit)
COMMAND_GET_PWM1 = 6  # Get the PWM duty cycle for drive #1 (16 bit)
COMMAND_SET_PWM2 = 7  # Set the PWM duty cycle for drive #2 (16 bit)
COMMAND_GET_PWM2 = 8  # Get the PWM duty cycle for drive #2 (16 bit)
COMMAND_SET_PWM3 = 9  # Set the PWM duty cycle for drive #3 (16 bit)
COMMAND_GET_PWM3 = 10  # Get the PWM duty cycle for drive #3 (16 bit)
COMMAND_SET_PWM4 = 11  # Set the PWM duty cycle for drive #4 (16 bit)
COMMAND_GET_PWM4 = 12  # Get the PWM duty cycle for drive #4 (16 bit)
COMMAND_CALIBRATE_PWM1 = (
    13  # Set the PWM duty cycle for drive #1 (16 bit, ignores limit checks)
)
COMMAND_CALIBRATE_PWM2 = (
    14  # Set the PWM duty cycle for drive #2 (16 bit, ignores limit checks)
)
COMMAND_CALIBRATE_PWM3 = (
    15  # Set the PWM duty cycle for drive #3 (16 bit, ignores limit checks)
)
COMMAND_CALIBRATE_PWM4 = (
    16  # Set the PWM duty cycle for drive #4 (16 bit, ignores limit checks)
)
COMMAND_GET_PWM_MIN_1 = 17  # Get the minimum allowed PWM duty cycle for drive #1
COMMAND_GET_PWM_MAX_1 = 18  # Get the maximum allowed PWM duty cycle for drive #1
COMMAND_GET_PWM_BOOT_1 = 19  # Get the startup PWM duty cycle for drive #1
COMMAND_GET_PWM_MIN_2 = 20  # Get the minimum allowed PWM duty cycle for drive #2
COMMAND_GET_PWM_MAX_2 = 21  # Get the maximum allowed PWM duty cycle for drive #2
COMMAND_GET_PWM_BOOT_2 = 22  # Get the startup PWM duty cycle for drive #2
COMMAND_GET_PWM_MIN_3 = 23  # Get the minimum allowed PWM duty cycle for drive #3
COMMAND_GET_PWM_MAX_3 = 24  # Get the maximum allowed PWM duty cycle for drive #3
COMMAND_GET_PWM_BOOT_3 = 25  # Get the startup PWM duty cycle for drive #3
COMMAND_GET_PWM_MIN_4 = 26  # Get the minimum allowed PWM duty cycle for drive #4
COMMAND_GET_PWM_MAX_4 = 27  # Get the maximum allowed PWM duty cycle for drive #4
COMMAND_GET_PWM_BOOT_4 = 28  # Get the startup PWM duty cycle for drive #4
COMMAND_SET_PWM_MIN_1 = 29  # Set the minimum allowed PWM duty cycle for drive #1
COMMAND_SET_PWM_MAX_1 = 30  # Set the maximum allowed PWM duty cycle for drive #1
COMMAND_SET_PWM_BOOT_1 = 31  # Set the startup PWM duty cycle for drive #1
COMMAND_SET_PWM_MIN_2 = 32  # Set the minimum allowed PWM duty cycle for drive #2
COMMAND_SET_PWM_MAX_2 = 33  # Set the maximum allowed PWM duty cycle for drive #2
COMMAND_SET_PWM_BOOT_2 = 34  # Set the startup PWM duty cycle for drive #2
COMMAND_SET_PWM_MIN_3 = 35  # Set the minimum allowed PWM duty cycle for drive #3
COMMAND_SET_PWM_MAX_3 = 36  # Set the maximum allowed PWM duty cycle for drive #3
COMMAND_SET_PWM_BOOT_3 = 37  # Set the startup PWM duty cycle for drive #3
COMMAND_SET_PWM_MIN_4 = 38  # Set the minimum allowed PWM duty cycle for drive #4
COMMAND_SET_PWM_MAX_4 = 39  # Set the maximum allowed PWM duty cycle for drive #4
COMMAND_SET_PWM_BOOT_4 = 40  # Set the startup PWM duty cycle for drive #4
COMMAND_GET_FILTER_USM1 = (
    41  # Get the filtered time measured by ultrasonic #1 in us (0 for no detection)
)
COMMAND_GET_FILTER_USM2 = (
    42  # Get the filtered time measured by ultrasonic #2 in us (0 for no detection)
)
COMMAND_GET_FILTER_USM3 = (
    43  # Get the filtered time measured by ultrasonic #3 in us (0 for no detection)
)
COMMAND_GET_FILTER_USM4 = (
    44  # Get the filtered time measured by ultrasonic #4 in us (0 for no detection)
)
COMMAND_GET_ID = 0x99  # Get the board identifier
COMMAND_SET_I2C_ADD = 0xAA  # Set a new I2C address

COMMAND_VALUE_FWD = 1  # I2C value representing forward
COMMAND_VALUE_REV = 2  # I2C value representing reverse

COMMAND_VALUE_ON = 1  # I2C value representing on
COMMAND_VALUE_OFF = 0  # I2C value representing off


def ScanForUltraBorg(busNumber=1):
    """
    ScanForUltraBorg([busNumber])

    Scans the I²C bus for a UltraBorg boards and returns a list of all usable addresses
    The busNumber if supplied is which I²C bus to scan, 0 for Rev 1 boards, 1 for Rev 2 boards, if not supplied the default is 1
    """
    found = []
    print("Scanning I²C bus #%d" % (busNumber))
    bus = UltraBorg()
    for address in range(0x03, 0x78, 1):
        try:
            bus.InitBusOnly(busNumber, address)
            i2cRecv = bus.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
            if len(i2cRecv) == I2C_MAX_LEN:
                if i2cRecv[1] == I2C_ID_SERVO_USM:
                    print("Found UltraBorg at %02X" % (address))
                    found.append(address)
                else:
                    pass
            else:
                pass
        except KeyboardInterrupt:
            raise
        except:
            pass
    if len(found) == 0:
        print(
            "No UltraBorg boards found, is bus #%d correct (should be 0 for Rev 1, 1 for Rev 2)"
            % (busNumber)
        )
    elif len(found) == 1:
        print("1 UltraBorg board found")
    else:
        print("%d UltraBorg boards found" % (len(found)))
    return found


def SetNewAddress(newAddress, oldAddress=-1, busNumber=1):
    """
    SetNewAddress(newAddress, [oldAddress], [busNumber])

    Scans the I²C bus for the first UltraBorg and sets it to a new I2C address
    If oldAddress is supplied it will change the address of the board at that address rather than scanning the bus
    The busNumber if supplied is which I²C bus to scan, 0 for Rev 1 boards, 1 for Rev 2 boards, if not supplied the default is 1
    Warning, this new I²C address will still be used after resetting the power on the device
    """
    if newAddress < 0x03:
        print(
            "Error, I²C addresses below 3 (0x03) are reserved, use an address between 3 (0x03) and 119 (0x77)"
        )
        return
    elif newAddress > 0x77:
        print(
            "Error, I²C addresses above 119 (0x77) are reserved, use an address between 3 (0x03) and 119 (0x77)"
        )
        return
    if oldAddress < 0x0:
        found = ScanForUltraBorg(busNumber)
        if len(found) < 1:
            print("No UltraBorg boards found, cannot set a new I²C address!")
            return
        else:
            oldAddress = found[0]
    print(
        "Changing I²C address from %02X to %02X (bus #%d)"
        % (oldAddress, newAddress, busNumber)
    )
    bus = UltraBorg()
    bus.InitBusOnly(busNumber, oldAddress)
    try:
        i2cRecv = bus.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
        if len(i2cRecv) == I2C_MAX_LEN:
            if i2cRecv[1] == I2C_ID_SERVO_USM:
                foundChip = True
                print("Found UltraBorg at %02X" % (oldAddress))
            else:
                foundChip = False
                print(
                    "Found a device at %02X, but it is not a UltraBorg (ID %02X instead of %02X)"
                    % (oldAddress, i2cRecv[1], I2C_ID_SERVO_USM)
                )
        else:
            foundChip = False
            print("Missing UltraBorg at %02X" % (oldAddress))
    except KeyboardInterrupt:
        raise
    except:
        foundChip = False
        print("Missing UltraBorg at %02X" % (oldAddress))
    if foundChip:
        bus.RawWrite(COMMAND_SET_I2C_ADD, [newAddress])
        time.sleep(0.1)
        print(
            "Address changed to %02X, attempting to talk with the new address"
            % (newAddress)
        )
        try:
            bus.InitBusOnly(busNumber, newAddress)
            i2cRecv = bus.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
            if len(i2cRecv) == I2C_MAX_LEN:
                if i2cRecv[1] == I2C_ID_SERVO_USM:
                    foundChip = True
                    print("Found UltraBorg at %02X" % (newAddress))
                else:
                    foundChip = False
                    print(
                        "Found a device at %02X, but it is not a UltraBorg (ID %02X instead of %02X)"
                        % (newAddress, i2cRecv[1], I2C_ID_SERVO_USM)
                    )
            else:
                foundChip = False
                print("Missing UltraBorg at %02X" % (newAddress))
        except KeyboardInterrupt:
            raise
        except:
            foundChip = False
            print("Missing UltraBorg at %02X" % (newAddress))
    if foundChip:
        print("New I²C address of %02X set successfully" % (newAddress))
    else:
        print("Failed to set new I²C address...")


# Class used to control UltraBorg
class UltraBorg:
    """
    This module is designed to communicate with the UltraBorg

    busNumber               I²C bus on which the UltraBorg is attached (Rev 1 is bus 0, Rev 2 is bus 1)
    bus                     the smbus object used to talk to the I²C bus
    i2cAddress              The I²C address of the UltraBorg chip to control
    foundChip               True if the UltraBorg chip can be seen, False otherwise
    printFunction           Function reference to call when printing text, if None "print" is used
    """

    # Shared values used by this class
    busNumber = 1  # Check here for Rev 1 vs Rev 2 and select the correct bus
    i2cAddress = I2C_ID_SERVO_USM  # I²C address, override for a different address
    foundChip = False
    printFunction = None
    i2cWrite = None
    i2cRead = None

    # Default calibration adjustments to standard values
    PWM_MIN_1 = PWM_MIN
    PWM_MAX_1 = PWM_MAX
    PWM_MIN_2 = PWM_MIN
    PWM_MAX_2 = PWM_MAX
    PWM_MIN_3 = PWM_MIN
    PWM_MAX_3 = PWM_MAX
    PWM_MIN_4 = PWM_MIN
    PWM_MAX_4 = PWM_MAX

    def RawWrite(self, command, data):
        """
        RawWrite(command, data)

        Sends a raw command on the I2C bus to the UltraBorg
        Command codes can be found at the top of UltraBorg.py, data is a list of 0 or more byte values

        Under most circumstances you should use the appropriate function instead of RawWrite
        """
        rawOutput = [command]
        rawOutput.extend(data)
        rawOutput = bytes(rawOutput)
        self.i2cWrite.write(rawOutput)

    def RawRead(self, command, length, retryCount=5):
        """
        RawRead(command, length, [retryCount])

        Reads data back from the UltraBorg after sending a GET command
        Command codes can be found at the top of UltraBorg.py, length is the number of bytes to read back

        The function checks that the first byte read back matches the requested command
        If it does not it will retry the request until retryCount is exhausted (default is 3 times)

        Under most circumstances you should use the appropriate function instead of RawRead
        """
        while retryCount > 0:
            try:
                self.RawWrite(command, [])
            except:
                # Delay on failed bus write
                retryCount -= 1
                time.sleep(0.1)
                continue
            time.sleep(0.000001)
            try:
                rawReply = self.i2cRead.read(length)
            except:
                # Delay on failed bus read
                retryCount -= 1
                time.sleep(0.1)
                continue
            reply = []
            for singleByte in rawReply:
                reply.append(singleByte)
            if command == reply[0]:
                # Reply successful
                break
            else:
                retryCount -= 1
        if retryCount > 0:
            return reply
        else:
            raise IOError("I2C read for command %d failed" % (command))

    def InitBusOnly(self, busNumber, address):
        """
        InitBusOnly(busNumber, address)

        Prepare the I2C driver for talking to a UltraBorg on the specified bus and I2C address
        This call does not check the board is present or working, under most circumstances use Init() instead
        """
        self.busNumber = busNumber
        self.i2cAddress = address
        self.i2cRead = io.open("/dev/i2c-" + str(self.busNumber), "rb", buffering=0)
        # fcntl.ioctl(self.i2cRead, I2C_SLAVE, self.i2cAddress)
        self.i2cWrite = io.open("/dev/i2c-" + str(self.busNumber), "wb", buffering=0)
        # fcntl.ioctl(self.i2cWrite, I2C_SLAVE, self.i2cAddress)

    def Print(self, message):
        """
        Print(message)

        Wrapper used by the UltraBorg instance to print messages, will call printFunction if set, print otherwise
        """
        if self.printFunction == None:
            print(message)
        else:
            self.printFunction(message)

    def NoPrint(self, message):
        """
        NoPrint(message)

        Does nothing, intended for disabling diagnostic printout by using:
        UB = UltraBorg.UltraBorg()
        UB.printFunction = UB.NoPrint
        """
        pass

    def Init(self, tryOtherBus=False):
        """
        Init([tryOtherBus])

        Prepare the I2C driver for talking to the UltraBorg

        If tryOtherBus is True, this function will attempt to use the other bus if the ThunderBorg devices can not be found on the current busNumber
            This is only really useful for early Raspberry Pi models!
        """
        self.Print(
            "Loading UltraBorg on bus %d, address %02X"
            % (self.busNumber, self.i2cAddress)
        )

        # Open the bus
        self.i2cRead = io.open("/dev/i2c-" + str(self.busNumber), "rb", buffering=0)
        # fcntl.ioctl(self.i2cRead, I2C_SLAVE, self.i2cAddress)
        self.i2cWrite = io.open("/dev/i2c-" + str(self.busNumber), "wb", buffering=0)
        # fcntl.ioctl(self.i2cWrite, I2C_SLAVE, self.i2cAddress)

        # Check for UltraBorg
        try:
            i2cRecv = self.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
            if len(i2cRecv) == I2C_MAX_LEN:
                if i2cRecv[1] == I2C_ID_SERVO_USM:
                    self.foundChip = True
                    self.Print("Found UltraBorg at %02X" % (self.i2cAddress))
                else:
                    self.foundChip = False
                    self.Print(
                        "Found a device at %02X, but it is not a UltraBorg (ID %02X instead of %02X)"
                        % (self.i2cAddress, i2cRecv[1], I2C_ID_SERVO_USM)
                    )
            else:
                self.foundChip = False
                self.Print("Missing UltraBorg at %02X" % (self.i2cAddress))
        except KeyboardInterrupt:
            raise
        except:
            self.foundChip = False
            self.Print("Missing UltraBorg at %02X" % (self.i2cAddress))

        # See if we are missing chips
        if not self.foundChip:
            self.Print("UltraBorg was not found")
            if tryOtherBus:
                if self.busNumber == 1:
                    self.busNumber = 0
                else:
                    self.busNumber = 1
                self.Print("Trying bus %d instead" % (self.busNumber))
                self.Init(False)
            else:
                self.Print(
                    "Are you sure your UltraBorg is properly attached, the correct address is used, and the I2C drivers are running?"
                )
                self.bus = None
        else:
            self.Print("UltraBorg loaded on bus %d" % (self.busNumber))

        # Read the calibration settings from the UltraBorg
        self.PWM_MIN_1 = self.GetWithRetry(self.GetServoMinimum1, 5)
        self.PWM_MAX_1 = self.GetWithRetry(self.GetServoMaximum1, 5)
        self.PWM_MIN_2 = self.GetWithRetry(self.GetServoMinimum2, 5)
        self.PWM_MAX_2 = self.GetWithRetry(self.GetServoMaximum2, 5)
        self.PWM_MIN_3 = self.GetWithRetry(self.GetServoMinimum3, 5)
        self.PWM_MAX_3 = self.GetWithRetry(self.GetServoMaximum3, 5)
        self.PWM_MIN_4 = self.GetWithRetry(self.GetServoMinimum4, 5)
        self.PWM_MAX_4 = self.GetWithRetry(self.GetServoMaximum4, 5)

    def GetWithRetry(self, function, count):
        """
        value = GetWithRetry(function, count)

        Attempts to read a value multiple times before giving up
        Pass a get function with no parameters
        e.g.
        distance = GetWithRetry(UB.GetDistance1, 5)
        Will try UB.GetDistance1() upto 5 times, returning when it gets a value
        Useful for ensuring a read is successful
        """
        value = None
        for i in range(count):
            okay = True
            try:
                value = function()
            except KeyboardInterrupt:
                raise
            except:
                okay = False
            if okay:
                break
        return value

    def SetWithRetry(self, setFunction, getFunction, value, count):
        """
        worked = SetWithRetry(setFunction, getFunction, value, count)

        Attempts to write a value multiple times before giving up
        Pass a set function with one parameter, and a get function no parameters
        The get function will be used to check if the set worked, if not it will be repeated
        e.g.
        worked = SetWithRetry(UB.SetServoMinimum1, UB.GetServoMinimum1, 2000, 5)
        Will try UB.SetServoMinimum1(2000) upto 5 times, returning when UB.GetServoMinimum1 returns 2000.
        Useful for ensuring a write is successful
        """
        for i in range(count):
            okay = True
            try:
                setFunction(value)
                readValue = getFunction()
            except KeyboardInterrupt:
                raise
            except:
                okay = False
            if okay:
                if readValue == value:
                    break
                else:
                    okay = False
        return okay

    def GetDistance1(self):
        """
        distance = GetDistance1()

        Gets the filtered distance for ultrasonic module #1 in millimeters
        Returns 0 for no object detected or no ultrasonic module attached
        If you need a faster response try GetRawDistance1 instead (no filtering)
        e.g.
        0     -> No object in range
        25    -> Object 25 mm away
        1000  -> Object 1000 mm (1 m) away
        3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading ultrasonic #1 distance!")
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM

    def GetDistance2(self):
        """
        distance = GetDistance2()

        Gets the filtered distance for ultrasonic module #2 in millimeters
        Returns 0 for no object detected or no ultrasonic module attached
        If you need a faster response try GetRawDistance2 instead (no filtering)
        e.g.
        0     -> No object in range
        25    -> Object 25 mm away
        1000  -> Object 1000 mm (1 m) away
        3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading ultrasonic #2 distance!")
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM

    def GetDistance3(self):
        """
        distance = GetDistance3()

        Gets the filtered distance for ultrasonic module #3 in millimeters
        Returns 0 for no object detected or no ultrasonic module attached
        If you need a faster response try GetRawDistance3 instead (no filtering)
        e.g.
        0     -> No object in range
        25    -> Object 25 mm away
        1000  -> Object 1000 mm (1 m) away
        3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading ultrasonic #3 distance!")
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM

    def GetDistance4(self):
        """
        distance = GetDistance4()

        Gets the filtered distance for ultrasonic module #4 in millimeters
        Returns 0 for no object detected or no ultrasonic module attached
        If you need a faster response try GetRawDistance4 instead (no filtering)
        e.g.
        0     -> No object in range
        25    -> Object 25 mm away
        1000  -> Object 1000 mm (1 m) away
        3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading ultrasonic #4 distance!")
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM

    def GetRawDistance1(self):
        """
        distance = GetRawDistance1()

        Gets the raw distance for ultrasonic module #1 in millimeters
        Returns 0 for no object detected or no ultrasonic module attached
        For a filtered (less jumpy) reading use GetDistance1
        e.g.
        0     -> No object in range
        25    -> Object 25 mm away
        1000  -> Object 1000 mm (1 m) away
        3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading ultrasonic #1 distance!")
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM

    def GetRawDistance2(self):
        """
        distance = GetRawDistance2()

        Gets the raw distance for ultrasonic module #2 in millimeters
        Returns 0 for no object detected or no ultrasonic module attached
        For a filtered (less jumpy) reading use GetDistance2
        e.g.
        0     -> No object in range
        25    -> Object 25 mm away
        1000  -> Object 1000 mm (1 m) away
        3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading ultrasonic #2 distance!")
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM

    def GetRawDistance3(self):
        """
        distance = GetRawDistance3()

        Gets the raw distance for ultrasonic module #3 in millimeters
        Returns 0 for no object detected or no ultrasonic module attached
        For a filtered (less jumpy) reading use GetDistance3
        e.g.
        0     -> No object in range
        25    -> Object 25 mm away
        1000  -> Object 1000 mm (1 m) away
        3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading ultrasonic #3 distance!")
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM

    def GetRawDistance4(self):
        """
        distance = GetRawDistance4()

        Gets the distance for ultrasonic module #4 in millimeters
        Returns 0 for no object detected or no ultrasonic module attached
        For a filtered (less jumpy) reading use GetDistance4
        e.g.
        0     -> No object in range
        25    -> Object 25 mm away
        1000  -> Object 1000 mm (1 m) away
        3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading ultrasonic #4 distance!")
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM

    def GetServoPosition1(self):
        """
        position = GetServoPosition1()

        Gets the drive position for servo output #1
        0 is central, -1 is maximum left, +1 is maximum right
        e.g.
        0     -> Central
        0.5   -> 50% to the right
        1     -> 100% to the right
        -0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo output #1!")
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_1) / (self.PWM_MAX_1 - self.PWM_MIN_1)
        return (2.0 * powerOut) - 1.0

    def GetServoPosition2(self):
        """
        position = GetServoPosition2()

        Gets the drive position for servo output #2
        0 is central, -1 is maximum left, +1 is maximum right
        e.g.
        0     -> Central
        0.5   -> 50% to the right
        1     -> 100% to the right
        -0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo output #2!")
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_2) / (self.PWM_MAX_2 - self.PWM_MIN_2)
        return (2.0 * powerOut) - 1.0

    def GetServoPosition3(self):
        """
        position = GetServoPosition3()

        Gets the drive position for servo output #3
        0 is central, -1 is maximum left, +1 is maximum right
        e.g.
        0     -> Central
        0.5   -> 50% to the right
        1     -> 100% to the right
        -0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo output #3!")
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_3) / (self.PWM_MAX_3 - self.PWM_MIN_3)
        return (2.0 * powerOut) - 1.0

    def GetServoPosition4(self):
        """
        position = GetServoPosition4()

        Gets the drive position for servo output #4
        0 is central, -1 is maximum left, +1 is maximum right
        e.g.
        0     -> Central
        0.5   -> 50% to the right
        1     -> 100% to the right
        -0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo output #4!")
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_4) / (self.PWM_MAX_4 - self.PWM_MIN_4)
        return (2.0 * powerOut) - 1.0

    def SetServoPosition1(self, position):
        """
        SetServoPosition1(position)

        Sets the drive position for servo output #1
        0 is central, -1 is maximum left, +1 is maximum right
        e.g.
        0     -> Central
        0.5   -> 50% to the right
        1     -> 100% to the right
        -0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_1 - self.PWM_MIN_1)) + self.PWM_MIN_1)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo output #1!")

    def SetServoPosition2(self, position):
        """
        SetServoPosition2(position)

        Sets the drive position for servo output #2
        0 is central, -1 is maximum left, +1 is maximum right
        e.g.
        0     -> Central
        0.5   -> 50% to the right
        1     -> 100% to the right
        -0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_2 - self.PWM_MIN_2)) + self.PWM_MIN_2)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo output #2!")

    def SetServoPosition3(self, position):
        """
        SetServoPosition3(position)

        Sets the drive position for servo output #3
        0 is central, -1 is maximum left, +1 is maximum right
        e.g.
        0     -> Central
        0.5   -> 50% to the right
        1     -> 100% to the right
        -0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_3 - self.PWM_MIN_3)) + self.PWM_MIN_3)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo output #3!")

    def SetServoPosition4(self, position):
        """
        SetServoPosition4(position)

        Sets the drive position for servo output #4
        0 is central, -1 is maximum left, +1 is maximum right
        e.g.
        0     -> Central
        0.5   -> 50% to the right
        1     -> 100% to the right
        -0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_4 - self.PWM_MIN_4)) + self.PWM_MIN_4)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo output #1!")

    def GetServoMinimum1(self):
        """
        pwmLevel = GetServoMinimum1()

        Gets the minimum PWM level for servo output #1
        This corresponds to position -1
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #1 minimum burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoMinimum2(self):
        """
        pwmLevel = GetServoMinimum2()

        Gets the minimum PWM level for servo output #2
        This corresponds to position -1
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #2 minimum burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoMinimum3(self):
        """
        pwmLevel = GetServoMinimum3()

        Gets the minimum PWM level for servo output #3
        This corresponds to position -1
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #3 minimum burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoMinimum4(self):
        """
        pwmLevel = GetServoMinimum4()

        Gets the minimum PWM level for servo output #4
        This corresponds to position -1
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #4 minimum burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoMaximum1(self):
        """
        pwmLevel = GetServoMaximum1()

        Gets the maximum PWM level for servo output #1
        This corresponds to position +1
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #1 maximum burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoMaximum2(self):
        """
        pwmLevel = GetServoMaximum2()

        Gets the maximum PWM level for servo output #2
        This corresponds to position +1
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #2 maximum burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoMaximum3(self):
        """
        pwmLevel = GetServoMaximum3()

        Gets the maximum PWM level for servo output #3
        This corresponds to position +1
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #3 maximum burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoMaximum4(self):
        """
        pwmLevel = GetServoMaximum4()

        Gets the maximum PWM level for servo output #4
        This corresponds to position +1
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #4 maximum burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoStartup1(self):
        """
        pwmLevel = GetServoStartup1()

        Gets the startup PWM level for servo output #1
        This can be anywhere in the minimum to maximum range
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #1 startup burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoStartup2(self):
        """
        pwmLevel = GetServoStartup2()

        Gets the startup PWM level for servo output #2
        This can be anywhere in the minimum to maximum range
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #2 startup burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoStartup3(self):
        """
        pwmLevel = GetServoStartup3()

        Gets the startup PWM level for servo output #3
        This can be anywhere in the minimum to maximum range
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #3 startup burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def GetServoStartup4(self):
        """
        pwmLevel = GetServoStartup4()

        Gets the startup PWM level for servo output #4
        This can be anywhere in the minimum to maximum range
        The value is an integer where 2000 represents a 1 ms servo burst
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst
        4000  -> 2 ms servo burst, typical longest burst
        3000  -> 1.5 ms servo burst, typical centre,
        5000  -> 2.5 ms servo burst, higher than typical longest burst
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading servo #4 startup burst!")
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]

    def CalibrateServoPosition1(self, pwmLevel):
        """
        CalibrateServoPosition1(pwmLevel)

        Sets the raw PWM level for servo output #1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending calibration servo output #1!")

    def CalibrateServoPosition2(self, pwmLevel):
        """
        CalibrateServoPosition2(pwmLevel)

        Sets the raw PWM level for servo output #2
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending calibration servo output #2!")

    def CalibrateServoPosition3(self, pwmLevel):
        """
        CalibrateServoPosition3(pwmLevel)

        Sets the raw PWM level for servo output #3
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending calibration servo output #3!")

    def CalibrateServoPosition4(self, pwmLevel):
        """
        CalibrateServoPosition4(pwmLevel)

        Sets the raw PWM level for servo output #4
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending calibration servo output #4!")

    def GetRawServoPosition1(self):
        """
        pwmLevel = GetRawServoPosition1()

        Gets the raw PWM level for servo output #1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        This value requires interpreting into an actual servo position, this is already done by GetServoPosition1
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading raw servo output #1!")
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty

    def GetRawServoPosition2(self):
        """
        pwmLevel = GetRawServoPosition2()

        Gets the raw PWM level for servo output #2
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        This value requires interpreting into an actual servo position, this is already done by GetServoPosition2
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading raw servo output #2!")
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty

    def GetRawServoPosition3(self):
        """
        pwmLevel = GetRawServoPosition3()

        Gets the raw PWM level for servo output #3
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        This value requires interpreting into an actual servo position, this is already done by GetServoPosition3
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading raw servo output #3!")
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty

    def GetRawServoPosition4(self):
        """
        pwmLevel = GetRawServoPosition4()

        Gets the raw PWM level for servo output #4
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        This value requires interpreting into an actual servo position, this is already done by GetServoPosition4
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed reading raw servo output #4!")
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty

    def SetServoMinimum1(self, pwmLevel):
        """
        SetServoMinimum1(pwmLevel)

        Sets the minimum PWM level for servo output #1
        This corresponds to position -1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        LIMIT CHECKING IS ALTERED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo minimum limit #1!")
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_1 = self.GetServoMinimum1()

    def SetServoMinimum2(self, pwmLevel):
        """
        SetServoMinimum2(pwmLevel)

        Sets the minimum PWM level for servo output #2
        This corresponds to position -1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        LIMIT CHECKING IS ALTERED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo minimum limit #2!")
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_2 = self.GetServoMinimum2()

    def SetServoMinimum3(self, pwmLevel):
        """
        SetServoMinimum3(pwmLevel)

        Sets the minimum PWM level for servo output #3
        This corresponds to position -1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        LIMIT CHECKING IS ALTERED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo minimum limit #3!")
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_3 = self.GetServoMinimum3()

    def SetServoMinimum4(self, pwmLevel):
        """
        SetServoMinimum4(pwmLevel)

        Sets the minimum PWM level for servo output #4
        This corresponds to position -1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        LIMIT CHECKING IS ALTERED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo minimum limit #4!")
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_4 = self.GetServoMinimum4()

    def SetServoMaximum1(self, pwmLevel):
        """
        SetServoMaximum1(pwmLevel)

        Sets the maximum PWM level for servo output #1
        This corresponds to position +1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        LIMIT CHECKING IS ALTERED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo maximum limit #1!")
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_1 = self.GetServoMaximum1()

    def SetServoMaximum2(self, pwmLevel):
        """
        SetServoMaximum2(pwmLevel)

        Sets the maximum PWM level for servo output #2
        This corresponds to position +1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        LIMIT CHECKING IS ALTERED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo maximum limit #2!")
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_2 = self.GetServoMaximum2()

    def SetServoMaximum3(self, pwmLevel):
        """
        SetServoMaximum3(pwmLevel)

        Sets the maximum PWM level for servo output #3
        This corresponds to position +1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        LIMIT CHECKING IS ALTERED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo maximum limit #3!")
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_3 = self.GetServoMaximum3()

    def SetServoMaximum4(self, pwmLevel):
        """
        SetServoMaximum4(pwmLevel)

        Sets the maximum PWM level for servo output #4
        This corresponds to position +1
        This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

        Setting values outside the range of the servo for extended periods of time can damage the servo
        LIMIT CHECKING IS ALTERED BY THIS COMMAND!
        We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo maximum limit #4!")
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_4 = self.GetServoMaximum4()

    def SetServoStartup1(self, pwmLevel):
        """
        SetServoStartup1(pwmLevel)

        Sets the startup PWM level for servo output #1
        This can be anywhere in the minimum to maximum range

        We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1
        This value is checked against the current servo limits before setting

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_1 < self.PWM_MAX_1:
            # Normal direction
            if pwmLevel < self.PWM_MIN_1:
                inRange = False
            elif pwmLevel > self.PWM_MAX_1:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_1:
                inRange = False
            elif pwmLevel < self.PWM_MAX_1:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print(
                "Servo #1 startup position %d is outside the limits of %d to %d"
                % (pwmLevel, self.PWM_MIN_1, self.PWM_MAX_1)
            )
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo startup position #1!")
        time.sleep(DELAY_AFTER_EEPROM)

    def SetServoStartup2(self, pwmLevel):
        """
        SetServoStartup2(pwmLevel)

        Sets the startup PWM level for servo output #2
        This can be anywhere in the minimum to maximum range

        We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2
        This value is checked against the current servo limits before setting

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_2 < self.PWM_MAX_2:
            # Normal direction
            if pwmLevel < self.PWM_MIN_2:
                inRange = False
            elif pwmLevel > self.PWM_MAX_2:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_2:
                inRange = False
            elif pwmLevel < self.PWM_MAX_2:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print(
                "Servo #2 startup position %d is outside the limits of %d to %d"
                % (pwmLevel, self.PWM_MIN_2, self.PWM_MAX_2)
            )
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo startup position #2!")
        time.sleep(DELAY_AFTER_EEPROM)

    def SetServoStartup3(self, pwmLevel):
        """
        SetServoStartup3(pwmLevel)

        Sets the startup PWM level for servo output #3
        This can be anywhere in the minimum to maximum range

        We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3
        This value is checked against the current servo limits before setting

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_3 < self.PWM_MAX_3:
            # Normal direction
            if pwmLevel < self.PWM_MIN_3:
                inRange = False
            elif pwmLevel > self.PWM_MAX_3:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_3:
                inRange = False
            elif pwmLevel < self.PWM_MAX_3:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print(
                "Servo #3 startup position %d is outside the limits of %d to %d"
                % (pwmLevel, self.PWM_MIN_3, self.PWM_MAX_3)
            )
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo startup position #3!")
        time.sleep(DELAY_AFTER_EEPROM)

    def SetServoStartup4(self, pwmLevel):
        """
        SetServoStartup4(pwmLevel)

        Sets the startup PWM level for servo output #4
        This can be anywhere in the minimum to maximum range

        We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4
        This value is checked against the current servo limits before setting

        The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
        e.g.
        2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
        4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
        3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
        5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_4 < self.PWM_MAX_4:
            # Normal direction
            if pwmLevel < self.PWM_MIN_4:
                inRange = False
            elif pwmLevel > self.PWM_MAX_4:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_4:
                inRange = False
            elif pwmLevel < self.PWM_MAX_4:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print(
                "Servo #4 startup position %d is outside the limits of %d to %d"
                % (pwmLevel, self.PWM_MIN_4, self.PWM_MAX_4)
            )
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print("Failed sending servo startup position #4!")
        time.sleep(DELAY_AFTER_EEPROM)

    def Help(self):
        """
        Help()

        Displays the names and descriptions of the various functions and settings provided
        """
        funcList = [
            UltraBorg.__dict__.get(a)
            for a in dir(UltraBorg)
            if isinstance(UltraBorg.__dict__.get(a), types.FunctionType)
        ]
        funcListSorted = sorted(funcList, key=lambda x: x.func_code.co_firstlineno)

        print(self.__doc__)
        print
        for func in funcListSorted:
            print("=== %s === %s" % (func.func_name, func.func_doc))


# === ./robot_mode_tree_visualise.py ===
import os
import py_trees
from src.behaviors.lidarchase import make_lidar_chase_sub_tree
from src.behaviors.utils import make_mode_sub_tree
from src.types.RobotModes import RobotMode
from src.behaviors.idle_mode import make_idle_mode_sub_tree
from src.behaviors.keyboard_mode import make_keyboard_mode_sub_tree
from src.behaviors.chase_mode import make_chase_mode_sub_tree
from unittest.mock import patch
from py_trees.common import VisibilityLevel, BlackBoxLevel
from py_trees.behaviour import Behaviour

mode_to_sub_tree = {
    RobotMode.IDLE: make_idle_mode_sub_tree(),
    RobotMode.KEYBOARD_CONTROL: make_keyboard_mode_sub_tree(),
    RobotMode.CHASE: make_chase_mode_sub_tree(),
    RobotMode.LIDARCHASE: make_lidar_chase_sub_tree(),
}


def tree_to_image(name: str, root: Behaviour, visability=VisibilityLevel.DETAIL):
    # generate images for each tree
    with patch("py_trees.console.has_unicode", return_value=False):
        py_trees.display.render_dot_tree(
            root, target_directory=IMAGE_DIR, name=name, visibility_level=visability
        )

    # render_dot_tree() generates .svg and .dot files as well, we dont need these
    images_to_delete = [
        os.path.join(IMAGE_DIR, f"{name}.svg"),
        os.path.join(IMAGE_DIR, f"{name}.dot"),
    ]
    for image in images_to_delete:
        os.remove(image)


def make_top_level_image():
    """
    Makes an image for the robot main root tree (without showing the tree for each mode)
    """
    sub_trees = []
    for robot_mode, sub_tree in mode_to_sub_tree.items():
        # change the mode's sub tree visability
        sub_tree.blackbox_level = BlackBoxLevel.BIG_PICTURE
        wrapped_sub_tree = make_mode_sub_tree(robot_mode, sub_tree)
        sub_trees.append(wrapped_sub_tree)

    # root node is a selector (will run first passing sub tree)
    # ie will run sub tree of the curent mode
    robot_root = py_trees.composites.Selector("root", memory=True, children=sub_trees)
    tree_to_image("main", robot_root, VisibilityLevel.BIG_PICTURE)


def make_mode_images():
    for mode, root in mode_to_sub_tree.items():
        image_name = f"{mode.name.lower()}_mode_tree"
        tree_to_image(image_name, root)


IMAGE_DIR = "robot_mode_images"
if __name__ == "__main__":
    os.makedirs(IMAGE_DIR, exist_ok=True)
    make_mode_images()
    make_top_level_image()


# === ./testing.py ===

import time
import UltraBorg
from src.robot.Robot import Robot
from src.robot.RobotFactory import RobotFactory
from src.types.HeadMovementDirection import HeadMovementDirection


CONFIG_FILE = "src/configs/config_maxine.yaml"

board = UltraBorg.UltraBorg()
board.i2cAddress = 10
board.Init()


def build_robot() -> Robot:
    robot_factory = RobotFactory(CONFIG_FILE)
    return robot_factory.build_robot()


robot = build_robot()

for t in range (50):
    robot.head_move_manager.perform_action(HeadMovementDirection.LEFT)
    time.sleep(0.1)


time.sleep(0.5)

for t in range (50):
    robot.head_move_manager.perform_action(HeadMovementDirection.RIGHT)
    time.sleep(0.1)





# === ./newnewmain.py ===
from math import radians
import math
import time

from matplotlib import pyplot as plt
import numpy as np
from src.path_finding.LidarPlot import LidarPlot
from src.path_finding.PolarAStarSearchVector import (
    a_star_search,
    prebuild_obstacles,
    convert_from_search_coord,
)
from src.path_finding.Position import Position
import cProfile
import pstats


def normalize_radians(angle):
    return angle % (2 * math.pi)


def get_obstacles_1():
    obstacles = []

    for angle in range(-128, 200, 1):
        obstacles.append(Position(normalize_radians(math.radians(angle)), 1000))

    for d in range(1000, 2500):
        obstacles.append(Position(math.radians(45), d))

    for d in range(1000, 1500):
        obstacles.append(Position(normalize_radians(math.radians(-55)), d))

    for angle in range(320, 70, -1):
        obstacles.append(Position(math.radians(angle), 1500))

    for angle in range(45, 300):
        obstacles.append(Position(math.radians(angle), 2275))

    return obstacles


def get_obstacles_2():
    obstacles = []

    for angle in range(-128, 200, 1):
        obstacles.append(Position(normalize_radians(math.radians(angle)), 1000))

    for d in range(1000, 2500):
        obstacles.append(Position(math.radians(45), d))

    for d in range(1000, 1500):
        obstacles.append(Position(normalize_radians(math.radians(-55)), d))

    return obstacles


def get_obstacles_2():
    obstacles = []

    for angle in range(-128, 200, 1):
        obstacles.append(Position(normalize_radians(math.radians(angle)), 1000))

    for d in range(1000, 2500):
        obstacles.append(Position(math.radians(45), d))

    for d in range(1000, 3000):
        obstacles.append(Position(math.radians(-45), d))

    for d in range(1000, 1500):
        obstacles.append(Position(normalize_radians(math.radians(-55)), d))

    return obstacles


def plot_res(plot, goal, path, obstacles):
    # obstacles = [
    #     convert_from_search_coord(*obs) for obs in list(prebuild_obstacles(obstacles))
    # ]

    plot.plot_destination(goal)
    plot.plot_obstacles(obstacles)
    plot.plot_path(path)
    plot.update_plot()


OBS = [get_obstacles_1(), get_obstacles_2()]

if __name__ == "__main__":

    plot = LidarPlot(3000)
    goal = Position(radians(22), 2000)

    obstacles = get_obstacles_1()
    while True:
        path = a_star_search(goal, obstacles)
        plot_res(plot, goal, path, obstacles)
        if np.random.random() < 0.5:
            idx = np.random.randint(len(OBS))
            obstacles = OBS[idx]
        time.sleep(1)


# === ./stop_lidar.py ===
from pyrplidar import PyRPlidar

lidar = PyRPlidar()
lidar.connect(port="/dev/ttyUSB0", baudrate=256000, timeout=5)
lidar.stop()
lidar.set_motor_pwm(0)
lidar.disconnect()

# === ./src/multithreading/TextToSpeechThread-old.py ===
from queue import Queue
from .LoopingThread import LoopingThread
import pyttsx3


class TextToSpeechThread(LoopingThread):
    """
    Thread running the Text to Speech engine
    """

    def __init__(self, queue: Queue, rate: int, voice: str, volume: int):
        """
        Initialises the thread

        arguments:
            - queue: the queue where text to say will be passed
            - rate: the engine rate
            - voice: the engine voice
            - volume: the engine volume
        """
        super().__init__(ms_delay=50)
        self.rate = rate
        self.queue = queue
        self.voice = voice
        self.volume = volume
        #self.engine = None

    def on_start(self):
        # initialise engine
        self.engine = pyttsx3.init()
        self.engine.setProperty("rate", self.rate)
        self.engine.setProperty("voice", self.voice)
        self.engine.setProperty("volume", self.volume)
        self.engine.startLoop(False)

    def tick(self):
        # if nothing in queue
        if self.queue.empty() and self.engine._inLoop:
        # Inside your tick() or thread loop:
            if self.queue!=None:
                self.engine.say('hi')
                self.engine.runAndWait()


            

        # add queue saying
        else:
            data = self.queue.get()
            self.engine.say(data)

    def on_finish(self):
        self.engine.endLoop()


# === ./src/multithreading/TextToSpeechThread-refact.py ===
from queue import Queue, Empty
from .LoopingThread import LoopingThread
import pyttsx3


class TextToSpeechThread(LoopingThread):
    """
    Thread running the Text to Speech engine
    """

    def __init__(self, queue: Queue, rate: int, voice: str, volume: int):
        """
        Initializes the thread

        arguments:
            - queue: the queue where text to say will be passed
            - rate: the engine rate
            - voice: the engine voice
            - volume: the engine volume
        """
        super().__init__(ms_delay=50)
        self.queue = queue
        self.rate = rate
        self.voice = voice
        self.volume = volume
        self.engine = None

    def on_start(self):
        # Initialize the pyttsx3 engine
        self.engine = pyttsx3.init()
        self.engine.setProperty("rate", self.rate)
        self.engine.setProperty("voice", self.voice)
        self.engine.setProperty("volume", self.volume)

        # Start the non-blocking event loop
        self.engine.startLoop(False)

    def tick(self):
        try:
            # Attempt to get something to say
            phrase = self.queue.get_nowait()
            print(f"TTS: Speaking -> {phrase}")
            self.engine.say(phrase)
        except Empty:
            pass

        # Always iterate the engine in each tick
        self.engine.iterate()

    def on_finish(self):
        self.engine.endLoop()

# === ./src/multithreading/LoopingThread.py ===
from threading import Thread
import time
from abc import ABC, abstractmethod

from . import THREAD_STOP_EVENT


class LoopingThread(Thread, ABC):
    """
    Base class for all looping threads.
    This is an abstract class that cannot be instantiated.
    """

    def __init__(self, ms_delay: int = 0) -> None:
        """
        Initialises the thread

        arguments:
            - ms_delay: the miliseconds to wait in between the polling loop
        """
        super().__init__(daemon=True)
        self.ms_delay = ms_delay

    def run(self):
        """
        Runs the thread
        """

        self.on_start()

        while True:
            # do work
            self.tick()

            # await delay
            time.sleep(self.ms_delay/1000)

            # stop if master event is set
            if THREAD_STOP_EVENT.is_set():
                break

        self.on_finish()

    @abstractmethod
    def on_finish(self):
        """
        Will be run upon finishing the thread.
        Children must implement this class
        """
        pass

    @abstractmethod
    def tick(self):
        """
        Is called at every loop of the thread.
        Children must implement this class
        """
        pass

    @abstractmethod
    def on_start(self):
        """
        Runs before the thread starts.
        Children must implement this class
        """
        pass


# === ./src/multithreading/I2CSensorThread.py ===
from typing import Dict, List
from .LoopingThread import LoopingThread
import UltraBorg_old
from ..sensors.I2CSensor import I2CSensor


class I2CSensorThread(LoopingThread):
    """
    A thread to poll I2C sensors
    """

    def __init__(self, sensors: List[I2CSensor], ms_delay: int = 50) -> None:
        """
        Initialises the thread

        arguments:
            - sensors: a list of sensors to update every tick
        """
        super().__init__(ms_delay)
        self.sensors = sensors

    def init_boards(self) -> Dict[int, UltraBorg_old.UltraBorg]:
        """
        Iniitiliases the UltraBorg boards needed
        """

        # add every board needed for the sensors provided
        boards = {}
        for sensor in self.sensors:
            if sensor.i2c_adress in boards:
                continue

            board = UltraBorg_old.UltraBorg()
            board.i2cAddress = sensor.i2c_adress
            board.Init()
            boards[sensor.i2c_adress] = board

        return boards

    def on_start(self):
        self.boards = self.init_boards()

    def tick(self):
        # update each sensor once each time
        for sensor in self.sensors:
            # find the board's update function for this sensor
            board = self.boards[sensor.i2c_adress]
            read_func = board.__getattribute__(f"GetDistance{sensor.port_index}")

            # get latest reading and update the sensor object
            reading = read_func()
            sensor.set_latest_reading(reading)

    def on_finish(self):
        pass


# === ./src/multithreading/TextToSpeechThread.py ===
from queue import Queue, Empty
from .LoopingThread import LoopingThread
import pyttsx3


class TextToSpeechThread(LoopingThread):
    """
    Thread running the Text to Speech engine using pyttsx3
    """

    def __init__(self, queue: Queue, rate: int, voice: str, volume: int):
        super().__init__(ms_delay=100)  # Slightly slower tick to reduce CPU use
        self.queue = queue
        self.rate = rate
        self.voice = voice
        self.volume = volume
        self.engine = None
        self.speaking = False

    def on_start(self):
        self.engine = pyttsx3.init()
        self.engine.setProperty("rate", self.rate)
        self.engine.setProperty("voice", self.voice)
        self.engine.setProperty("volume", self.volume)

    def tick(self):
        if not self.speaking:
            try:
                phrase = self.queue.get_nowait()
                print(f"[TTS] Speaking: {phrase}")
                self.speaking = True
                self.engine.say(phrase)
                self.engine.runAndWait()
                self.speaking = False
            except Empty:
                pass

    def on_finish(self):
        # No need to stop loop; runAndWait handles everything
        pass


# === ./src/multithreading/__init__.py ===
from threading import Event, enumerate, main_thread

# this event handles all the looping threads.
# when it is set, all looping threads will stop what they are doing
THREAD_STOP_EVENT = Event()


def stop_all_threads():
    """
    Sets the event to stop all threads
    """
    THREAD_STOP_EVENT.set()


def run_all_threads():
    """
    Clears the event to run all threads
    """
    THREAD_STOP_EVENT.clear()


def graceful_thread_exit(function):
    """
    Function decorator to handle exiting with multiple threads.
    When the program exits (either because of an exception or exiting), it will:
        - Set the event so all looping threads stop
        - Await all the running threads
    """

    def inner(*args, **kwargs):
        try:
            function(*args, **kwargs)
        finally:
            # set event to stop looping threads
            stop_all_threads()

            # join all threads except for main thread
            main = main_thread()
            for thread in enumerate():
                if thread != main:
                    print(f"awaiting thread {str(thread.__class__.__name__)}")
                    thread.join(timeout=5)

                    # check timeout occured when joining the thread
                    if thread.is_alive():
                        print(f"Error joining thread {str(thread.__class__.__name__)}")

    return inner


# === ./src/multithreading/PathFindingThread.py ===
from typing import List
from src.path_finding.PolarAStarSearch import a_star_search
from src.multithreading.LoopingThread import LoopingThread
from queue import LifoQueue
from src.path_finding.AStarSearch import AStarSearch
from src.path_finding.OccupancyGrid import OccupancyGrid
from src.path_finding.Position import Position


class PathFindingThread(LoopingThread):
    def __init__(
        self,
        target_pos_queue: LifoQueue,
        obstacle_queue: LifoQueue,
        path_queue: LifoQueue,
    ):
        super().__init__(0)
        self.target_pos_queue = target_pos_queue
        self.obstacle_queue = obstacle_queue
        self.path_queue = path_queue

    def on_start(self):
        pass

    def on_finish(self):
        pass

    def tick(self):
        if self.target_pos_queue.empty():
            return

        if self.obstacle_queue.empty():
            return

        target_person = self.target_pos_queue.get()
        obstacles = self.obstacle_queue.get()

        path = a_star_search(target_person, obstacles)
        self.path_queue.put(path)


# === ./src/multithreading/LidarSensorThread.py ===
from typing import List
from .LoopingThread import LoopingThread
from pyrplidar import PyRPlidar
import time
from math import radians
from ..path_finding.Position import Position


class LidarSensorThread(LoopingThread):
    def __init__(
        self,
        obstacles: List[Position],
        ms_delay: int = 0,
        scan_count: int = 750,
    ) -> None:
        super().__init__(ms_delay)
        self.scan_count = scan_count
        self.obstacles = obstacles

    def tick(self):
        obstacles = []

        #`for angle in range(270, 360, 3):
        #`    obstacles.append(Position(angle=radians(angle), distance=1000))
        #`
        #`for angle in range(0, 60, 3):
        #`    obstacles.append(Position(angle=radians(angle), distance=1000))
#`
#`
        #`for distance in range(1000, 3000):
        #`    obstacles.append(Position(angle=radians(30), distance=distance))
        #`    obstacles.append(Position(angle=radians(300), distance=distance))
#`
        #`for angle in range(95, 180, 3):
        #`    obstacles.append(Position(angle=radians(angle), distance=1000))
        for count, scan in enumerate(self.scan_generator()):
             if scan.quality < 10:
                 continue

             if scan.distance <= 40:
                 continue


             obstacles.append(
                 Position(angle=radians(scan.angle), distance=scan.distance)
             )

             if count >= self.scan_count:
                 break

        self.obstacles.clear()
        self.obstacles.extend(obstacles)

    def on_start(self):
        self.lidar = PyRPlidar()
        self.lidar.connect(port="/dev/ttyUSB0", baudrate=256000, timeout=3)
        self.lidar.set_motor_pwm(400)
        time.sleep(1)
        self.scan_generator = self.lidar.force_scan()
        
    def on_finish(self):
        self.lidar.stop()
        self.lidar.set_motor_pwm(0)
        self.lidar.disconnect()
        

# === ./src/UltraBorg.py ===
#!/usr/bin/env python3
# coding: utf-8
"""
This module is designed to communicate with the UltraBorg

Use by creating an instance of the class, call the Init function, then command as desired, e.g.
import UltraBorg
UB = UltraBorg.UltraBorg()
UB.Init()
# User code here, use UB to control the board

Multiple boards can be used when configured with different I²C addresses by creating multiple instances, e.g.
import UltraBorg
UB1 = UltraBorg.UltraBorg()
UB2 = UltraBorg.UltraBorg()
UB1.i2cAddress = 0x44
UB2.i2cAddress = 0x45
UB1.Init()
UB2.Init()
# User code here, use UB1 and UB2 to control each board separately

For explanations of the functions available call the Help function, e.g.
import UltraBorg
UB = UltraBorg.UltraBorg()
UB.Help()
See the website at www.piborg.org/ultraborg for more details
"""

# Import the libraries we need
import io
import fcntl
import types
import time

# Constant values
I2C_SLAVE               = 0x0703
I2C_MAX_LEN             = 4
USM_US_TO_MM            = 0.171500
PWM_MIN                 = 2000  # Should be a 1 ms burst, typical servo minimum
PWM_MAX                 = 4000  # Should be a 2 ms burst, typical servo maximum
DELAY_AFTER_EEPROM      = 0.01  # Time to wait after updating an EEPROM value before reading
PWM_UNSET               = 0xFFFF

I2C_ID_SERVO_USM        = 0x36

COMMAND_GET_TIME_USM1   = 1     # Get the time measured by ultrasonic #1 in us (0 for no detection)
COMMAND_GET_TIME_USM2   = 2     # Get the time measured by ultrasonic #2 in us (0 for no detection)
COMMAND_GET_TIME_USM3   = 3     # Get the time measured by ultrasonic #3 in us (0 for no detection)
COMMAND_GET_TIME_USM4   = 4     # Get the time measured by ultrasonic #4 in us (0 for no detection)
COMMAND_SET_PWM1        = 5     # Set the PWM duty cycle for drive #1 (16 bit)
COMMAND_GET_PWM1        = 6     # Get the PWM duty cycle for drive #1 (16 bit)
COMMAND_SET_PWM2        = 7     # Set the PWM duty cycle for drive #2 (16 bit)
COMMAND_GET_PWM2        = 8     # Get the PWM duty cycle for drive #2 (16 bit)
COMMAND_SET_PWM3        = 9     # Set the PWM duty cycle for drive #3 (16 bit)
COMMAND_GET_PWM3        = 10    # Get the PWM duty cycle for drive #3 (16 bit)
COMMAND_SET_PWM4        = 11    # Set the PWM duty cycle for drive #4 (16 bit)
COMMAND_GET_PWM4        = 12    # Get the PWM duty cycle for drive #4 (16 bit)
COMMAND_CALIBRATE_PWM1  = 13    # Set the PWM duty cycle for drive #1 (16 bit, ignores limit checks)
COMMAND_CALIBRATE_PWM2  = 14    # Set the PWM duty cycle for drive #2 (16 bit, ignores limit checks)
COMMAND_CALIBRATE_PWM3  = 15    # Set the PWM duty cycle for drive #3 (16 bit, ignores limit checks)
COMMAND_CALIBRATE_PWM4  = 16    # Set the PWM duty cycle for drive #4 (16 bit, ignores limit checks)
COMMAND_GET_PWM_MIN_1   = 17    # Get the minimum allowed PWM duty cycle for drive #1
COMMAND_GET_PWM_MAX_1   = 18    # Get the maximum allowed PWM duty cycle for drive #1
COMMAND_GET_PWM_BOOT_1  = 19    # Get the startup PWM duty cycle for drive #1
COMMAND_GET_PWM_MIN_2   = 20    # Get the minimum allowed PWM duty cycle for drive #2
COMMAND_GET_PWM_MAX_2   = 21    # Get the maximum allowed PWM duty cycle for drive #2
COMMAND_GET_PWM_BOOT_2  = 22    # Get the startup PWM duty cycle for drive #2
COMMAND_GET_PWM_MIN_3   = 23    # Get the minimum allowed PWM duty cycle for drive #3
COMMAND_GET_PWM_MAX_3   = 24    # Get the maximum allowed PWM duty cycle for drive #3
COMMAND_GET_PWM_BOOT_3  = 25    # Get the startup PWM duty cycle for drive #3
COMMAND_GET_PWM_MIN_4   = 26    # Get the minimum allowed PWM duty cycle for drive #4
COMMAND_GET_PWM_MAX_4   = 27    # Get the maximum allowed PWM duty cycle for drive #4
COMMAND_GET_PWM_BOOT_4  = 28    # Get the startup PWM duty cycle for drive #4
COMMAND_SET_PWM_MIN_1   = 29    # Set the minimum allowed PWM duty cycle for drive #1
COMMAND_SET_PWM_MAX_1   = 30    # Set the maximum allowed PWM duty cycle for drive #1
COMMAND_SET_PWM_BOOT_1  = 31    # Set the startup PWM duty cycle for drive #1
COMMAND_SET_PWM_MIN_2   = 32    # Set the minimum allowed PWM duty cycle for drive #2
COMMAND_SET_PWM_MAX_2   = 33    # Set the maximum allowed PWM duty cycle for drive #2
COMMAND_SET_PWM_BOOT_2  = 34    # Set the startup PWM duty cycle for drive #2
COMMAND_SET_PWM_MIN_3   = 35    # Set the minimum allowed PWM duty cycle for drive #3
COMMAND_SET_PWM_MAX_3   = 36    # Set the maximum allowed PWM duty cycle for drive #3
COMMAND_SET_PWM_BOOT_3  = 37    # Set the startup PWM duty cycle for drive #3
COMMAND_SET_PWM_MIN_4   = 38    # Set the minimum allowed PWM duty cycle for drive #4
COMMAND_SET_PWM_MAX_4   = 39    # Set the maximum allowed PWM duty cycle for drive #4
COMMAND_SET_PWM_BOOT_4  = 40    # Set the startup PWM duty cycle for drive #4
COMMAND_GET_FILTER_USM1 = 41    # Get the filtered time measured by ultrasonic #1 in us (0 for no detection)
COMMAND_GET_FILTER_USM2 = 42    # Get the filtered time measured by ultrasonic #2 in us (0 for no detection)
COMMAND_GET_FILTER_USM3 = 43    # Get the filtered time measured by ultrasonic #3 in us (0 for no detection)
COMMAND_GET_FILTER_USM4 = 44    # Get the filtered time measured by ultrasonic #4 in us (0 for no detection)
COMMAND_GET_ID          = 0x99  # Get the board identifier
COMMAND_SET_I2C_ADD     = 0xAA  # Set a new I2C address

COMMAND_VALUE_FWD       = 1     # I2C value representing forward
COMMAND_VALUE_REV       = 2     # I2C value representing reverse

COMMAND_VALUE_ON        = 1     # I2C value representing on
COMMAND_VALUE_OFF       = 0     # I2C value representing off


def ScanForUltraBorg(busNumber = 7):
    """
ScanForUltraBorg([busNumber])

Scans the I²C bus for a UltraBorg boards and returns a list of all usable addresses
The busNumber if supplied is which I²C bus to scan, 0 for Rev 1 boards, 1 for Rev 2 boards, if not supplied the default is 1
    """
    found = []
    print('Scanning I²C bus #%d' % (busNumber))
    bus = UltraBorg()
    for address in range(0x03, 0x78, 1):
        try:
            bus.InitBusOnly(busNumber, address)
            i2cRecv = bus.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
            if len(i2cRecv) == I2C_MAX_LEN:
                if i2cRecv[1] == I2C_ID_SERVO_USM:
                    print('Found UltraBorg at %02X' % (address))
                    found.append(address)
                else:
                    pass
            else:
                pass
        except KeyboardInterrupt:
            raise
        except:
            pass
    if len(found) == 0:
        print('No UltraBorg boards found, is bus #%d correct (should be 0 for Rev 1, 1 for Rev 2)' % (busNumber))
    elif len(found) == 1:
        print('1 UltraBorg board found')
    else:
        print('%d UltraBorg boards found' % (len(found)))
    return found


def SetNewAddress(newAddress, oldAddress = -1, busNumber = 7):
    """
SetNewAddress(newAddress, [oldAddress], [busNumber])

Scans the I²C bus for the first UltraBorg and sets it to a new I2C address
If oldAddress is supplied it will change the address of the board at that address rather than scanning the bus
The busNumber if supplied is which I²C bus to scan, 0 for Rev 1 boards, 1 for Rev 2 boards, if not supplied the default is 1
Warning, this new I²C address will still be used after resetting the power on the device
    """
    if newAddress < 0x03:
        print('Error, I²C addresses below 3 (0x03) are reserved, use an address between 3 (0x03) and 119 (0x77)')
        return
    elif newAddress > 0x77:
        print('Error, I²C addresses above 119 (0x77) are reserved, use an address between 3 (0x03) and 119 (0x77)')
        return
    if oldAddress < 0x0:
        found = ScanForUltraBorg(busNumber)
        if len(found) < 1:
            print('No UltraBorg boards found, cannot set a new I²C address!')
            return
        else:
            oldAddress = found[0]
    print('Changing I²C address from %02X to %02X (bus #%d)' % (oldAddress, newAddress, busNumber))
    bus = UltraBorg()
    bus.InitBusOnly(busNumber, oldAddress)
    try:
        i2cRecv = bus.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
        if len(i2cRecv) == I2C_MAX_LEN:
            if i2cRecv[1] == I2C_ID_SERVO_USM:
                foundChip = True
                print('Found UltraBorg at %02X' % (oldAddress))
            else:
                foundChip = False
                print('Found a device at %02X, but it is not a UltraBorg (ID %02X instead of %02X)' % (oldAddress, i2cRecv[1], I2C_ID_SERVO_USM))
        else:
            foundChip = False
            print('Missing UltraBorg at %02X' % (oldAddress))
    except KeyboardInterrupt:
        raise
    except:
        foundChip = False
        print('Missing UltraBorg at %02X' % (oldAddress))
    if foundChip:
        bus.RawWrite(COMMAND_SET_I2C_ADD, [newAddress])
        time.sleep(0.1)
        print('Address changed to %02X, attempting to talk with the new address' % (newAddress))
        try:
            bus.InitBusOnly(busNumber, newAddress)
            i2cRecv = bus.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
            if len(i2cRecv) == I2C_MAX_LEN:
                if i2cRecv[1] == I2C_ID_SERVO_USM:
                    foundChip = True
                    print('Found UltraBorg at %02X' % (newAddress))
                else:
                    foundChip = False
                    print('Found a device at %02X, but it is not a UltraBorg (ID %02X instead of %02X)' % (newAddress, i2cRecv[1], I2C_ID_SERVO_USM))
            else:
                foundChip = False
                print('Missing UltraBorg at %02X' % (newAddress))
        except KeyboardInterrupt:
            raise
        except:
            foundChip = False
            print('Missing UltraBorg at %02X' % (newAddress))
    if foundChip:
        print('New I²C address of %02X set successfully' % (newAddress))
    else:
        print('Failed to set new I²C address...')


# Class used to control UltraBorg
class UltraBorg:
    """
This module is designed to communicate with the UltraBorg

busNumber               I²C bus on which the UltraBorg is attached (Rev 1 is bus 0, Rev 2 is bus 1)
bus                     the smbus object used to talk to the I²C bus
i2cAddress              The I²C address of the UltraBorg chip to control
foundChip               True if the UltraBorg chip can be seen, False otherwise
printFunction           Function reference to call when printing text, if None "print" is used
    """

    # Shared values used by this class
    busNumber               = 7                # Check here for Rev 1 vs Rev 2 and select the correct bus
    i2cAddress              = I2C_ID_SERVO_USM  # I²C address, override for a different address
    foundChip               = False
    printFunction           = None
    i2cWrite                = None
    i2cRead                 = None

    # Default calibration adjustments to standard values
    PWM_MIN_1               = PWM_MIN
    PWM_MAX_1               = PWM_MAX
    PWM_MIN_2               = PWM_MIN
    PWM_MAX_2               = PWM_MAX
    PWM_MIN_3               = PWM_MIN
    PWM_MAX_3               = PWM_MAX
    PWM_MIN_4               = PWM_MIN
    PWM_MAX_4               = PWM_MAX

    def RawWrite(self, command, data):
        """
RawWrite(command, data)

Sends a raw command on the I2C bus to the UltraBorg
Command codes can be found at the top of UltraBorg.py, data is a list of 0 or more byte values

Under most circumstances you should use the appropriate function instead of RawWrite
        """
        rawOutput = [command]
        rawOutput.extend(data)
        rawOutput = bytes(rawOutput)
        self.i2cWrite.write(rawOutput)


    def RawRead(self, command, length, retryCount = 5):
        """
RawRead(command, length, [retryCount])

Reads data back from the UltraBorg after sending a GET command
Command codes can be found at the top of UltraBorg.py, length is the number of bytes to read back

The function checks that the first byte read back matches the requested command
If it does not it will retry the request until retryCount is exhausted (default is 3 times)

Under most circumstances you should use the appropriate function instead of RawRead
        """
        while retryCount > 0:
            try:
                self.RawWrite(command, [])
            except:
                # Delay on failed bus write
                retryCount -= 1
                time.sleep(0.1)
                continue
            time.sleep(0.000001)
            try:
                rawReply = self.i2cRead.read(length)
            except:
                # Delay on failed bus read
                retryCount -= 1
                time.sleep(0.1)
                continue
            reply = []
            for singleByte in rawReply:
                reply.append(singleByte)
            if command == reply[0]:
                # Reply successful
                break
            else:
                retryCount -= 1
        if retryCount > 0:
            return reply
        else:
            raise IOError('I2C read for command %d failed' % (command))


    def InitBusOnly(self, busNumber, address):
        """
InitBusOnly(busNumber, address)

Prepare the I2C driver for talking to a UltraBorg on the specified bus and I2C address
This call does not check the board is present or working, under most circumstances use Init() instead
        """
        self.busNumber = busNumber
        self.i2cAddress = address
        self.i2cRead = io.open("/dev/i2c-" + str(self.busNumber), "rb", buffering = 0)
        fcntl.ioctl(self.i2cRead, I2C_SLAVE, self.i2cAddress)
        self.i2cWrite = io.open("/dev/i2c-" + str(self.busNumber), "wb", buffering = 0)
        fcntl.ioctl(self.i2cWrite, I2C_SLAVE, self.i2cAddress)


    def Print(self, message):
        """
Print(message)

Wrapper used by the UltraBorg instance to print messages, will call printFunction if set, print otherwise
        """
        if self.printFunction == None:
            print(message)
        else:
            self.printFunction(message)


    def NoPrint(self, message):
        """
NoPrint(message)

Does nothing, intended for disabling diagnostic printout by using:
UB = UltraBorg.UltraBorg()
UB.printFunction = UB.NoPrint
        """
        pass


    def Init(self, tryOtherBus = False):
        """
Init([tryOtherBus])

Prepare the I2C driver for talking to the UltraBorg

If tryOtherBus is True, this function will attempt to use the other bus if the ThunderBorg devices can not be found on the current busNumber
    This is only really useful for early Raspberry Pi models!
        """
        self.Print('Loading UltraBorg on bus %d, address %02X' % (self.busNumber, self.i2cAddress))

        # Open the bus
        self.i2cRead = io.open("/dev/i2c-" + str(self.busNumber), "rb", buffering = 0)
        fcntl.ioctl(self.i2cRead, I2C_SLAVE, self.i2cAddress)
        self.i2cWrite = io.open("/dev/i2c-" + str(self.busNumber), "wb", buffering = 0)
        fcntl.ioctl(self.i2cWrite, I2C_SLAVE, self.i2cAddress)

        # Check for UltraBorg
        try:
            i2cRecv = self.RawRead(COMMAND_GET_ID, I2C_MAX_LEN)
            if len(i2cRecv) == I2C_MAX_LEN:
                if i2cRecv[1] == I2C_ID_SERVO_USM:
                    self.foundChip = True
                    self.Print('Found UltraBorg at %02X' % (self.i2cAddress))
                else:
                    self.foundChip = False
                    self.Print('Found a device at %02X, but it is not a UltraBorg (ID %02X instead of %02X)' % (self.i2cAddress, i2cRecv[1], I2C_ID_SERVO_USM))
            else:
                self.foundChip = False
                self.Print('Missing UltraBorg at %02X' % (self.i2cAddress))
        except KeyboardInterrupt:
            raise
        except:
            self.foundChip = False
            self.Print('Missing UltraBorg at %02X' % (self.i2cAddress))

        # See if we are missing chips
        if not self.foundChip:
            self.Print('UltraBorg was not found')
            if tryOtherBus:
                if self.busNumber == 1:
                    self.busNumber = 7
                else:
                    self.busNumber = 1
                self.Print('Trying bus %d instead' % (self.busNumber))
                self.Init(False)
            else:
                self.Print('Are you sure your UltraBorg is properly attached, the correct address is used, and the I2C drivers are running?')
                self.bus = None
        else:
            self.Print('UltraBorg loaded on bus %d' % (self.busNumber))

        # Read the calibration settings from the UltraBorg
        self.PWM_MIN_1 = self.GetWithRetry(self.GetServoMinimum1, 5)
        self.PWM_MAX_1 = self.GetWithRetry(self.GetServoMaximum1, 5)
        self.PWM_MIN_2 = self.GetWithRetry(self.GetServoMinimum2, 5)
        self.PWM_MAX_2 = self.GetWithRetry(self.GetServoMaximum2, 5)
        self.PWM_MIN_3 = self.GetWithRetry(self.GetServoMinimum3, 5)
        self.PWM_MAX_3 = self.GetWithRetry(self.GetServoMaximum3, 5)
        self.PWM_MIN_4 = self.GetWithRetry(self.GetServoMinimum4, 5)
        self.PWM_MAX_4 = self.GetWithRetry(self.GetServoMaximum4, 5)


    def GetWithRetry(self, function, count):
        """
value = GetWithRetry(function, count)

Attempts to read a value multiple times before giving up
Pass a get function with no parameters
e.g.
distance = GetWithRetry(UB.GetDistance1, 5)
Will try UB.GetDistance1() upto 5 times, returning when it gets a value
Useful for ensuring a read is successful
        """
        value = None
        for i in range(count):
            okay = True
            try:
                value = function()
            except KeyboardInterrupt:
                raise
            except:
                okay = False
            if okay:
                break
        return value


    def SetWithRetry(self, setFunction, getFunction, value, count):
        """
worked = SetWithRetry(setFunction, getFunction, value, count)

Attempts to write a value multiple times before giving up
Pass a set function with one parameter, and a get function no parameters
The get function will be used to check if the set worked, if not it will be repeated
e.g.
worked = SetWithRetry(UB.SetServoMinimum1, UB.GetServoMinimum1, 2000, 5)
Will try UB.SetServoMinimum1(2000) upto 5 times, returning when UB.GetServoMinimum1 returns 2000.
Useful for ensuring a write is successful
        """
        for i in range(count):
            okay = True
            try:
                setFunction(value)
                readValue = getFunction()
            except KeyboardInterrupt:
                raise
            except:
                okay = False
            if okay:
                if readValue == value:
                    break
                else:
                    okay = False
        return okay


    def GetDistance1(self):
        """
distance = GetDistance1()

Gets the filtered distance for ultrasonic module #1 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
If you need a faster response try GetRawDistance1 instead (no filtering)
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #1 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetDistance2(self):
        """
distance = GetDistance2()

Gets the filtered distance for ultrasonic module #2 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
If you need a faster response try GetRawDistance2 instead (no filtering)
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #2 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetDistance3(self):
        """
distance = GetDistance3()

Gets the filtered distance for ultrasonic module #3 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
If you need a faster response try GetRawDistance3 instead (no filtering)
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #3 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetDistance4(self):
        """
distance = GetDistance4()

Gets the filtered distance for ultrasonic module #4 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
If you need a faster response try GetRawDistance4 instead (no filtering)
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_FILTER_USM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #4 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM



    def GetRawDistance1(self):
        """
distance = GetRawDistance1()

Gets the raw distance for ultrasonic module #1 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
For a filtered (less jumpy) reading use GetDistance1
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #1 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetRawDistance2(self):
        """
distance = GetRawDistance2()

Gets the raw distance for ultrasonic module #2 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
For a filtered (less jumpy) reading use GetDistance2
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #2 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetRawDistance3(self):
        """
distance = GetRawDistance3()

Gets the raw distance for ultrasonic module #3 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
For a filtered (less jumpy) reading use GetDistance3
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #3 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetRawDistance4(self):
        """
distance = GetRawDistance4()

Gets the distance for ultrasonic module #4 in millimeters
Returns 0 for no object detected or no ultrasonic module attached
For a filtered (less jumpy) reading use GetDistance4
e.g.
0     -> No object in range
25    -> Object 25 mm away
1000  -> Object 1000 mm (1 m) away
3500  -> Object 3500 mm (3.5 m) away
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_TIME_USM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading ultrasonic #4 distance!')
            return

        time_us = (i2cRecv[1] << 8) + i2cRecv[2]
        if time_us == 65535:
            time_us = 0
        return time_us * USM_US_TO_MM


    def GetServoPosition1(self):
        """
position = GetServoPosition1()

Gets the drive position for servo output #1
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo output #1!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_1) / (self.PWM_MAX_1 - self.PWM_MIN_1)
        return (2.0 * powerOut) - 1.0


    def GetServoPosition2(self):
        """
position = GetServoPosition2()

Gets the drive position for servo output #2
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo output #2!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_2) / (self.PWM_MAX_2 - self.PWM_MIN_2)
        return (2.0 * powerOut) - 1.0


    def GetServoPosition3(self):
        """
position = GetServoPosition3()

Gets the drive position for servo output #3
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo output #3!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_3) / (self.PWM_MAX_3 - self.PWM_MIN_3)
        return (2.0 * powerOut) - 1.0


    def GetServoPosition4(self):
        """
position = GetServoPosition4()

Gets the drive position for servo output #4
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo output #4!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        powerOut = (float(pwmDuty) - self.PWM_MIN_4) / (self.PWM_MAX_4 - self.PWM_MIN_4)
        return (2.0 * powerOut) - 1.0


    def SetServoPosition1(self, position):
        """
SetServoPosition1(position)

Sets the drive position for servo output #1
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_1 - self.PWM_MIN_1)) + self.PWM_MIN_1)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo output #1!')


    def SetServoPosition2(self, position):
        """
SetServoPosition2(position)

Sets the drive position for servo output #2
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_2 - self.PWM_MIN_2)) + self.PWM_MIN_2)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo output #2!')


    def SetServoPosition3(self, position):
        """
SetServoPosition3(position)

Sets the drive position for servo output #3
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_3 - self.PWM_MIN_3)) + self.PWM_MIN_3)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo output #3!')


    def SetServoPosition4(self, position):
        """
SetServoPosition4(position)

Sets the drive position for servo output #4
0 is central, -1 is maximum left, +1 is maximum right
e.g.
0     -> Central
0.5   -> 50% to the right
1     -> 100% to the right
-0.75 -> 75% to the left
        """
        powerOut = (position + 1.0) / 2.0
        pwmDuty = int((powerOut * (self.PWM_MAX_4 - self.PWM_MIN_4)) + self.PWM_MIN_4)
        pwmDutyLow = pwmDuty & 0xFF
        pwmDutyHigh = (pwmDuty >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo output #1!')


    def GetServoMinimum1(self):
        """
pwmLevel = GetServoMinimum1()

Gets the minimum PWM level for servo output #1
This corresponds to position -1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #1 minimum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMinimum2(self):
        """
pwmLevel = GetServoMinimum2()

Gets the minimum PWM level for servo output #2
This corresponds to position -1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #2 minimum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMinimum3(self):
        """
pwmLevel = GetServoMinimum3()

Gets the minimum PWM level for servo output #3
This corresponds to position -1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #3 minimum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMinimum4(self):
        """
pwmLevel = GetServoMinimum4()

Gets the minimum PWM level for servo output #4
This corresponds to position -1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MIN_4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #4 minimum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMaximum1(self):
        """
pwmLevel = GetServoMaximum1()

Gets the maximum PWM level for servo output #1
This corresponds to position +1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #1 maximum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMaximum2(self):
        """
pwmLevel = GetServoMaximum2()

Gets the maximum PWM level for servo output #2
This corresponds to position +1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #2 maximum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMaximum3(self):
        """
pwmLevel = GetServoMaximum3()

Gets the maximum PWM level for servo output #3
This corresponds to position +1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #3 maximum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoMaximum4(self):
        """
pwmLevel = GetServoMaximum4()

Gets the maximum PWM level for servo output #4
This corresponds to position +1
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_MAX_4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #4 maximum burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoStartup1(self):
        """
pwmLevel = GetServoStartup1()

Gets the startup PWM level for servo output #1
This can be anywhere in the minimum to maximum range
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #1 startup burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoStartup2(self):
        """
pwmLevel = GetServoStartup2()

Gets the startup PWM level for servo output #2
This can be anywhere in the minimum to maximum range
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #2 startup burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoStartup3(self):
        """
pwmLevel = GetServoStartup3()

Gets the startup PWM level for servo output #3
This can be anywhere in the minimum to maximum range
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #3 startup burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def GetServoStartup4(self):
        """
pwmLevel = GetServoStartup4()

Gets the startup PWM level for servo output #4
This can be anywhere in the minimum to maximum range
The value is an integer where 2000 represents a 1 ms servo burst
e.g.
2000  -> 1 ms servo burst, typical shortest burst
4000  -> 2 ms servo burst, typical longest burst
3000  -> 1.5 ms servo burst, typical centre, 
5000  -> 2.5 ms servo burst, higher than typical longest burst 
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM_BOOT_4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading servo #4 startup burst!')
            return

        return (i2cRecv[1] << 8) + i2cRecv[2]


    def CalibrateServoPosition1(self, pwmLevel):
        """
CalibrateServoPosition1(pwmLevel)

Sets the raw PWM level for servo output #1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending calibration servo output #1!')


    def CalibrateServoPosition2(self, pwmLevel):
        """
CalibrateServoPosition2(pwmLevel)

Sets the raw PWM level for servo output #2
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending calibration servo output #2!')


    def CalibrateServoPosition3(self, pwmLevel):
        """
CalibrateServoPosition3(pwmLevel)

Sets the raw PWM level for servo output #3
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending calibration servo output #3!')


    def CalibrateServoPosition4(self, pwmLevel):
        """
CalibrateServoPosition4(pwmLevel)

Sets the raw PWM level for servo output #4
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
NO LIMIT CHECKING IS PERFORMED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_CALIBRATE_PWM4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending calibration servo output #4!')


    def GetRawServoPosition1(self):
        """
pwmLevel = GetRawServoPosition1()

Gets the raw PWM level for servo output #1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

This value requires interpreting into an actual servo position, this is already done by GetServoPosition1
We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM1, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading raw servo output #1!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty


    def GetRawServoPosition2(self):
        """
pwmLevel = GetRawServoPosition2()

Gets the raw PWM level for servo output #2
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

This value requires interpreting into an actual servo position, this is already done by GetServoPosition2
We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM2, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading raw servo output #2!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty


    def GetRawServoPosition3(self):
        """
pwmLevel = GetRawServoPosition3()

Gets the raw PWM level for servo output #3
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

This value requires interpreting into an actual servo position, this is already done by GetServoPosition3
We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM3, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading raw servo output #3!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty


    def GetRawServoPosition4(self):
        """
pwmLevel = GetRawServoPosition4()

Gets the raw PWM level for servo output #4
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

This value requires interpreting into an actual servo position, this is already done by GetServoPosition4
We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        try:
            i2cRecv = self.RawRead(COMMAND_GET_PWM4, I2C_MAX_LEN)
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed reading raw servo output #4!')
            return

        pwmDuty = (i2cRecv[1] << 8) + i2cRecv[2]
        return pwmDuty


    def SetServoMinimum1(self, pwmLevel):
        """
SetServoMinimum1(pwmLevel)

Sets the minimum PWM level for servo output #1
This corresponds to position -1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo minimum limit #1!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_1 = self.GetServoMinimum1()


    def SetServoMinimum2(self, pwmLevel):
        """
SetServoMinimum2(pwmLevel)

Sets the minimum PWM level for servo output #2
This corresponds to position -1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo minimum limit #2!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_2 = self.GetServoMinimum2()


    def SetServoMinimum3(self, pwmLevel):
        """
SetServoMinimum3(pwmLevel)

Sets the minimum PWM level for servo output #3
This corresponds to position -1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo minimum limit #3!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_3 = self.GetServoMinimum3()


    def SetServoMinimum4(self, pwmLevel):
        """
SetServoMinimum4(pwmLevel)

Sets the minimum PWM level for servo output #4
This corresponds to position -1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MIN_4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo minimum limit #4!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MIN_4 = self.GetServoMinimum4()


    def SetServoMaximum1(self, pwmLevel):
        """
SetServoMaximum1(pwmLevel)

Sets the maximum PWM level for servo output #1
This corresponds to position +1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo maximum limit #1!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_1 = self.GetServoMaximum1()


    def SetServoMaximum2(self, pwmLevel):
        """
SetServoMaximum2(pwmLevel)

Sets the maximum PWM level for servo output #2
This corresponds to position +1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo maximum limit #2!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_2 = self.GetServoMaximum2()


    def SetServoMaximum3(self, pwmLevel):
        """
SetServoMaximum3(pwmLevel)

Sets the maximum PWM level for servo output #3
This corresponds to position +1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo maximum limit #3!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_3 = self.GetServoMaximum3()


    def SetServoMaximum4(self, pwmLevel):
        """
SetServoMaximum4(pwmLevel)

Sets the maximum PWM level for servo output #4
This corresponds to position +1
This value can be set anywhere from 0 for a 0% duty cycle to 65535 for a 100% duty cycle

Setting values outside the range of the servo for extended periods of time can damage the servo
LIMIT CHECKING IS ALTERED BY THIS COMMAND!
We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF

        try:
            self.RawWrite(COMMAND_SET_PWM_MAX_4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo maximum limit #4!')
        time.sleep(DELAY_AFTER_EEPROM)
        self.PWM_MAX_4 = self.GetServoMaximum4()


    def SetServoStartup1(self, pwmLevel):
        """
SetServoStartup1(pwmLevel)

Sets the startup PWM level for servo output #1
This can be anywhere in the minimum to maximum range

We recommend using the tuning GUI for setting the servo limits for SetServoPosition1 / GetServoPosition1
This value is checked against the current servo limits before setting

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_1 < self.PWM_MAX_1:
            # Normal direction
            if pwmLevel < self.PWM_MIN_1:
                inRange = False
            elif pwmLevel > self.PWM_MAX_1:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_1:
                inRange = False
            elif pwmLevel < self.PWM_MAX_1:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print('Servo #1 startup position %d is outside the limits of %d to %d' % (pwmLevel, self.PWM_MIN_1, self.PWM_MAX_1))
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_1, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo startup position #1!')
        time.sleep(DELAY_AFTER_EEPROM)


    def SetServoStartup2(self, pwmLevel):
        """
SetServoStartup2(pwmLevel)

Sets the startup PWM level for servo output #2
This can be anywhere in the minimum to maximum range

We recommend using the tuning GUI for setting the servo limits for SetServoPosition2 / GetServoPosition2
This value is checked against the current servo limits before setting

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_2 < self.PWM_MAX_2:
            # Normal direction
            if pwmLevel < self.PWM_MIN_2:
                inRange = False
            elif pwmLevel > self.PWM_MAX_2:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_2:
                inRange = False
            elif pwmLevel < self.PWM_MAX_2:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print('Servo #2 startup position %d is outside the limits of %d to %d' % (pwmLevel, self.PWM_MIN_2, self.PWM_MAX_2))
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_2, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo startup position #2!')
        time.sleep(DELAY_AFTER_EEPROM)


    def SetServoStartup3(self, pwmLevel):
        """
SetServoStartup3(pwmLevel)

Sets the startup PWM level for servo output #3
This can be anywhere in the minimum to maximum range

We recommend using the tuning GUI for setting the servo limits for SetServoPosition3 / GetServoPosition3
This value is checked against the current servo limits before setting

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_3 < self.PWM_MAX_3:
            # Normal direction
            if pwmLevel < self.PWM_MIN_3:
                inRange = False
            elif pwmLevel > self.PWM_MAX_3:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_3:
                inRange = False
            elif pwmLevel < self.PWM_MAX_3:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print('Servo #3 startup position %d is outside the limits of %d to %d' % (pwmLevel, self.PWM_MIN_3, self.PWM_MAX_3))
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_3, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo startup position #3!')
        time.sleep(DELAY_AFTER_EEPROM)


    def SetServoStartup4(self, pwmLevel):
        """
SetServoStartup4(pwmLevel)

Sets the startup PWM level for servo output #4
This can be anywhere in the minimum to maximum range

We recommend using the tuning GUI for setting the servo limits for SetServoPosition4 / GetServoPosition4
This value is checked against the current servo limits before setting

The value is an integer where 2000 represents a 1ms servo burst, approximately 3% duty cycle
e.g.
2000  -> 1 ms servo burst, typical shortest burst, ~3% duty cycle
4000  -> 2 ms servo burst, typical longest burst, ~ 6.1% duty cycle
3000  -> 1.5 ms servo burst, typical centre, ~4.6% duty cycle
5000  -> 2.5 ms servo burst, higher than typical longest burst, ~ 7.6% duty cycle
        """
        pwmDutyLow = pwmLevel & 0xFF
        pwmDutyHigh = (pwmLevel >> 8) & 0xFF
        inRange = True

        if self.PWM_MIN_4 < self.PWM_MAX_4:
            # Normal direction
            if pwmLevel < self.PWM_MIN_4:
                inRange = False
            elif pwmLevel > self.PWM_MAX_4:
                inRange = False
        else:
            # Inverted direction
            if pwmLevel > self.PWM_MIN_4:
                inRange = False
            elif pwmLevel < self.PWM_MAX_4:
                inRange = False
        if pwmLevel == PWM_UNSET:
            # Force to unset behaviour (central)
            inRange = True

        if not inRange:
            print('Servo #4 startup position %d is outside the limits of %d to %d' % (pwmLevel, self.PWM_MIN_4, self.PWM_MAX_4))
            return

        try:
            self.RawWrite(COMMAND_SET_PWM_BOOT_4, [pwmDutyHigh, pwmDutyLow])
        except KeyboardInterrupt:
            raise
        except:
            self.Print('Failed sending servo startup position #4!')
        time.sleep(DELAY_AFTER_EEPROM)


    def Help(self):
        """
Help()

Displays the names and descriptions of the various functions and settings provided
        """
        funcList = [UltraBorg.__dict__.get(a) for a in dir(UltraBorg) if isinstance(UltraBorg.__dict__.get(a), types.FunctionType)]
        funcListSorted = sorted(funcList, key = lambda x: x.func_code.co_firstlineno)

        print(self.__doc__)
        print
        for func in funcListSorted:
            print('=== %s === %s' % (func.func_name, func.func_doc))




# === ./src/depth_ai/ObjectDetectionPipeline.py ===
from enum import Enum
from ..depth_ai.DepthAiPipeline import DepthAiPipeline
import depthai as dai
from typing import Dict


class ObjectDetectionPipeline(DepthAiPipeline):
    """
    A DAI pipeline for object detection
    """

    # the path to the spacial weights
    SPATIAL_NETWORK_PATH = (
        "/home/jetson/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_6shave.blob"
    )

    # the keys for each type of queue outputed by this pipeline
    class ObjectDetectionQueues(Enum):
        IMAGE_OUT = "image_out"
        OBJECT_OUT = "object_out"
        DEPTH_OUT = "depth_out"

    def make_detection_net(self) -> dai.node.MobileNetSpatialDetectionNetwork:
        """
        Makes a spatial detection network
        """
        spatial_net: dai.node.MobileNetSpatialDetectionNetwork = self.make_net(
            dai.node.MobileNetSpatialDetectionNetwork,
            self.SPATIAL_NETWORK_PATH,
            0.75,
            10,
            blocking=False,
        )

        spatial_net.setBoundingBoxScaleFactor(0.5)
        spatial_net.setDepthLowerThreshold(100)
        spatial_net.setDepthUpperThreshold(5000)

        return spatial_net

    def configure(self):
        # build sensors
        left_mono = self.make_mono_camera("left")
        right_mono = self.make_mono_camera("right")
        stero_sensor = self.make_stereo_sensor()

        # image manip and detection net
        image_manip = self.make_image_manip(300, False)
        detection_net = self.make_detection_net()

        # link
        left_mono.out.link(stero_sensor.left)
        right_mono.out.link(stero_sensor.right)
        image_manip.out.link(detection_net.input)
        stero_sensor.rectifiedRight.link(image_manip.inputImage)
        stero_sensor.depth.link(detection_net.inputDepth)

        # build outputs
        self.make_output(image_manip.out, self.ObjectDetectionQueues.IMAGE_OUT.value)
        self.make_output(detection_net.out, self.ObjectDetectionQueues.OBJECT_OUT.value)
        self.make_output(
            detection_net.passthroughDepth, self.ObjectDetectionQueues.DEPTH_OUT.value
        )

    def get_output_queues(
        self, device: dai.Device
    ) -> Dict[ObjectDetectionQueues, dai.DataOutputQueue]:
        # returns the queue for each type in object detection
        return {
            queue: device.getOutputQueue(queue.value, maxSize=8, blocking=False)
            for queue in self.ObjectDetectionQueues
        }


# === ./src/depth_ai/ObjectDetectionReading2.py ===
from typing import List
from .ObjectDetectionPipeline import ObjectDetectionPipeline
from .CameraReading import CameraReading

import depthai as dai
import cv2
import numpy as np


class ObjectDetectionReading2(CameraReading):
    """
    Camera reading from Object Detection Camera mode
    """

    # the key to get the image from
    IMAGE_KEY = ObjectDetectionPipeline.ObjectDetectionQueues.IMAGE_OUT

    # the labels from object detection
    LABEL_MAP = [
        "background",
        "aeroplane",
        "bicycle",
        "bird",
        "boat",
        "bottle",
        "bus",
        "car",
        "cat",
        "chair",
        "cow",
        "diningtable",
        "dog",
        "horse",
        "motorbike",
        "person",
        "pottedplant",
        "sheep",
        "sofa",
        "train",
        "tvmonitor",
    ]

    def get_people_locations(self) -> List[dai.SpatialImgDetection]:
        objects = self.queue_readings[
            ObjectDetectionPipeline.ObjectDetectionQueues.OBJECT_OUT
        ]
        people = []
        for object in objects.detections:
            people.append(object)
        return people
    
    def get_object_locations(self) -> List[dai.SpatialImgDetection]:
        objects = self.queue_readings[
            ObjectDetectionPipeline.ObjectDetectionQueues.OBJECT_OUT
        ]
        people = []
        for object in objects.detections:
            if self.LABEL_MAP[object.label] == "person":
                people.append(object)

        return people
    
    def get_object_locations(self) -> List[dai.SpatialImgDetection]:
        objects = self.queue_readings[
            ObjectDetectionPipeline.ObjectDetectionQueues.OBJECT_OUT
        ]
        allobjects = []
        for object in objects.detections:
            allobjects.append(object)
        return allobjects

    def get_frame(self):
        """
        Ovverides the get_frame function by plotting a bounding box of detected objects along with their label
        """
        # get the base image from parent class
        base_frame = super().get_frame()

        # converts an x, y to a coordinate on the image
        def point_to_image_coord(x, y):
            return (np.array((x, y)).clip(0, 1) * base_frame.shape[:2]).astype(np.int32)

        objects = self.queue_readings[
            ObjectDetectionPipeline.ObjectDetectionQueues.OBJECT_OUT
        ]

        # red, blue, green
        colors = ((255, 0, 0), (0, 255, 0), (0, 0, 255))

        for color, object in zip(colors, objects.detections):
            # get bounding box coordinates
            start_point = point_to_image_coord(object.xmin, object.ymin)
            end_point = point_to_image_coord(object.xmax, object.ymax)

            # build label text
            label = self.LABEL_MAP[object.label]
            confidence = object.confidence
            text = f"{label} ({round(confidence, 2)})"

            # add bounding box rectangle on image
            base_frame = cv2.rectangle(
                base_frame,
                start_point,
                end_point,
                color=color,
                thickness=1,
            )

            # add label on image
            base_frame = cv2.putText(
                base_frame,
                text,
                start_point + np.array((0, 20)),
                cv2.FONT_HERSHEY_TRIPLEX,
                0.5,
                color,
            )

        return base_frame


# === ./src/depth_ai/__init__.py ===
    

# === ./src/depth_ai/CameraReading.py ===
class CameraReading:
    """
    The base class for all camera reading
    All camera readings should be able to get a cv2 frame to display the robots view
    """

    # the key of the queue readings that corresponds to the image
    IMAGE_KEY = ""

    def __init__(self, queue_readings) -> None:
        """
        Initialises the reading

        arguments:
            - queue_readings: The latest reading from the camera for each type of output
        """
        self.queue_readings = queue_readings

    def get_frame(self):
        """
        Returns the latest cv frame
        """
        image = self.queue_readings[self.IMAGE_KEY]
        return image.getCvFrame()


# === ./src/depth_ai/DepthAiPipeline.py ===
from typing import Dict
from depthai import Pipeline
import depthai as dai
from depthai import MonoCameraProperties, ColorCameraProperties
from abc import ABC, abstractmethod


class DepthAiPipeline(ABC):
    """
    Base class for all DepthAI Pipelines
    Children must implement the configure() method, which configures the pipeline
    This class contains helper methods to create various depth ai nodes
    """

    def __init__(self) -> None:
        """
        Initiliases the pipeline
        Will call configure() which will build the pipeline
        """
        self.pipeline = None
        self.pipeline = Pipeline()
        self.configure()

    @abstractmethod
    def configure(self):
        """
        Configures the pipeline
        All children must implement this method
        """
        pass

    @abstractmethod
    def get_output_queues(self, device: dai.Device) -> Dict[str, dai.DataOutputQueue]:
        """
        Returns a dictionary of output queues that represent the predictions from the pipeline
        all children must implement this

        arguments:
            - device: the current dai device
        """
        pass

    def make_mono_camera(self, name: str) -> dai.node.MonoCamera:
        """
        Builds a mono camera
        """
        camera = self.pipeline.create(dai.node.MonoCamera)
        camera.setResolution(MonoCameraProperties.SensorResolution.THE_400_P)
        camera.setFps(21)
        camera.setCamera(name)

        return camera

    def make_color_camera(self) -> dai.node.ColorCamera:
        """
        Builds a colour camera
        """
        camera = self.pipeline.create(dai.node.ColorCamera)
        camera.setResolution(ColorCameraProperties.resolution.THE_1080_P)
        camera.setInterleaved(False)
        camera.setBoardSocket(dai.CameraBoardSocket.RGB)

        return camera

    def make_stereo_sensor(self) -> dai.node.StereoDepth:
        """
        Builds a stero sensor
        """
        stereo = self.pipeline.create(dai.node.StereoDepth)
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        stereo.setSubpixel(False)

        return stereo

    def make_image_manip(
        self, resize_dim: int, keep_aspect: bool
    ) -> dai.node.ImageManip:
        """
        Builds an ImageManip node

        arguments:
            - resize_dim: the dimension to resize to
            - keep_aspect: wether to keep the aspect ratio or not
        """
        manip = self.pipeline.create(dai.node.ImageManip)
        manip.initialConfig.setResize(resize_dim, resize_dim)
        manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        manip.setKeepAspectRatio(keep_aspect)

        return manip

    def make_net(
        self,
        network: type[dai.node.DetectionNetwork],
        blob_path: str,
        threshold: float,
        queue_size: int,
        blocking: bool,
    ) -> dai.node.DetectionNetwork:
        """
        Builds a depth ai NN.

        arguments:
            - network: the type of NN to build
            - blob_path: the path of the NN weights
            - threshold: the threshold for detection
            - queue_size: the size of the queue
            - blocking: wether acessing the queue is blocking
        """
        net = self.pipeline.create(network)
        net.setConfidenceThreshold(threshold)
        net.setBlobPath(blob_path)
        net.input.setQueueSize(queue_size)
        net.input.setBlocking(blocking)

        return net

    def make_output(self, node_output: dai.Node.Output, name: str) -> dai.node.XLinkOut:
        """
        Builds a XLinkOut node and links a previous output to its input

        arguments:
            - node_output: the previous layer to link the input to
            - name: the name of the output
        """
        output = self.pipeline.create(dai.node.XLinkOut)
        output.setStreamName(name)

        node_output.link(output.input)

        return output


# === ./src/depth_ai/ObjectDetectionReading.py ===
from typing import List
from .ObjectDetectionPipeline import ObjectDetectionPipeline
from .CameraReading import CameraReading
from py_trees.common import Status
import py_trees
import depthai as dai
import cv2
import numpy as np


class ObjectDetectionReading(CameraReading):
    """
    Camera reading from Object Detection Camera mode
    """

    # the key to get the image from
    IMAGE_KEY = ObjectDetectionPipeline.ObjectDetectionQueues.IMAGE_OUT

    # the labels from object detection
    LABEL_MAP = [
        "background",
        "aeroplane",
        "bicycle",
        "bird",
        "boat",
        "bottle",
        "bus",
        "car",
        "cat",
        "chair",
        "cow",
        "diningtable",
        "dog",
        "horse",
        "motorbike",
        "person",
        "pottedplant",
        "sheep",
        "sofa",
        "train",
        "tvmonitor",
    ]

    def get_people_locations(self) -> List[dai.SpatialImgDetection]:
        objects = self.queue_readings[
            ObjectDetectionPipeline.ObjectDetectionQueues.OBJECT_OUT
        ]
        people = []
        for object in objects.detections:
            if object.label == self.LABEL_MAP.index("person"):
                people.append(object)

        return people
    
    def get_object_locations(self) -> List[dai.SpatialImgDetection]:
        objects = self.queue_readings[
            ObjectDetectionPipeline.ObjectDetectionQueues.OBJECT_OUT
        ]
        allobjects = []
        for object in objects.detections:
            allobjects.append(object)
        return allobjects

    def get_frame(self):
        """
        Ovverides the get_frame function by plotting a bounding box of detected objects along with their label
        """
        # get the base image from parent class
        base_frame = super().get_frame()

        # converts an x, y to a coordinate on the image
        def point_to_image_coord(x, y):
            return (np.array((x, y)).clip(0, 1) * base_frame.shape[:2]).astype(np.int32)

        objects = self.queue_readings[
            ObjectDetectionPipeline.ObjectDetectionQueues.OBJECT_OUT
        ]

        # red, blue, green
        colors = ((255, 0, 0), (0, 255, 0), (0, 0, 255))

        for color, object in zip(colors, objects.detections):
            # get bounding box coordinates
            start_point = point_to_image_coord(object.xmin, object.ymin)
            end_point = point_to_image_coord(object.xmax, object.ymax)

            # build label text
            label = self.LABEL_MAP[object.label]
            confidence = object.confidence
            text = f"{label} ({round(confidence, 2)})"

            # add bounding box rectangle on image
            base_frame = cv2.rectangle(
                base_frame,
                start_point,
                end_point,
                color=color,
                thickness=1,
            )

            # add label on image
            base_frame = cv2.putText(
                base_frame,
                text,
                start_point + np.array((0, 20)),
                cv2.FONT_HERSHEY_TRIPLEX,
                0.5,
                color,
            )

        return base_frame


# === ./src/behaviors/lidarchase/PathFind.py ===
import multiprocessing
import multiprocessing.connection
from src.behaviors.lidarchase.run_search import run_search
from src.behaviors.MaxineBehavior import MaxineBehavior
from src.path_finding.AStarSearch import AStarSearch
from src.path_finding.OccupancyGrid import OccupancyGrid
from src.path_finding.Position import Position
import atexit

import depthai as dai
import py_trees
from py_trees.common import Status


from math import radians
from typing import List
import datetime

class PathFind(MaxineBehavior):
    

    def __init__(self):
        super().__init__("Path find")
        self.blackboard.register_key(
            "TARGET_PERSON", access=py_trees.common.Access.WRITE
        )
        self.planning = None
        self.blackboard.register_key("PATH", access=py_trees.common.Access.WRITE)
        
    def setup_planning_thread(self) -> None:
        self.search_connection, search_connection = multiprocessing.Pipe()
        self.path_connection, path_connection = multiprocessing.Pipe()
        self.planning = multiprocessing.Process(
            target=run_search, args=(search_connection, path_connection)
        )
        atexit.register(self.planning.kill)
        self.planning.start()
        self.is_running = False

    def initialise(self) -> None:
        if self.planning is None:
            self.setup_planning_thread()

        try:
            target_person: dai.SpatialImgDetection = self.blackboard.get(
                "TARGET_PERSON"
            )
            distance = target_person.spatialCoordinates.z - 650
            x_center = self.get_target_x_center()
            angle = radians((x_center * 2 - 1) * 63.5)

            destination = Position(angle=angle, distance=distance)
            # destination = Position(angle=0, distance=2000)

            robot = self.get_robot()
            lidar = robot.lidar_sensor
            readings = lidar.get_reading()

            self.search_connection.send((destination, readings))
            self.is_running = True
            self.start_time = datetime.datetime.now()

        except KeyError:
            self.is_running = False
            # target person doesnt exist
            return

    def get_target_x_center(self) -> float:
        """
        Returns the middle of the current targets x axis coordinate
        """
        target_person: dai.SpatialImgDetection = self.blackboard.get("TARGET_PERSON")
        x_diff = target_person.xmax - target_person.xmin
        return target_person.xmin + (x_diff / 2)

    
    def has_run_too_long(self):
        current_time = datetime.datetime.now()
        time_spent = current_time - self.start_time
        time_spent_seconds = time_spent.total_seconds()
        
        has_run_too_long = time_spent_seconds >= 0.11
        if has_run_too_long:
            # kill the process
            self.planning.kill()
            self.setup_planning_thread()
        
        return has_run_too_long

    
    def update(self) -> Status:
        if self.has_run_too_long():
            # print("Ran too long, killing off search")
            return Status.FAILURE

        if not self.is_running:
            return Status.FAILURE

        if not self.path_connection.poll():
            # print("No path found yet")
            return Status.RUNNING

        path = self.path_connection.recv()
        if path is not None:
            self.blackboard.set("PATH", path)
        self.is_running = False

        return Status.SUCCESS

    def terminate(self, new_status):
        self.is_running = False


# === ./src/behaviors/lidarchase/CloseLidarPlot.py ===
from src.behaviors.MaxineBehavior import MaxineBehavior


import py_trees
from py_trees.common import Status


class CloseLidarPlot(MaxineBehavior):
    def __init__(self):
        super().__init__("Close lidar plot")
        self.blackboard.register_key("LIDAR_PLOT", access=py_trees.common.Access.WRITE)

    def update(self):
        if not self.blackboard.exists("LIDAR_PLOT"):
            return Status.SUCCESS

        lidar_plot = self.blackboard.get("LIDAR_PLOT")
        lidar_plot.close()
        self.blackboard.unset("LIDAR_PLOT")

        return Status.SUCCESS


# === ./src/behaviors/lidarchase/FollowPathBehvior.py ===
from math import degrees, radians
import math
import time
from src.action_managers.VelocityManager import VelocityConfig
from src.path_finding.Position import Position
from src.types.MovementDirection import MovementDirection
from src.behaviors.MaxineBehavior import MaxineBehavior
from py_trees.common import Status
import py_trees


def to_radians(degrees) -> float:
    return radians(degrees) % (2 * math.pi)

class FollowPathBehavior(MaxineBehavior):
    def __init__(self):
        super().__init__("Follow path")
        self.blackboard.register_key("PATH", access=py_trees.common.Access.WRITE)

    def path_exists(self):
        path_exists  = self.blackboard.exists("PATH")
        path_not_none = self.blackboard.get("PATH") is not None

        return path_exists and path_not_none
    
    def direction_of_next_position(self, position: Position) -> MovementDirection:
        angle = degrees(position.angle)

        if angle < 0:
            angle = 360 + angle
        
        if angle >= 350 or angle <= 10:
            return MovementDirection.FORWARDS

        if 10 <= angle <= 50:
            return MovementDirection.FORWARDS_RIGHT
        
        if 310 <= angle <= 350:
            return MovementDirection.FORWARDS_LEFT
        
        if 50 <= angle <= 180:
            return MovementDirection.RIGHT
        
        if 180 <= angle <= 310:
            return MovementDirection.LEFT

        return MovementDirection.NONE
    
    def find_next_position(self, path):
        try: 
            next_position = path[1]
            i = 1
            while next_position.distance == 0:
                i+= 1
                next_position = path[i]

            return next_position
        except:
            return path[0]
    
    def update(self):
        # check path exists
        path_missing = not self.path_exists()
        if path_missing:
            return Status.FAILURE
        
        # find direction of next position in path
        path = self.blackboard.get("PATH")
        if len(path) == 0:
            return Status.SUCCESS
        next_position = self.find_next_position(path)
        next_direction = self.direction_of_next_position(next_position)

        # print(f"Next position: {next_position}, next direction: {next_direction}")
        # use velocity manager to move in the direction
        robot = self.get_robot()
        movement_manager  = robot.velocity_manager

        movement_manager.perform_action(VelocityConfig(next_direction, 0.6))
        # time.sleep(0.25)
        # movement_manager.perform_action(VelocityConfig(MovementDirection.NONE))


        return Status.SUCCESS


# === ./src/behaviors/lidarchase/__init__.py ===
from py_trees.composites import Selector, Sequence
from src.behaviors.SetRobotMode import SetRobotMode
from src.behaviors.chase_mode.AnnounceFoundPerson import AnnounceFoundPerson
from src.behaviors.lidarchase.FollowPathBehvior import FollowPathBehavior
from src.types.MovementDirection import MovementDirection
from ..MoveBehavior import MoveBehavior
from ..chase_mode.HasReachedPerson import HasReachedPerson
from src.behaviors.lidarchase.CloseLidarPlot import CloseLidarPlot
from src.behaviors.chase_mode.SelectPerson import SelectPerson
from src.behaviors.lidarchase.PathFind import PathFind
from src.behaviors.lidarchase.UpdateLidarPlot import UpdateLidarPlot
from src.behaviors.utils import make_press_esc_to_exit_behavior, make_run_every_n_ticks
from src.types.RobotModes import RobotMode
from ...behaviors.ShowCameraFrame import ShowCameraFrame



def make_target_reached_sub_tree():
    """
    Makes the sub tree for the target being reached
    """

    target_is_close = HasReachedPerson(600)

    # actions to perform if conditions are met
    stop_moving = MoveBehavior(MovementDirection.NONE)
    announce_found_person = AnnounceFoundPerson()
    exit_to_idle = SetRobotMode(RobotMode.IDLE)
    close_lidar_plot = CloseLidarPlot()
    

    # target reached sub tree is sequence (performs all the following actions unless one of them fails):
    # - checks target is infront of robot
    # - checks target is close enough
    # - stops moving
    # - announces its found a person
    # - exits to idle mode
    return Sequence(
        "Target Reached sub tree",
        memory=True,
        children=[
            target_is_close,
            stop_moving,
            announce_found_person,
            exit_to_idle,
            close_lidar_plot
        ],
    )

def make_path_find_sub_tree():
    show_frame = ShowCameraFrame()
    initial_lidar_update = UpdateLidarPlot()
       
    return Sequence(
        f"path find",
        memory=True,
        children=[
            show_frame,             # always show camera
            initial_lidar_update,   # show lidar plot before detection of person
            SelectPerson(),         # try to detect person
            UpdateLidarPlot(),      # update lidar after selecting target
            PathFind(),             # pathfind shortest route to target (a-star)
            UpdateLidarPlot(),      # update after pathfining
        ],
    )

def make_chase_subtree():

    path_find_subtree = make_path_find_sub_tree()
    move_towards_target = FollowPathBehavior()
    stop_moving = MoveBehavior(MovementDirection.NONE)
    move_sequence =  Sequence(
        "Chase subtree",
        children=[path_find_subtree, move_towards_target],
        memory=True
    )

    return  Selector(
        "move or stop moving",
        memory=True,
        children=[move_sequence, stop_moving],
    )



def make_lidar_chase_sub_tree():
    # exit to idle behavior
    exit_mode_behavior = make_press_esc_to_exit_behavior(RobotMode.IDLE)
    close_lidar_plot = CloseLidarPlot()
    exit_subtree = Sequence(
        "Exit from lidar chase",
        memory=True,
        children=[exit_mode_behavior, close_lidar_plot],
    )

    # chase subtree is either announce that you've reached target or chase the target
    target_reached_sub_tree = make_target_reached_sub_tree()
    move_towards_target_sub_tree= make_chase_subtree()
    chase_target_subtree = Selector(   
        "Chase sub tree actions",
        memory=True,
        children=[target_reached_sub_tree, move_towards_target_sub_tree],
    ) 

    return Selector(
        "lidar chase mode behavior",
        memory=True,
        children=[exit_subtree, chase_target_subtree],
    )


# === ./src/behaviors/lidarchase/UpdateLidarPlot.py ===
# import cv2
from math import radians
import matplotlib

from src.path_finding.LidarPlot import LidarPlot
from src.path_finding.Position import Position

matplotlib.use("TkAgg")

from matplotlib import pyplot as plt
from src.behaviors.MaxineBehavior import MaxineBehavior
from py_trees.common import Status
import depthai as dai
import py_trees


class UpdateLidarPlot(MaxineBehavior):
    def __init__(self):
        """
        Initilialises the behavior
        """
        super().__init__("show lidar plot")

        self.blackboard.register_key(
            "TARGET_PERSON", access=py_trees.common.Access.WRITE
        )
        self.blackboard.register_key("PATH", access=py_trees.common.Access.WRITE)
        self.blackboard.register_key("LIDAR_PLOT", access=py_trees.common.Access.WRITE)

    def get_target_x_center(self) -> float:
        """
        Returns the middle of the current targets x axis coordinate
        """
        target_person: dai.SpatialImgDetection = self.blackboard.get("TARGET_PERSON")
        x_diff = target_person.xmax - target_person.xmin
        return target_person.xmin + (x_diff / 2)

    def get_or_create_lidar_plot(self):
        # if plot already exists, return from blackboard
        if self.blackboard.exists("LIDAR_PLOT"):
            return self.blackboard.get("LIDAR_PLOT")

        # create plot
        lidar_plot = LidarPlot(range_mm=7000)

        self.blackboard.set("LIDAR_PLOT", lidar_plot)
        return lidar_plot

    def update(self) -> Status:
        lidar_plot = self.get_or_create_lidar_plot()

        # plot obstacles
        robot = self.get_robot()
        lidar = robot.lidar_sensor
        readings = lidar.get_reading()
        lidar_plot.plot_obstacles(readings)

        # plot the target person (as a red star)
        try:
            target_person: dai.SpatialImgDetection = self.blackboard.get(
                "TARGET_PERSON"
            )
            distance = target_person.spatialCoordinates.z - 650
            x_center = self.get_target_x_center()

            angle = radians((x_center * 2 - 1) * 63.5)

            destination = Position(angle=angle, distance=distance)
            lidar_plot.plot_destination(destination)
        except KeyError:
            print("Target person missing, cant plot red star")

        # plot path
        try:
            path = self.blackboard.get("PATH")
            lidar_plot.plot_path(path)
        except KeyError:
            print("Path missing, cant plot red line")

        # update plot
        lidar_plot.update_plot()
        return Status.SUCCESS


# === ./src/behaviors/lidarchase/run_search.py ===
from src.path_finding.new_a_star import a_star


import multiprocessing.connection
import time


def run_search(
    receive_connection: multiprocessing.connection.Connection,
    send_connection: multiprocessing.connection.Connection,
) -> None:
    try:
        while True:
            if receive_connection.poll():
                (goal, obstacles) = receive_connection.recv()
                try:
                    # print("Starting A Search")
                    path, *_ = a_star(obstacles, goal)
                    # print("Search complete")
                    send_connection.send(path)
                except:
                    send_connection.send(None)
                    print("error")

            time.sleep(0.01)
    except KeyboardInterrupt:
        pass


# === ./src/behaviors/SetRobotMode.py ===
import cv2
from py_trees.common import Status

from ..types.CameraMode import CameraMode
from .MaxineBehavior import MaxineBehavior
from ..types.RobotModes import RobotMode


class SetRobotMode(MaxineBehavior):
    """
    Behavior that sets the robot in a particular mode
    Used to switch modes
    """

    # stores the camera mode for each robot mode
    ROBOT_MODE_TO_CAMERA_MODE = {
        RobotMode.IDLE: CameraMode.DISABLED,
        RobotMode.EXIT: CameraMode.DISABLED,
        RobotMode.KEYBOARD_CONTROL: CameraMode.DISABLED,
        RobotMode.CHASE: CameraMode.OBJECT_DETECTION,
        RobotMode.LIDARCHASE: CameraMode.OBJECT_DETECTION,
        RobotMode.DISCOVERY: CameraMode.OBJECT_DETECTION,
        RobotMode.HEADTURN: CameraMode.DISABLED,
        RobotMode.DIAGNOSTIC: CameraMode.DISABLED,
        RobotMode.PLAYGAME: CameraMode.DISABLED,
    }

    MODE_SAYING = {
        RobotMode.IDLE: "idle",
        RobotMode.KEYBOARD_CONTROL: "keyboard control",
        RobotMode.CHASE: "chase",
        RobotMode.EXIT: "Exit",
        RobotMode.LIDARCHASE: "Lidar",
        RobotMode.DISCOVERY: "Discovery",
        RobotMode.HEADTURN: "Head Turn",
        RobotMode.DIAGNOSTIC: "Diagnostic",
        RobotMode.PLAYGAME: "Play Game",
    }

    def __init__(self, mode: RobotMode):
        """
        Initialises the Behavior

        arguments:
            - mode: the mode to set the robot in
        """
        super().__init__(f"Set to {mode.name} mode")
        self.mode = mode

    def update_camera(self):
        """
        Handles updating the camera sensor realted settings upon changing robot mode
        """
        robot = self.get_robot()

        # switch the camera mode on camera sensor
        camera_mode = self.ROBOT_MODE_TO_CAMERA_MODE[self.mode]
        robot.camera_sensor.switch_mode(camera_mode)

        # close the camera window
        try:
            cv2.destroyWindow("Camera")
        except:
            # window does not exist -> can ignore
            pass

        # open camera window if required
        if camera_mode != CameraMode.DISABLED:
            # cv2.namedWindow("Camera", cv2.WINDOW_GUI_NORMAL)
            pass
   

    def update(self) -> Status:
        # set the robot mode
        robot = self.get_robot()
        robot.set_mode(self.mode)
        robot.speech_manager.perform_action(
            "Entering " + self.MODE_SAYING[self.mode] + " mode"
        )

        print(f"Setting to {self.mode}")

        # clear keyboard sensor currently pressed keys when swithing modes
        robot.keyboard_sensor.flush_readings()

        # update the camera settings
        self.update_camera()

        # this behavior always passes
        return Status.SUCCESS


# === ./src/behaviors/utils.py ===
import py_trees
from py_trees.behaviour import Behaviour
from py_trees.composites import Sequence

from ..behaviors.CountBehavior import CountBehavior

from ..behaviors.conditions.RobotInModeCondition import RobotInModeCondition
from ..behaviors.SetRobotMode import SetRobotMode
from ..behaviors.conditions.KeyPressedCondition import KeyPressedCondition
from ..types.KeyboardKey import KeyboardKey
from ..types.RobotModes import RobotMode
from .conditions.Condition import Condition


def conditional_behavior(behavior: Behaviour, condition: Condition):
    """
    Builds a conditional behavior branch.
    This is a Sequence of Condition, Behavior that will run the behavior if a condition is met
    Or return FAILURE if the condition is not met
    """
    name = f"{behavior.name} if {condition.name}"
    return Sequence(name, memory=True, children=[condition, behavior])


def make_run_every_n_ticks(n: int, *behaviors: Behaviour) -> Sequence:
    count_behavior = CountBehavior(n)

    return Sequence(
        f"Run {behaviors[0].name} every {n} ticks",
        memory=True,
        children=[count_behavior, *behaviors],
    )


def make_press_esc_to_exit_behavior(target_mode: RobotMode):
    """
    A behavior that sets the robot mode if ESC is pressed.
    Used throughout all modes.

    arguments:
        - target_mode: the mode to switch to when ESC is pressed
    """
    # set robot mode if esc is pressed
    return conditional_behavior(
        SetRobotMode(target_mode), KeyPressedCondition(KeyboardKey.ESC)
    )


def make_mode_sub_tree(mode: RobotMode, mode_sub_tree: py_trees.behaviour.Behaviour):
    """
    Builds a sub tree for a particular mode.
    Adds a check to see if robot is in that particular mode.
    If it is in that mode, then follow that sub tree.

    arguments:
        - mode: the mode to be in
        - mode_sub_tree: the sub tree of the mode
    """
    # follow sub tree if robot is in this mode
    return conditional_behavior(mode_sub_tree, RobotInModeCondition(mode))


# === ./src/behaviors/keyboard_mode/__init__.py ===
import py_trees

from src.behaviors.HeadTurnBehavior import HeadTurn
from src.types.HeadMovementDirection import HeadMovementDirection
from ...behaviors.MoveBehavior import DecreaseSpeedBehavior, IncreaseSpeedBehavior, MoveBehavior
from ...behaviors.conditions.KeyPressedCondition import KeyPressedCondition
from ...types.KeyboardKey import KeyboardKey
from ...types.MovementDirection import MovementDirection
from ...behaviors.utils import (
    conditional_behavior,
    make_press_esc_to_exit_behavior,
)
from ...types.RobotModes import RobotMode


def make_stop_moving_behavior():
    """
    builds the sub tree for stop moving condition
    """
    movement_keys = [
        KeyboardKey.Q,
        KeyboardKey.W,
        KeyboardKey.E,
        KeyboardKey.A,
        KeyboardKey.S,
        KeyboardKey.D,
        KeyboardKey.Z,
        KeyboardKey.X,
    ]

    # conditions that each key is NOT being pressed
    keys_not_pressed = [
        py_trees.decorators.Inverter(
            f"{key.name} not pressed", child=KeyPressedCondition(key)
        )
        for key in movement_keys
    ]

    stop_moving_behavior = MoveBehavior(MovementDirection.NONE)

    # stop moving if each key is not being pressed
    return py_trees.composites.Sequence(
        "No Movement Keys Pressed",
        memory=True,
        children=[*keys_not_pressed, stop_moving_behavior],
    )


def make_move_in_direction_behaviors():
    """
    Builds the behavior to move in each of the directions.
    Returns a list of behaviors - 1 for each direction
    """
    # mapping between keys and directions
    key_to_direction = {
        KeyboardKey.Q: MovementDirection.FORWARDS_LEFT,
        KeyboardKey.W: MovementDirection.FORWARDS,
        KeyboardKey.E: MovementDirection.FORWARDS_RIGHT,
        KeyboardKey.A: MovementDirection.LEFT,
        KeyboardKey.S: MovementDirection.BACKWARDS,
        KeyboardKey.D: MovementDirection.RIGHT,
        KeyboardKey.Z: MovementDirection.BACKWARDS_LEFT,
        KeyboardKey.X: MovementDirection.BACKWARDS_RIGHT,

    }

    # make 1 behavior per direction
    move_in_direction_behaviors = []
    for key, direction in key_to_direction.items():
        # behavior = move if key is being pressed
        move_behavior = conditional_behavior(
            MoveBehavior(direction), KeyPressedCondition(key)
        )
        move_in_direction_behaviors.append(move_behavior)

    return move_in_direction_behaviors

def make_head_move_behaviours():
    """
    builds behaviour to turn head left right
    """
    
    move_left= conditional_behavior(HeadTurn(HeadMovementDirection.LEFT), KeyPressedCondition(KeyboardKey.O))
    move_right= conditional_behavior(HeadTurn(HeadMovementDirection.RIGHT), KeyPressedCondition(KeyboardKey.P))

    return move_left, move_right



def make_speed_change_behaviors():
    increase = IncreaseSpeedBehavior()
    decrease = DecreaseSpeedBehavior()

    return [conditional_behavior(
           increase, KeyPressedCondition(KeyboardKey.Plus)
        ),conditional_behavior(
            decrease, KeyPressedCondition(KeyboardKey.Minus)
        )]

def make_keyboard_mode_sub_tree():
    """
    Returns the keyboard mode subtree
    """
    # exit to idle behavior
    exit_mode_behavior = make_press_esc_to_exit_behavior(RobotMode.IDLE)

    # behavior for each of the directions
    move_in_direction_behaviors = make_move_in_direction_behaviors()

    # also add a behavior for not moving
    stop_moving_behavior = make_stop_moving_behavior()

    change_speed_bahviors = make_speed_change_behaviors()

    move_behaviours= make_head_move_behaviours()

    # sub tree is a selector (will perform the first behavior that passes)
    # ie will either
    # - exit to idle mode if esc pressed
    # - move in the first direction that is matching a key being pressed
    # - or stop moving if no key is being pressed
    return py_trees.composites.Selector(
        "keyboard mode behavior",
        memory=True,
        children=[
            exit_mode_behavior,
            *change_speed_bahviors,
            *move_in_direction_behaviors,
            *move_behaviours,
            stop_moving_behavior,
        ]
    )


# === ./src/behaviors/chase_mode/HasReachedPerson.py ===
from ...behaviors.conditions.Condition import Condition


import py_trees


class HasReachedPerson(Condition):
    """
    Condition that determines if the robot is in range of a person
    """

    def __init__(self, threshold: float):
        """
        Initialises the condition

        arguments:
            - threshold: the threshold distance to concider the target reached
        """
        super().__init__("Has Reached Person")
        self.blackboard.register_key(
            "TARGET_PERSON", access=py_trees.common.Access.WRITE
        )
        self.threshold = threshold

    def condition_met(self) -> bool:
        if not self.blackboard.exists("TARGET_PERSON"):
            return False
        
        target_object = self.blackboard.get("TARGET_PERSON")

        # when target is first aquired, all values will be 0 for a few ticks, need to ignor this
        if target_object.spatialCoordinates.z== 0 and target_object.spatialCoordinates.x== 0 and target_object.spatialCoordinates.y ==  0:
            return  False

        # reached target if is within threshold
        return target_object.spatialCoordinates.z <= self.threshold


# === ./src/behaviors/chase_mode/SelectPerson.py ===
import py_trees
from py_trees.common import Status
from ...behaviors.MaxineBehavior import MaxineBehavior
from ...depth_ai.ObjectDetectionReading import ObjectDetectionReading
from ...types.MovementDirection import MovementDirection


class SelectPerson(MaxineBehavior):
    def __init__(self):
        super().__init__("Select A Person")
        self.blackboard.register_key(
            "TARGET_PERSON", access=py_trees.common.Access.WRITE
        )

    def update(self) -> Status:
        camera = self.get_robot().camera_sensor
        reading: ObjectDetectionReading = camera.get_reading()
        people = reading.get_people_locations()

        if len(people) == 0:
            return Status.FAILURE

        people = sorted(people, key=lambda person: person.spatialCoordinates.z)
        closest_person = people[0]
        self.blackboard.set("TARGET_PERSON", closest_person)

        return Status.SUCCESS


# === ./src/behaviors/chase_mode/__init__.py ===
from typing import Tuple
import py_trees
from py_trees.composites import Sequence, Selector
from ...behaviors.MoveBehavior import MoveBehavior
from ...behaviors.SetRobotMode import SetRobotMode
from ...behaviors.chase_mode.AnnounceFoundPerson import AnnounceFoundPerson
from ...behaviors.chase_mode.AnnounceObstacleDetection import (AnnounceObstacleDetection,)
from ...behaviors.chase_mode.HasReachedPerson import HasReachedPerson
from ...behaviors.chase_mode.ObjectInTheWay import ObjectInTheWay
from .TargetOffCenterCondition import (TargetOffCenterCondition,)
from ...behaviors.chase_mode.SelectPerson import SelectPerson
from ...types.DirectionSensorLocation import DirectionSensorLocation
from ...types.MovementDirection import MovementDirection
from ...behaviors.ShowCameraFrame import ShowCameraFrame
from ...behaviors.utils import make_press_esc_to_exit_behavior
from ...types.RobotModes import RobotMode


# def make_avoid_obsticle_sub_tree():
#     def make_avoid(sensor_dir, turn_dir, revert_turn_dir):
#         condition = ObjectInTheWay(sensor_dir, 0.1)
#         stop_moving = MoveBehavior(MovementDirection.NONE)
#         announcement = AnnounceObstacleDetection(sensor_dir)
#         turn = MoveBehavior(turn_dir)
#         move_forward = MoveBehavior(MovementDirection.FORWARDS)
#         revert_turn = MoveBehavior(revert_turn_dir)

#         return Sequence(
#             f"Avoid in {sensor_dir.value} direction",
#             memory=True,
#             children=[
#                 condition,
#                 stop_moving,
#                 announcement,
#                 repeated_turn,
#                 repeated_move,
#                 repeated_revert_turn,
#             ],
#         )

#     sensor_to_turns = {
#         DirectionSensorLocation.LEFT: (MovementDirection.RIGHT, MovementDirection.LEFT),
#         DirectionSensorLocation.RIGHT: (
#             MovementDirection.LEFT,
#             MovementDirection.RIGHT,
#         ),
#     }
#     return Selector(
#         "Avoid sub tree",
#         memory=True,
#         children=[
#             make_avoid(sensor_dir, turn_dir, revert_dir)
#             for sensor_dir, (turn_dir, revert_dir) in sensor_to_turns.items()
#         ],
#     )


def make_target_reached_sub_tree():
    """
    Makes the sub tree for the target being reached
    """

    # conditions to enter this sub tree: target needs to be in the center and close to robot
    target_is_infront = TargetOffCenterCondition(0.4, 0.6)
    target_is_close = HasReachedPerson(600)

    # actions to perform if conditions are met
    stop_moving = MoveBehavior(MovementDirection.NONE)
    announce_found_person = AnnounceFoundPerson()
    exit_to_idle = SetRobotMode(RobotMode.IDLE)

    # target reached sub tree is sequence (performs all the following actions unless one of them fails):
    # - checks target is infront of robot
    # - checks target is close enough
    # - stops moving
    # - announces its found a person
    # - exits to idle mode
    return Sequence(
        "Target Reached sub tree",
        memory=True,
        children=[
            target_is_infront,
            target_is_close,
            stop_moving,
            announce_found_person,
            exit_to_idle,
        ],
    )


def make_move_towards_person_sub_tree():
    """
    Builds the sub tree to move towards a taget
    """

    # makes the move towards target in a direction for an x axis threshold
    def make_behavior(thresholds, direction):
        correct_direction = TargetOffCenterCondition(*thresholds)
        move = MoveBehavior(direction)


        # will move repeatedly in direction if the target is within the x axis threshold
        return Sequence(
            f"Move {direction.value} if target there",
            memory=True,
            children=[correct_direction, move],
        )

    # threshold directions in x axis and corresponding directions to move in
    threshold_directions = {
        MovementDirection.LEFT: (0.0, 0.25),
        MovementDirection.FORWARDS_LEFT: (0.25, 0.4),
        MovementDirection.FORWARDS: (0.4, 0.6),
        MovementDirection.FORWARDS_RIGHT: (0.6, 0.75),
        MovementDirection.RIGHT: (0.75, 1.0),
    }

    # make a move behaviour for each direction
    movement_behaviors = [
        make_behavior(threshold, direction)
        for direction, threshold in threshold_directions.items()
    ]

    # move towards target sub tree is a selector (will run the first condition that passes)
    # Will check if target is within each threshold and move in that direction if it is
    return Selector("Move towards target", memory=True, children=movement_behaviors)


def make_chase_sub_tree():
    """
    Builds the chase target sub tree
    """

    # build sub trees for the chase mode
    target_reached = make_target_reached_sub_tree()
    # avoid_obsticle = make_avoid_obsticle_sub_tree()
    move_towards_target = make_move_towards_person_sub_tree()

    # chase sub tree will run either:
    # - target reached
    # - avoid an obstacle
    # - move towards the target
    chase_action = Selector(
        "Chase sub tree actions",
        memory=True,
        children=[target_reached, move_towards_target],
    )

    # build behaviors that run every time
    select_target = SelectPerson()
    show_frame = ShowCameraFrame()

    # chase sub tree will:
    # - show the current frame
    # - select a target
    # - then run on of the selector actions
    return Sequence(
        "Chase sub tree",
        memory=True,
        children=[show_frame, select_target, chase_action],
    )


def make_chase_mode_sub_tree():
    """
    Returns the chase mode subtree
    """
    # exit to idle behavior
    exit_mode_behavior = make_press_esc_to_exit_behavior(RobotMode.IDLE)

    # the chase sub tree
    chase_sub_tree = make_chase_sub_tree()

    # chase mode behaviors is a Selector either:
    # - go to idle if esc pressed
    # - run the chase sub tree
    return py_trees.composites.Selector(
        "chase mode behavior",
        memory=True,
        children=[exit_mode_behavior, chase_sub_tree],
    )


# === ./src/behaviors/chase_mode/AnnounceObstacleDetection.py ===
from ...behaviors.MaxineBehavior import MaxineBehavior
from ...types.DirectionSensorLocation import DirectionSensorLocation
from ...types.FacialAnimation import FacialAnimation
from ...types.Sound import Sound


from py_trees.common import Status


class AnnounceObstacleDetection(MaxineBehavior):
    """
    Accounces and obstacle has been detected
    """

    def __init__(self, direction: DirectionSensorLocation):
        super().__init__(f"Announce obstacle in {direction.value} direction")
        self.direction = direction

    def update(self) -> Status:
        robot = self.get_robot()

        # play sound
        sound_manager = robot.sound_manager
        sound_manager.perform_action(Sound.inmyway)

        # play animation
        animation_manager = robot.facial_animation_manager
        animation_manager.perform_action(FacialAnimation.inmyway)

        # always passes
        return Status.SUCCESS


# === ./src/behaviors/chase_mode/ObjectInTheWay.py ===
from ...behaviors.conditions.Condition import Condition
from ...types.DirectionSensorLocation import DirectionSensorLocation


class ObjectInTheWay(Condition):
    """
    Determines if an object is in the way in a given direction.
    Uses distance sensor to check this
    """

    def __init__(self, direction: DirectionSensorLocation, threshold: float):
        """
        Initialises the condition

        arguments:
            - direction: the sensor direction to check for
            - threshold: the threashold to concider an object being in the way
        """
        super().__init__(f"Is object within {threshold} of {direction.value}")
        self.direction = direction
        self.threshold = threshold

    def condition_met(self) -> bool:
        distance_sensor = self.get_robot().distance_sensor

        # check sensor in the direction is below threshold
        distances = distance_sensor.get_reading()
        distance = distances[self.direction]

        return distance >= self.threshold


# === ./src/behaviors/chase_mode/AnnounceFoundPerson.py ===
from ...behaviors.MaxineBehavior import MaxineBehavior
from ...types.FacialAnimation import FacialAnimation
from ...types.Sound import Sound


from py_trees.common import Status


class AnnounceFoundPerson(MaxineBehavior):
    """
    Announce that the robot has found a person
    """

    def __init__(self):
        super().__init__(f"Announce found person")

    def update(self) -> Status:
        robot = self.get_robot()

        # play sound
        sound_manager = robot.sound_manager
        sound_manager.perform_action(Sound.found_person)

        # facial animation
        animation_manager = robot.facial_animation_manager
        animation_manager.perform_action(FacialAnimation.found_person)

        # always sucessed
        return Status.SUCCESS


# === ./src/behaviors/chase_mode/TargetOffCenterCondition.py ===
import py_trees
from ..conditions.Condition import Condition


import depthai as dai


class TargetOffCenterCondition(Condition):
    """
    Condition checking if the currently tracked target is within a threshold in the x axis
    """

    def __init__(self, min_threshold: float, max_threshold: float):
        """
        Initialises the condition

        argument:
            - min_threshold: the minimum value in the x axis for the condition to pass
            - max_threshold: the maximum value in the x axis for the condition to pass
        """
        super().__init__(f"{min_threshold} <= Target person offset <= {max_threshold}")
        self.min_threshold = min_threshold
        self.max_threshold = max_threshold

        # register use of the TARGET_PERSON on the blackboard
        self.blackboard.register_key(
            "TARGET_PERSON", access=py_trees.common.Access.WRITE
        )

    def get_target_x_center(self) -> float:
        """
        Returns the middle of the current targets x axis coordinate
        """
        target_person: dai.SpatialImgDetection = self.blackboard.get("TARGET_PERSON")
        x_diff = target_person.xmax - target_person.xmin
        return target_person.xmin + (x_diff / 2)

    def condition_met(self) -> bool:
        target_offset = self.get_target_x_center()

        # if target is below threshold -> condition not met
        if target_offset < self.min_threshold:
            return False

        # if target is above threshold -> condition not met
        if target_offset > self.max_threshold:
            return False

        # otherwise the target is within the threshold
        return True


# === ./src/behaviors/WaitNTicksBehavior.py ===

import py_trees
from py_trees.common import Status
from src.behaviors.MaxineBehavior import MaxineBehavior
from py_trees.behaviour import Behaviour
from uuid import uuid4

class WaitNTicksBehavior(MaxineBehavior):
    def __init__(self, n: int):
        super().__init__(f"Wait {n} ticks")
        self.n = n
        self.id = uuid4()

        # register use of the TARGET_PERSON on the blackboard
        self.blackboard.register_key(
            f"N-{self.id}", access=py_trees.common.Access.WRITE
        )
        self.blackboard.set(f"N-{self.id}",0)

    def update(self) -> Status:
        current_n = self.blackboard.get(f"N-{self.id}")
        
        if current_n < self.n:
            self.blackboard.set(f"N-{self.id}",current_n + 1)
            return Status.FAILURE
        
        self.blackboard.set(f"N-{self.id}",0)
        return Status.SUCCESS
        



# === ./src/behaviors/game_mode/play_game.py ===
import pygame
import os, sys
from pygame.locals import *
from os import path
sys.path.append("../")
from parallax import parallax
from multiprocessing import Process
import random
from pygame import *
import time


def play_game():
    WIDTH = 800     
    HEIGHT = 600
    screen = pygame.display.set_mode((WIDTH, HEIGHT), pygame.DOUBLEBUF)

    # screen = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
    # Initializing
    pygame.mixer.pre_init(48000, -16, 8, 8192)# initialise the music and sound mixer
    # pygame.init() # initialise pygame
    pygame.mouse.set_visible(0)

    entity=None

    # Create Graphics Window of Width and Height call it screen
                                      
    music_volume=0.5
    pygame.mixer.music.set_volume(music_volume)
    clock = pygame.time.Clock()
    explosion_finished=False
    shield_kill=False

    POWERUP_TIME =6000
    Boss_died_anim_finished=False
    boss_explode_inprogress=False
    Boss_level_triggered=False
    explosion_time=0

    orientation = 'vertical'
    speed=4

    bg = parallax.ParallaxSurface((1024, 768), pygame.RLEACCEL)
    bg.add('/home/jetson/maxine/MiniMax/src/behaviors/game_mode/bkgd_0.png', 6)
    bg.add('/home/jetson/maxine/MiniMax/src/behaviors/game_mode/bkgd_1.png', 5)
    bg.add('/home/jetson/maxine/MiniMax/src/behaviors/game_mode/bkgd_2.png', 4)
    bg.add('/home/jetson/maxine/MiniMax/src/behaviors/game_mode/bkgd_4.png', 3)
    bg.add('/home/jetson/maxine/MiniMax/src/behaviors/game_mode/bkgd_6.png', 2)
    bg.add('/home/jetson/maxine/MiniMax/src/behaviors/game_mode/bkgd_7.png', 1)

    # Creating colors
    BLUE  = (0, 0, 255)
    RED   = (255, 0, 0)
    GREEN = (0, 255, 0)
    BLACK = (0, 0, 0)
    WHITE = (255, 255, 255)

    # Initialise variables for zoom_player function
    x=0
    y=0
    zoom_speed=8
    t_ref = 0
    col1 =(255,244,11)
    col2 =(0,255,3)
    col3 =(255,153,33)
    col4 =(58,0,239)
    col5 =(175,0,231)
    cols = [col1,col2,col3,col4,col5]

    # Define sound objects
    bullet_sound = pygame.mixer.Sound("Sound/Laser.wav")
    crash_sound = pygame.mixer.Sound("Sound/Explosion.wav")  
    boss_hit_sound = pygame.mixer.Sound("Sound/Boss_hit_sound.wav")
    enemy_hit = pygame.mixer.Sound("Sound/Hit_Enemy.wav")
    player_hit = pygame.mixer.Sound("Sound/expl6.wav")
    boss_bullet_sound = pygame.mixer.Sound("Sound/Boss_bullet.wav")
    game_over_message = pygame.mixer.Sound("Sound/Game_over.wav")
    powerup_shields = pygame.mixer.Sound("Sound/Powerup_shields.wav")
    bolt_powerup = pygame.mixer.Sound("Sound/Bolt_powerup.wav")
    boss_explode_sound = pygame.mixer.Sound("Sound/BIG_explosion.wav")
    chris_getready = pygame.mixer.Sound("Sound/Chris_getready.wav")
    owwyeah = pygame.mixer.Sound("Sound/Oh_yeah.wav")
    evil_laugh = pygame.mixer.Sound("Sound/Evil_laugh.wav")

    # Define graphic objects
    player_ship = pygame.image.load("Player/Ship2.png")
    enemy_ships = pygame.image.load("Enemy/Zorg.png")
    boss_monster = pygame.image.load("Boss/Boss1.png")
    player_shield = pygame.image.load("Player/shield.png")
    sheet = pygame.image.load('Explosion/Explosion.png')
    Background = pygame.image.load('/home/jetson/maxine/MiniMax/src/behaviors/game_mode/bkgd_0.png')
    
    img_dir=("Powerups")
    powerup_images = {}
    powerup_images['shield'] = pygame.image.load('Powerups/shield_powerup.png')
    powerup_images['power'] = pygame.image.load('Powerups/bolt.png')
    player_mini_img = pygame.transform.scale(player_ship, (35, 29))


    # -----------------------------------------------------------------------------------------------------------------------------
    # --------------------------------- Load Animated sprites into lists ----------------------------------------------------------
    # -----------------------------------------------------------------------------------------------------------------------------

    # loading the explosion images into lists using filename of images with 0-8 on the end

    img_dir = ("Explosion") 
    explosion_anim = {}
    explosion_anim['lg'] = []
    explosion_anim['sm'] = []
    explosion_anim['player'] = []
    for i in range(9):
        filename = 'regularExplosion0{}.png'.format(i)
        img = pygame.image.load(path.join(img_dir, filename)).convert()
        img.set_colorkey(BLACK)
        img_lg = pygame.transform.scale(img, (75, 75))
        explosion_anim['lg'].append(img_lg)
        img_sm = pygame.transform.scale(img, (32, 32))
        explosion_anim['sm'].append(img_sm)
        filename = 'sonicExplosion0{}.png'.format(i)
        img = pygame.image.load(path.join(img_dir, filename)).convert()
        img.set_colorkey(BLACK)
        explosion_anim['player'].append(img)

    # loading the BOSS images into lists using filename of images with 0-7 on the end

    boss_dir = ("Boss")  
    eye_anim = {}
    eye_anim['boss'] = []
    for i in range(8):
        filename = 'Boss{}.png'.format(i)
        img = pygame.image.load(path.join(boss_dir, filename))
        eye_anim['boss'].append(img)

    # loading BIG BOSS EXPLOSION into lists

    size = 192,192
    pos = 0,0
    len_sprt_x,len_sprt_y = size                                         #sprite size
    sprt_rect_x,sprt_rect_y = pos                                        #where to find first sprite on sheet
    sheet = pygame.image.load('Explosion/Explosion.png').convert_alpha() #Load the sheet
    sheet_rect = sheet.get_rect()
    msprites = []   

    for i in range(0,22):#columns
        sheet.set_clip(pygame.Rect(sprt_rect_x, sprt_rect_y, len_sprt_x, len_sprt_y)) #find sprite you want
        sprite = sheet.subsurface(sheet.get_clip())                                   #grab the sprite you want
        msprites.append(sprite)
        sprt_rect_x += len_sprt_x
    bexplosion = msprites

    #Setting up Frames Per Second 
    FPS = 60
    FramePerSec = pygame.time.Clock()
    
    # Other Variables for use in the program
    score = 0
    enemy_speedx = 1.2
    enemy_speedy = 1
    boss_speedx = 1
    boss_speedy = 1
    direction = 1
    scroll_speed = 2

    # Bullet settings
    bullet_speed = -20
    boss_bullet_speed = 12
    bullet_width = 4
    bullet_height = 16
    bullet_color = (255, 255, 255)
    bullets_allowed = 3

    #Setting up Fonts
    font = pygame.font.SysFont("Verdana", 60)
    font_small = pygame.font.SysFont("Verdana", 20)
    game_over_text = font.render("Game Over", True, BLACK)

    # Set screen caption
    pygame.display.set_caption("Chris's Crazy Space Game")

    # Setting up the background for vertical scrolling space effect so it looks like your moving through space! TRICKY TRICKY!
    # bg = pygame.image.load(os.path.join("Background",'bg2.png')).convert()
    # bg_rect = bg.get_rect()
    # bgY = 0 # bgY cordinate is at the top of screen
    # bgY2 = bg.get_height() * -1 # the bgY2 coordinate is a whole screen above the visable area

    # -----------------------------------------------------------------------------------------------------------------------------
    # ------------------------------------ Class Definitions for all the major sprites --------------------------------------------
    # -----------------------------------------------------------------------------------------------------------------------------

    class Powerups(pygame.sprite.Sprite):
        def __init__(self, center):
            pygame.sprite.Sprite.__init__(self)
            self.type = random.choice(['shield', 'power'])
            self.image = powerup_images[self.type]
            self.rect = self.image.get_rect()
            self.rect.center = center
            self.speedy = 5

        def move(self):
            self.rect.y += self.speedy
            # kill if it moves off the bottom of the screen
            if self.rect.top > HEIGHT:
                self.kill()    
        
    # Class function to draw 1st BOSS bullet and move them
    class Boss_Bullet1(pygame.sprite.Sprite):
        def __init__(self, x, y):
            pygame.sprite.Sprite.__init__(self)
            self.image = pygame.Surface((bullet_width, bullet_height))
            self.image.fill(bullet_color)
            self.rect = self.image.get_rect()
            self.rect.bottom = y
            self.rect.centerx = x 
            self.speedy = boss_bullet_speed # boss bullet speed is positive int so bullet moves DOWN the screen

        def move(self):
            self.rect.y += self.speedy
            # kill if it moves off the BOTTOM of the screen
            if self.rect.bottom > HEIGHT:
                self.kill()
                
    # Class function to draw 2nd BOSS bullet and move them
    class Boss_Bullet2(pygame.sprite.Sprite):
        def __init__(self, x, y):
            pygame.sprite.Sprite.__init__(self)
            self.image = pygame.Surface((bullet_width, bullet_height))
            self.image.fill(bullet_color)
            self.rect = self.image.get_rect()
            self.rect.bottom = y + 64   # move start position of second boss bullet to the LEFT WING of the BOSS
            self.rect.centerx = x -116  # move start position of second boss bullet to the LEFT WING of the BOSS
            self.speedy = boss_bullet_speed # boss bullet speed is positive int so bullet moves DOWN the screen

        def move(self):
            self.rect.y += self.speedy
            # kill if it moves off the BOTTOM of the screen
            if self.rect.bottom > HEIGHT:
                self.kill()            
                
    # Class function to draw 3rd BOSS bullet and move them
    class Boss_Bullet3(pygame.sprite.Sprite):
        def __init__(self, x, y):
            pygame.sprite.Sprite.__init__(self)
            self.image = pygame.Surface((bullet_width, bullet_height))
            self.image.fill(bullet_color)
            self.rect = self.image.get_rect()
            self.rect.bottom = y + 64   # move start position of third boss bullet to the RIGHT WING of the BOSS
            self.rect.centerx = x +116  # move start position of third boss bullet to the RIGHT WING of the BOSS
            self.speedy = boss_bullet_speed # boss bullet speed is positive int so bullet moves DOWN the screen

        def move(self):
            self.rect.y += self.speedy
            # kill if it moves off the BOTTOM of the screen
            if self.rect.bottom > HEIGHT:
                self.kill()                        
    
    # Class function to draw bullets and move them
    class Bullet(pygame.sprite.Sprite):
        def __init__(self, x, y):
            pygame.sprite.Sprite.__init__(self)
            self.image = pygame.Surface((bullet_width, bullet_height))
            self.image.fill(bullet_color)
            self.rect = self.image.get_rect()
            self.rect.bottom = y
            self.rect.centerx = x 
            self.speedy = bullet_speed  # bullet speed is negative int so bullet moves UP the screen

        def move(self):
            self.rect.y += self.speedy
            # kill if it moves off the TOP of the screen
            if self.rect.bottom < 0:
                self.kill()
                
    # a class  function to draw the ememies, move them
    class Enemy(pygame.sprite.Sprite):
        def __init__(self):
            pygame.sprite.Sprite.__init__(self)
            self.image = enemy_ships
            self.rect = self.image.get_rect()
            self.radius = int(self.rect.width * .75 / 2) # trying to make a hit box of correct size for enemy need HELP with this
            #pygame.draw.circle(self.image, RED, self.rect.center, self.radius)
            self.rect.x = random.randrange(WIDTH - self.rect.width)
            self.rect.y = random.randrange(-100, -40)
            self.speedy = random.randrange(1, 8)
            self.speedx = random.randrange(-3, 3)
            self.last_update = pygame.time.get_ticks()

        def move(self): # moves the ememies down and randomly to the left or right
            self.rect.x += self.speedx
            self.rect.y += self.speedy + enemy_speedx
            if self.rect.top > HEIGHT + 10 or self.rect.left < -25 or self.rect.right > WIDTH + 20:
                self.rect.x = random.randrange(WIDTH - self.rect.width)
                self.rect.y = random.randrange(-100, -40)
                self.speedy = random.randrange(1, 8)

    # a class  function to draw the BOSS MONSTER, move it
    class Boss(pygame.sprite.Sprite):
        def __init__(self, center, size):     
            pygame.sprite.Sprite.__init__(self)
            self.size = size
            self.image = eye_anim[self.size][0]
            self.rect = self.image.get_rect()
            self.rect.center = center
            self.frame = 0
            self.radius = int(self.rect.width * .68 / 2) # trying to make a hit box of correct size for enemy need HELP with this
            #pygame.draw.circle(self.image, RED, self.rect.center, self.radius)
            self.rect.x = random.randrange(WIDTH - self.rect.width)
            self.rect.y = 100
            self.speedy = boss_speedy
            self.speedx = boss_speedx
            self.shoot_delay = 140
            self.last_shot = pygame.time.get_ticks()
            self.last_update = pygame.time.get_ticks()
            self.frame_rate = 25
            self.life = 100

        def move(self): # moves the ememies down and randomly to the left or right
            if not boss_explode_inprogress:
                now = pygame.time.get_ticks()
                if now - self.last_update > self.frame_rate:
                    self.last_update = now
                    self.frame += 1
                    if self.frame == len(eye_anim[self.size]):
                        self.frame=0
                    else:
                        #center = self.rect.center
                        self.image = eye_anim[self.size][self.frame]
                        #self.rect = self.image.get_rect()
                        #self.rect.center = center  
                self.rect.x += self.speedx
                self.rect.y += self.speedy 
                if self.rect.left < 0 or self.rect.right > WIDTH :
                    #self.rect.x = random.randrange(WIDTH - 100)
                    #self.rect.y = random.randrange(100, 200)
                    #self.speedy = 0
                    self.speedx = self.speedx * -1
                if self.rect.top < 14 or self.rect.bottom > HEIGHT -130:
                    self.speedy = self.speedy * -1
                if self.rect.top < 14:
                    self.rect.y +=1
                if self.rect.bottom > HEIGHT -130:
                    self.rect.y -=1
                reverse_chance =random.randrange(1,100)
                if reverse_chance >=99:
                    self.speedx = self.speedx * -1
                if reverse_chance <=1:
                    self.speedy = self.speedy * -1
                bullet_chance = random.randrange(1,100)
                if bullet_chance >93:   # fire a bullet!
                    boss_bullet_sound.set_volume(0.5)
                    #boss_bullet_sound.play()
                    Big_boss.shoot()
                
        def shoot(self):  # create a new bullet
            Bnow = pygame.time.get_ticks()
            if Bnow - self.last_shot > self.shoot_delay:
                self.last_shot = Bnow
                limit_fire=random.randrange(1,100)
                if limit_fire<31:
                    boss_bullet_sound.play()
                    B_bullet1 = Boss_Bullet1(self.rect.centerx, self.rect.centery)   #shoot 1st Bullet
                    all_sprites.add(B_bullet1)
                    boss_bullet_list.add(B_bullet1)
                if limit_fire>30 and limit_fire<61:
                    boss_bullet_sound.play()
                    B_bullet2 = Boss_Bullet2(self.rect.centerx, self.rect.centery)   #shoot 2nd bullet
                    all_sprites.add(B_bullet2)
                    boss_bullet_list.add(B_bullet2)
                if limit_fire>60 and limit_fire<93:
                    boss_bullet_sound.play() 
                    B_bullet3 = Boss_Bullet3(self.rect.centerx, self.rect.centery)   #shoot 3rd bullet
                    all_sprites.add(B_bullet3)
                    boss_bullet_list.add(B_bullet3)
                if limit_fire>93:
                    boss_bullet_sound.play()
                    evil_laugh.play()
                    B_bullet3 = Boss_Bullet3(self.rect.centerx, self.rect.centery)   #shoot 3rd bullet
                    B_bullet1 = Boss_Bullet1(self.rect.centerx, self.rect.centery)   #shoot 1st Bullet
                    all_sprites.add(B_bullet1)
                    boss_bullet_list.add(B_bullet1) 
                    all_sprites.add(B_bullet3)
                    boss_bullet_list.add(B_bullet3)
                
    # a class function to draw the player and move, as well as start the shoot bullet process if the space bar is pressed

    class Player(pygame.sprite.Sprite):
        def __init__(self):
            pygame.sprite.Sprite.__init__(self)
            self.image = player_ship
            self.surf = pygame.Surface((140, 98))
            self.rect = self.image.get_rect(center =((WIDTH/2)-46, HEIGHT - 40))
            self.radius = 40   # trying to make a hit box of correct size for player need HELP with this
            #pygame.draw.circle(screen, RED, self.rect.center, self.radius)
            self.speedx = 0
            self.shield = 100
            self.shoot_delay = 250
            self.last_shot = pygame.time.get_ticks()
            self.lives = 3
            self.hidden = False
            self.power = 1
            self.fire_now = pygame.time.get_ticks()
            self.power_time = pygame.time.get_ticks()
            self.hide_timer = pygame.time.get_ticks()
            
        def move(self): # move the player left and right
            # unhide if hidden
            if self.hidden and pygame.time.get_ticks() - self.hide_timer > 1000:
                self.hidden = False
                self.rect.centerx = ((WIDTH /2)-46)
                self.rect.bottom = HEIGHT
            self.speedx = 0
            pressed_keys = pygame.key.get_pressed()
                
            if self.rect.left > 0:
                if pressed_keys[K_LEFT]:
                    self.speedx = -5
            if self.rect.right < WIDTH:        
                if pressed_keys[K_RIGHT]:
                    self.speedx = 5
            if pressed_keys[K_SPACE] and (pygame.time.get_ticks() - self.fire_now) >300:   # fire a bullet!
                self.fire_now = pygame.time.get_ticks()
                P1.shoot()      
            self.rect.x += self.speedx
            # timeout for powerups
            if self.power >= 2 and pygame.time.get_ticks() - self.power_time > POWERUP_TIME:
                self.power -= 1
                self.power_time = pygame.time.get_ticks()
                
        def zoom(self):
            x=0
            y=0
            Dspeed=zoom_speed
            for i in range(0, int((HEIGHT/Dspeed)/5)):
                for i in range(len(cols)):
                    color=cols[i]
                    pygame.draw.line(screen, (color), ((WIDTH/2),HEIGHT), ((WIDTH/2),HEIGHT-y), 32)
                    self.rect.x = int(WIDTH/2)-46
                    if self.rect.y>-70:
                        screen.blit(P1.image, P1.rect)
                        time.sleep(0.009)
                        self.rect.y -=8
                    pygame.display.flip()
                    bg.draw(screen)
                    time.sleep(0.009)
                    y=y+Dspeed
            x=0
            y=0
            Dspeed =+10
            for i in range(0, int(WIDTH/Dspeed/2)):
                pygame.draw.line(screen, (color), (512-x,HEIGHT), (512-x,0), 5)
                pygame.draw.line(screen, (color), (512+x,HEIGHT), (512+x,0), 5)
                pygame.display.flip()
                bg.draw(screen)
                time.sleep(0.009)
                x=x+Dspeed
            x=0
            y=0
            for i in range(0, int(WIDTH/Dspeed/2)):
                pygame.draw.line(screen, (color), (0+x,HEIGHT), (0+x,0), 5)
                pygame.draw.line(screen, (color), (WIDTH-x,HEIGHT), (WIDTH-x,0), 5)
                pygame.display.flip()
                bg.draw(screen)
                time.sleep(0.009)
                x=x+Dspeed
            self.rect.y = HEIGHT - 76   
            x=0
            y=0
            Dspeed=zoom_speed
            
        def shoot(self):
            now = pygame.time.get_ticks()
            if now - self.last_shot > self.shoot_delay:
                self.last_shot = now
                if self.power == 1:
                    bullet = Bullet(self.rect.centerx, self.rect.top+26)
                    all_sprites.add(bullet)
                    bullet_list.add(bullet)
                    bullet_sound.set_volume(0.7) 
                    bullet_sound.play()
                if self.power >= 2:
                    bullet_sound.set_volume(0.7) 
                    bullet_sound.play()
                    bullet1 = Bullet(self.rect.left+7, self.rect.centery+26)
                    bullet2 = Bullet(self.rect.right-7, self.rect.centery+26)
                    all_sprites.add(bullet1)
                    all_sprites.add(bullet2)
                    bullet_list.add(bullet1)
                    bullet_list.add(bullet2)
                    #sound.play()
                if self.power >= 3:
                    bullet_sound.set_volume(0.7) 
                    bullet_sound.play()
                    bullet1 = Bullet(self.rect.left+7, self.rect.centery+26)
                    bullet2 = Bullet(self.rect.right-7, self.rect.centery+26)
                    bullet3 = Bullet(self.rect.centerx, self.rect.top+26)
                    all_sprites.add(bullet1)
                    all_sprites.add(bullet2)
                    all_sprites.add(bullet3)
                    bullet_list.add(bullet1)
                    bullet_list.add(bullet2)
                    bullet_list.add(bullet3)
    
        def powerup(self):
            self.power += 1
            self.power_time = pygame.time.get_ticks()

        def hide(self):
            # hide the player temporarily
            self.hidden = True
            self.hide_timer = pygame.time.get_ticks()
            self.rect.center = (WIDTH +250, HEIGHT + 250)
            
    # a class function to draw the player SHIELD and move 
    class Shield(pygame.sprite.Sprite):
        def __init__(self, center):
            pygame.sprite.Sprite.__init__(self)
            self.image = player_shield
            self.rect = self.image.get_rect()
            self.rect.center = center
            self.surf = pygame.Surface((140, 68))
            self.radius = 40   # trying to make a hit box of correct size for player need HELP with this
            #pygame.draw.circle(screen, RED, self.rect.center, self.radius)
            self.speedx = 0
            self.rect.x = (P1.rect.x -10 )
        
        def move(self): # move the shield left and right
            self.rect.x = (P1.rect.x -10 )    # -------------  make sure the shield appears in the same position as the player's ship
            if shield_kill:
                all_sprites.remove(P1_shield)
                shield_sprites.remove(P1_shield)
                self.kill()

    class Explosion(pygame.sprite.Sprite):
        def __init__(self, center, size):
            pygame.sprite.Sprite.__init__(self)
            self.size = size
            self.image = explosion_anim[self.size][0]      # set first frame in animation sequence
            self.rect = self.image.get_rect()
            self.rect.center = center
            self.frame = 0
            self.last_update = pygame.time.get_ticks()
            self.frame_rate = 40

        def move(self):
            now = pygame.time.get_ticks()
            if now - self.last_update > self.frame_rate:
                self.last_update = now
                self.frame += 1                                    # goto the next frame
                if self.frame == len(explosion_anim[self.size]):
                    # if the animation has reached the last frame the remove sprite from all lists
                    shield_kill = True
                    if self.size == 'sm' or self.size == 'player' and len(shield_sprites)>0:
                        all_sprites.remove(P1_shield)
                        P1_shield.kill()
                    self.kill()
                else:
                    center = self.rect.center
                    self.image = explosion_anim[self.size][self.frame]
                    self.rect = self.image.get_rect()
                    self.rect.center = center
                    
    class Boss_explosion(pygame.sprite.Sprite):
        def __init__(self, center):
            pygame.sprite.Sprite.__init__(self)
            self.image = bexplosion[0]      # set first frame in animation sequence
            self.rect = self.image.get_rect()
            self.rect.center = center
            self.frame = 0
            self.last_update = pygame.time.get_ticks()
            self.frame_rate = 30
            

        def move(self):
            now = pygame.time.get_ticks()
            if now - self.last_update > self.frame_rate:
                self.last_update = now
                self.frame += 1                                    # goto the next frame
                time.sleep(0.005)
                if self.frame == len(bexplosion):
                    # if the animation has reached the last frame the remove sprite from all lists
                    Boss_died_anim_finished=True
                    
                    self.kill()
                    all_sprites.remove(Big_boss)
                    enemy_boss.remove(Big_boss)
                else:
                    center = self.rect.center
                    Boss_died_anim_finished=False
                    self.image = bexplosion[self.frame]
                    self.rect = self.image.get_rect()
                    self.rect.center = center       

    # -----------------------------------------------------------------------------------------------------------------------------
    # -------------------------------------------------------- END of Class Definitions -------------------------------------------
    # -----------------------------------------------------------------------------------------------------------------------------

    def do_big_explosion():
        boss_explode_sound.set_volume(1)
        boss_explode_sound.play()
        boss_explode_inprogress=True
        Big_boss.life = 100
        XX = hit.rect.center[0]
        YY = hit.rect.center[1]
        YY=YY-60
        ZZZ= (XX,YY)
        ZZ = (XX,YY+192)
        XX=XX-192                            # multiple explosions in middle and below 6 explosions in all
        AA= (XX,YY)
        BB= (XX,YY+192)
        XX=XX+384
        CC=(XX,YY)
        DD=(XX,YY+192)
        Huge_expl1 = Boss_explosion(ZZZ)
        Huge_expl2 = Boss_explosion(ZZ)
        Huge_expl3 = Boss_explosion(AA)
        Huge_expl4 = Boss_explosion(BB)
        Huge_expl5 = Boss_explosion(CC)
        Huge_expl6 = Boss_explosion(DD)
        all_sprites.add(Huge_expl1)
        all_sprites.add(Huge_expl2)
        all_sprites.add(Huge_expl3)
        all_sprites.add(Huge_expl4)
        all_sprites.add(Huge_expl5)
        all_sprites.add(Huge_expl6)
        myexpl.add(Huge_expl1)
        myexpl.add(Huge_expl2)
        myexpl.add(Huge_expl3)
        myexpl.add(Huge_expl4)
        myexpl.add(Huge_expl5)
        myexpl.add(Huge_expl6)
        myexpl.draw(screen)
                    

    # A Function to draw text
    font_name = pygame.font.match_font('arial')
    def draw_text(surf, text, size, x, y, text_color):
        font = pygame.font.Font(font_name, size)
        text_surface = font.render(text, True, text_color)
        text_rect = text_surface.get_rect()
        text_rect.midtop = (x, y)
        surf.blit(text_surface, text_rect)

    # function to draw 2 backgrounds one above the visible area            
    #def DrawBackground(): 
    #    screen.blit(bg, (0,bgY))   # draws our first bg image
    #    screen.blit(bg, (0,bgY2))  # draws the seconf bg image

    # function to show the start screen with instructions
    def show_go_screen():
        #screen.blit(bg, bg_rect)
        pygame.mixer.music.load('Sound/Background_music.mp3')   # loads BACKGROUND MUSIC
        pygame.mixer.music.set_volume(music_volume)
        pygame.mixer.music.play(-1, 1000)   # Plays background music in continous loop
        bg.draw(screen)
        draw_text(screen, "Chris' Crazy Space Game!", 84, WIDTH / 2, HEIGHT * 0.1, WHITE)
        draw_text(screen, "Arrow keys move, space to fire", 48, WIDTH / 2, HEIGHT * 0.4, WHITE)
        draw_text(screen, "PRESS  S  KEY TO START", 32, WIDTH / 2, HEIGHT * 0.6, WHITE)
        draw_text(screen, "PRESS  Q  KEY TO QUIT", 28, WIDTH / 2, HEIGHT * 0.85, WHITE)
        pygame.display.update()
        waiting = True
        while waiting:
            clock.tick(FPS)
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    main_loop=False
                    pygame.display.quit()
                    pygame.mixer.music.stop()
                    return True
                    # sys.exit()
                if event.type == pygame.KEYUP and event.key == pygame.K_q :
                    main_loop=False
                    pygame.display.quit()
                    pygame.mixer.music.stop()
                    return True
                #pygame.quit()
                    #sys.exit()
                if event.type == pygame.KEYUP and event.key == pygame.K_s :
                    waiting = False

        return False
    def new_game():
        game_over = False
        boss_explode_inprogress=False
        enemy_speedx = 1.2
        enemy_speedy = 1
        boss_speedx = 1
        boss_speedy = 1
        enemies = pygame.sprite.Group()      # create a group of sprites that include all the ememies
        boss_bullet_list = pygame.sprite.Group()
        bullet_list = pygame.sprite.Group()  # create a group of sprites for the bullets
        all_sprites = pygame.sprite.Group()  # create a group of sprites that include all the sprites from player, enemies and bullets
        enemy_boss = pygame.sprite.Group()
        shield_sprites = pygame.sprite.Group()
        powerups = pygame.sprite.Group()
        myexpl = pygame.sprite.Group()
        P1 = Player()
        all_sprites.add(P1)
        P1_shield = Shield(P1.rect.center)
        for i in range(8):   # create 8 new enemies
            new_enemy()
        score = 0            # reset score back to 0 
        shield = 100         # reset shields back to 100
        P1.lives = 3         # set lives back to 3
        direction = 1        # set scroll direction back to normal
        scroll_speed = 2     # set scroll speed back to normal
        speed = scroll_speed * direction
        Boss_level_triggered=False
        musical_volume=0.5

    def get_ready():
        #screen.blit(bg, bg_rect)
        chris_getready.play()
        bg.draw(screen)
        pygame.display.update()
        draw_text(screen, "Get Ready", 84, WIDTH / 2, HEIGHT * 0.4, WHITE)
        waiting = True   
        pygame.display.update()
        time.sleep(1.5)
        
    def super_zoom():
        #screen.blit(bg, bg_rect)
        bg.draw(screen)
        pygame.display.update()
        P1.zoom()
        #bg.scroll(speed, orientation)
        #t = pygame.time.get_ticks()
        bg.draw(screen)
        pygame.display.update()
        time.sleep(0.5)    
        
    def show_boss_level():
        #screen.blit(bg, bg_rect)
        bg.draw(screen)
        draw_text(screen, "BOSS LEVEL", 84, WIDTH / 2, HEIGHT * 0.3, WHITE)
        draw_text(screen, "Get Ready", 48, WIDTH / 2, HEIGHT * 0.6, WHITE)
        waiting = True   
        pygame.display.update()
        pygame.mixer.music.load('Sound/Boss_music2.mp3')
        pygame.mixer.music.set_volume(music_volume)
        pygame.mixer.music.play(-1, 1000)
        time.sleep(2)
        
    # function to draw the shield bar at the top middle of the screen
    def draw_boss_bar(surf, x, y, pct):
        if pct < 0:
            pct = 0
        BAR_LENGTH = 100
        BAR_HEIGHT = 15
        fill = (pct / 100) * BAR_LENGTH
        outline_rect = pygame.Rect(x, y, BAR_LENGTH, BAR_HEIGHT)
        fill_rect = pygame.Rect(x, y, fill, BAR_HEIGHT)
        pygame.draw.rect(surf, RED, fill_rect)
        pygame.draw.rect(surf, WHITE, outline_rect, 2)

    # function to draw the shield bar at the top middle of the screen
    def draw_shield_bar(surf, x, y, pct):
        if pct < 0:
            pct = 0
        BAR_LENGTH = 100
        BAR_HEIGHT = 15
        fill = (pct / 100) * BAR_LENGTH
        outline_rect = pygame.Rect(x, y, BAR_LENGTH, BAR_HEIGHT)
        fill_rect = pygame.Rect(x, y, fill, BAR_HEIGHT)
        pygame.draw.rect(surf, GREEN, fill_rect)
        pygame.draw.rect(surf, WHITE, outline_rect, 2)
        
    # function to draw the player lives on the right of the screen
    def draw_lives(surf, x, y, lives, img):
        for i in range(lives-1):
            img_rect = img.get_rect()
            img_rect.x = x + 30 * i
            img_rect.y = y
            surf.blit(img, img_rect)

    # Setting up Sprites creating the player ship        
    P1 = Player()  # creates a player ship

    # Creating Sprites Groups
    enemies = pygame.sprite.Group() # create a group of sprites that include all the ememies
    boss_bullet_list = pygame.sprite.Group()
    bullet_list = pygame.sprite.Group()  # create a group of sprites for the bullets
    all_sprites = pygame.sprite.Group()  # create a group of sprites that include all the sprites from player, enemies and bullets
    enemy_boss = pygame.sprite.Group()
    shield_sprites = pygame.sprite.Group()
    powerups = pygame.sprite.Group()
    myexpl = pygame.sprite.Group()

    all_sprites.add(P1) # adds players ship to all sprites group
    P1_shield = Shield(P1.rect.center) # spawns shield so we don't have definition problems later.

    # a function to create a new ememy sprite    
    def new_enemy():
        e = Enemy()
        all_sprites.add(e)
        enemies.add(e)
            
    for i in range(8):          # create 8 new enemies and place them in the sprite groups
        new_enemy()

    # Adding a new User event 
    INC_SPEED = pygame.USEREVENT + 1 # creates new EVENT called INC_SPEED and gives it a new ID 
    pygame.time.set_timer(INC_SPEED, 2000)    # creates the pygame event called INC_SPEED every 2000 miliseconds this will allow us to slowly increase the speed of enemies



    #  ---------------------------------------------------------------------------------------------------------------------------

    # Main Game Loop
    should_exit = show_go_screen()
    if should_exit:
        return
    
    
    get_ready()
    game_over = False        
    main_loop = True
    shield=100

    while main_loop:
        
        if game_over:
            should_exit = show_go_screen()
            if should_exit:
                return
            
            game_over = False
            boss_explode_inprogress=False
            all_sprites = pygame.sprite.Group()
            enemies = pygame.sprite.Group()
            bullet_list = pygame.sprite.Group()
            powerups = pygame.sprite.Group()
            enemy_speedx = 1.2
            enemy_speedy = 1
            boss_speedx = 1
            boss_speedy = 1
            P1 = Player()
            all_sprites.add(P1)
            for i in range(8):   # create 8 new enemies
                new_enemy()
            score = 0        # reset score back to 0 
            shield = 100     # reset shields back to 100
            P1.lives = 3     # set lives back to 3
            direction = 1    # set scroll direction back to normal 
            scroll_speed = 2
            speed = scroll_speed * direction   # Set scroll speed back to normal
            Boss_level_triggered=False
            musical_volume=0.5
            get_ready()
        bg.scroll(speed, orientation)
        t = pygame.time.get_ticks()
        bg.draw(screen)
        #DrawBackground() # makes my moving background scroll he he he Tricky Tricky!
        # Cycles through all events that are occuring  
        for event in pygame.event.get():
            if event.type == INC_SPEED:
                enemy_speedx += 0.4
                enemy_speedy +=0.4
            elif event.type == pygame.QUIT:  # quit if x in window is clicked
                main_loop=False
                pygame.display.close()
                return
                break
                # screen = pygame.display.set_mode((0, 0), FULLSCREEN)
                # sys.exit()
                #pygame.quit()
                #sys.exit()
        #if not boss_explode_inprogress:      # STOP scrolling if BOSS DIES 
        # bgY += direction * scroll_speed  # Move both background images down
        #  bgY2 += direction * scroll_speed # Move both background images down
        #  if direction >0:
            #   if bgY > bg.get_height() * 1:  # If our 1st background has moved to the bottom of the screen then reset its position back to zero
            #       bgY = 0
            #   if bgY2 > 0:
            #       bgY2 = bg.get_height() * -1 # If our 2nd background has moved to the top of the screen then reset its position back to 1 screen above the visable area
            #  if direction <0:
            #    if bgY < 0:  
            #      bgY = bg.get_height() * 1
            # if bgY2 < bg.get_height() *-1:
            #    bgY2 = 0
    #                                                check the Shield powerup AND Bolt powerup for a colission
    # ----------------------------------------------------------------------------------------------------------------------------------------------------------------

        hits = pygame.sprite.spritecollide(P1, powerups, True)
        for hit in hits:
            if hit.type == 'shield':
                powerup_shields.play()
                P1.shield += random.randrange(10, 30)
                if P1.shield >= 100:
                    P1.shield = 100
            if hit.type == 'power':
                bolt_powerup.play()
                P1.powerup()
    #                                                Check for Boss bullet and Player collisions
    # ---------------------------------------------------------------------------------------------------------------------------------------------------------------   
        if len(enemy_boss) >0:
            # Calculate mechanics for each bossbullet
            boss_hit_list = pygame.sprite.spritecollide(P1, boss_bullet_list, True)   # collision detection FOR BOSS BULLETS and PLAYER SHIP
            if len(boss_hit_list)>0 and len(shield_sprites)<=0:
                    P1_shield = Shield(P1.rect.center)
                    all_sprites.add(P1_shield)
                    shield_sprites.add(P1_shield)
            for bullet_hit in boss_hit_list:
                player_hit.play()
                P1.shield -= 10
                
                expl = Explosion(bullet_hit.rect.center, 'sm')
                all_sprites.add(expl)
                if P1.shield <= 0: # if player has no shield left then play explosion sound & animation, subract 1 from lives and reset shield to 100
                    crash_sound.play()
                    death_explosion = Explosion(P1.rect.center, 'player')
                    all_sprites.add(death_explosion)
                    P1.hide()
                    P1.lives -=1
                    P1.shield = 100
    # ---------------------------------------------------------------------------------------------------------------------------------------------------------------------  
    # -------------------------------------------- Test for collision between PLAYER BULLET and BOSS MONSTER! -------------------------------------------------------------
    # ---------------------------------------------------------------------------------------------------------------------------------------------------------------------           
    # Calculate mechanics for each bullet
        if len(enemy_boss) >0:   # ONLY CHECK IF BOSS MONSTER EXISTS
            for bullet in bullet_list:
                # See if the bullet has hit BOSS MONSTER
                enemy_hit_list = pygame.sprite.spritecollide(bullet, enemy_boss, False, pygame.sprite.collide_circle )   # collision detection using the circle hit box method
                # For each Player hit, remove the bullet and add to the score
                for hit in enemy_hit_list:
                    boss_hit_sound.play()
                    bullet_list.remove(bullet)
                    all_sprites.remove(bullet)
                    tempx = bullet.rect.x
                    tempy = hit.rect.y+126
                    tempxy = tempx, tempy
                    expl = Explosion(tempxy, 'sm')
                    all_sprites.add(expl)
                    score += 5
                    Big_boss.life -= 1
                    if Big_boss.life<5:
                        pygame.mixer.music.fadeout(1400)
                    
                    if Big_boss.life<=0 and Boss_died_anim_finished==False and boss_explode_inprogress==False and len(enemy_boss)>0:
                        do_big_explosion()
                        explosion_time = pygame.time.get_ticks()
                        
        
        if len(enemy_boss)<=0 and Boss_level_triggered==True and pygame.time.get_ticks() - explosion_time >2500:
            owwyeah.play()
            time.sleep(0.9)
            Boss_died_anim_finished=False
            pygame.display.flip()
            game_over=True
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------   
    # -------------------------------------------------------- Test for collision between PLAYER bullet and enemies! ---------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------            
        # Calculate mechanics for each bullet
        for bullet in bullet_list:
            # See if the bullet has hit an enemy
            enemy_hit_list = pygame.sprite.spritecollide(bullet, enemies, True, pygame.sprite.collide_circle )   # collision detection using the circle hit box method
            # For each enemy hit, remove the bullet and add to the score
            for hit in enemy_hit_list:
                enemy_hit.play()
                bullet_list.remove(bullet)
                all_sprites.remove(bullet)
                expl = Explosion(hit.rect.center, 'lg')
                all_sprites.add(expl)
                score += 10
                if random.random() > 0.85:
                    Pups = Powerups(hit.rect.center)
                    all_sprites.add(Pups)
                    powerups.add(Pups)
                if score<200:
                    new_enemy()
    
            # Remove the bullet if it flies up off the screen
            if bullet.rect.y < -10:
                bullet_list.remove(bullet)
                all_sprites.remove(bullet)
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------
    # -------------- Refreshes display and MOVES all Sprites  calls move function in all sprite classes----- VERY IMPORTANT -------------------------------------------- 
    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------
        for entity in all_sprites:
            screen.blit(entity.image, entity.rect)
            entity.move()
        if score >200:
            number = len(enemies)  # the number of enemies left
            if number == 0 and len(enemy_boss)==0 and Boss_level_triggered==False:     #     if there are 0 enemies left and the score is more than 150 then CHANGE background scroll direction, speed create BOSS sprite, add to sprite groups!
                super_zoom()
                Boss_level_triggered=True
                direction = -1
                scroll_speed = 6
                speed = scroll_speed * direction
                musical_volume = 0.9 # turn up music for Boss Monster level
                show_boss_level()
                Big_boss = Boss(entity.rect.center,'boss')
                all_sprites.add(Big_boss)
                enemy_boss.add(Big_boss)
                enemy_boss.draw(screen)
    # ---------------------------------------------------------------------------------------------------------------------------------------- 
    # To be run if collision occurs between PLAYER and Enemy
    # ---------------------------------------------------------------------------------------------------------------------------------------- 
        hits = pygame.sprite.spritecollide(P1, enemies, True, pygame.sprite.collide_circle)
        if len(hits)>0 and len(shield_sprites)<=0:
            P1_shield = Shield(P1.rect.center)
            all_sprites.add(P1_shield)
            shield_sprites.add(P1_shield)
        for hit in hits:
            player_hit.play()
            P1.shield -= 20
            expl = Explosion(hit.rect.center, 'sm')
            all_sprites.add(expl)
            new_enemy()
        if P1.shield <= 0: # if player has no shield left then play explosion sound & animation, subract 1 from lives and reset shield to 100
            all_sprites.remove(P1_shield)
            shield_sprites.remove(P1_shield)
            P1_shield.kill()
            shield_kill=False
            crash_sound.play()
            death_explosion = Explosion(P1.rect.center, 'player')
            all_sprites.add(death_explosion)
            P1.hide()
            P1.lives -=1
            P1.shield = 100
    # ---------------------------------------------------------------------------------------------------------------------------------------
    # If the player has no more lives left and the explosion animation has finished playing then GAME OVER   
    # ---------------------------------------------------------------------------------------------------------------------------------------           
        if P1.lives == 0 and not death_explosion.alive():
            all_sprites.remove(P1_shield)
            shield_sprites.remove(P1_shield) 
            P1_shield.kill()
            draw_text(screen, "GAME OVER", 94, WIDTH / 2, (HEIGHT / 2)-70, RED)
            pygame.display.update()
            game_over_message.play()
            time.sleep(3)
            enemy_speedx=1
            enemy_speedy=1
            if len(enemy_boss)>0:
                enemy_boss.remove(Big_boss)
                all_sprites.remove(Big_boss)
            all_sprites.remove(P1)
            P1.kill()
            game_over=True
    # ---------------------------------------------------------------------------------------------------------------------------------------       
        draw_shield_bar(screen, WIDTH / 1.3, 7, P1.shield)
        draw_text(screen, 'SCORE ' +str(score), 22, WIDTH / 11, 3, WHITE) # display score on screen on far left
        draw_lives(screen, WIDTH - 100, 7, P1.lives, player_mini_img)
        if len(enemy_boss) >0:   # ONLY DRAW BOSS BAR IF BOSS MONSTER EXISTS
            draw_text(screen, 'BOSS HEATH', 20, WIDTH / 2.4, 3, WHITE) 
            draw_boss_bar(screen, WIDTH / 2.1, 7, Big_boss.life)    
        pygame.display.flip()
        FramePerSec.tick(FPS)
    # pygame.quit()
    pygame.display.quit()



# === ./src/behaviors/game_mode/__init__.py ===
import py_trees
from src.behaviors.game_mode.PlayGameBehavior import PlayGameBehavior
from src.behaviors.SetRobotMode import SetRobotMode
from src.behaviors.diagnostic_mode.ReportErrorBehavior import ReportErrorBehavior
from src.behaviors.utils import make_press_esc_to_exit_behavior
from src.types.RobotModes import RobotMode
from src.behaviors.CountBehavior import CountBehavior
from src.behaviors.chase_mode.SelectPerson import SelectPerson
from typing import Tuple
from py_trees.composites import Sequence, Selector

def make_game_sub_tree():
    exit_mode_behavior = make_press_esc_to_exit_behavior(RobotMode.IDLE)

    play_game_behavior = PlayGameBehavior()

    set_robot_mode = SetRobotMode(RobotMode.IDLE)

     
    play_game_sequence = Sequence("Game sequence", 
                                   memory=True,
                                    children=[play_game_behavior, set_robot_mode]
                                    )

    # Diagnostic mode:
    #   Exit if esc pressed 
    #   OR 
    #   run diagnostic mode
    #       - Report Errore AND
    #       - Set robot mode to idle
    return Selector("Game mode", memory=True,children=[exit_mode_behavior, play_game_sequence])

# === ./src/behaviors/game_mode/play_windows.py ===
import os
import pygame
import multiprocessing
import time

def play_windows():
    pygame.quit()
    time.sleep(1)
    # Set the window position BEFORE initializing the display.
    print('still workin')
    pygame.init()
    os.environ['SDL_VIDEO_WINDOW_POS'] = f"{100},{100}"
    print('hmmm still going')
    screen = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
    print('set mode')
    pygame.display.set_caption('hello there')
    clock = pygame.time.Clock()
    print('NOT anymore!')
    running = True
    while running:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False
            
            # Draw a simple background (black)
            screen.fill((0, 0, 0))
            pygame.display.flip()
            clock.tick(30)
        
        #pygame.quit()
    play_windows()





# === ./src/behaviors/game_mode/PlayGameBehavior.py ===
import random
import time
import pygame
import os, sys
from pygame.locals import *
from os import path
sys.path.append("../")
defaultpaths = os.path.join('/home/raspberrypi/maxine/MiniMax/Explosion/', '/home/raspberrypi/maxine/MiniMax/Boss', '/home/raspberrypi/maxine/MiniMax/Enemy', '/home/raspberrypi/maxine/MiniMax/Powerups')
from .play_windows import play_windows
from parallax import parallax
from py_trees.common import Status
from ...types.FacialAnimation import FacialAnimation
from ...types.Sound import Sound
from ..MaxineBehavior import MaxineBehavior
from multiprocessing import Process
from .play_game import play_game

class PlayGameBehavior(MaxineBehavior):
    """
    Play Game
    """

    def __init__(self):
        super().__init__("Play Game")
        
    def update(self):
        robot = self.get_robot()
        robot.facial_animation_manager.close_window()
        
        robot.speech_manager.perform_action('Stand by for Game Mode')
        time.sleep(0.5)
        
        play_game()

               
        # encapsulate into function
        # update() -> start new process 
        # play_windows()
        #self.process.start()
        #self.process.join()

        robot.facial_animation_manager.open_window()
        return Status.SUCCESS
        #    
        # Import libraries
        
        # from pygame.locals import *
        
        

       
        # 
        #    
        #     
        # 
        # 
        
        
        return Status.SUCCESS

# === ./src/behaviors/idle_mode/__init__.py ===
import py_trees

from ...behaviors.SetRobotMode import SetRobotMode
from ...behaviors.conditions.KeyPressedCondition import KeyPressedCondition
from ...types.KeyboardKey import KeyboardKey
from ...behaviors.conditions.RandomProbCondition import RandomProbabilityCondition
from ...behaviors.idle_mode.RandomSayingBehavior import RandomSayingBehavior
from ...behaviors.utils import conditional_behavior
from ...types.RobotModes import RobotMode


def make_idle_mode_sub_tree():
    """
    Returns the idle mode subtree
    """
    keys_to_mode = {
        KeyboardKey.ESC: RobotMode.EXIT,
        KeyboardKey.K: RobotMode.KEYBOARD_CONTROL,
        KeyboardKey.C: RobotMode.CHASE,
        KeyboardKey.L: RobotMode.LIDARCHASE,
        KeyboardKey.R: RobotMode.DISCOVERY,
        KeyboardKey.H: RobotMode.HEADTURN,
        KeyboardKey.D: RobotMode.DIAGNOSTIC,
        KeyboardKey.G: RobotMode.PLAYGAME,
    }
    
    # build behavior to switch to each mode
    switch_to_mode_behaviors = []
    for key, mode in keys_to_mode.items():
        # switch to mode if key is pressed
        switch_mode = conditional_behavior(SetRobotMode(mode), KeyPressedCondition(key))
        switch_to_mode_behaviors.append(switch_mode)

    # random saying behavior
    random_saying_behavior = conditional_behavior(
        RandomSayingBehavior(), RandomProbabilityCondition(0.01)
    )

    # sub tree is a selector (will perform the first behavior that passes)
    # either switch mode because a correct key is pressed
    # or perform a random saying 0.1 % of the time
    return py_trees.composites.Selector(
        "idle mode behavior",
        memory=True,
        children=[*switch_to_mode_behaviors, random_saying_behavior],
    )


# === ./src/behaviors/idle_mode/RandomSayingBehavior.py ===
import random
from py_trees.common import Status
from ...types.FacialAnimation import FacialAnimation
from ...types.Sound import Sound
from ..MaxineBehavior import MaxineBehavior

SAYINGS = [
    "better",
    "compute",
    "cross",
    "danger",
    "directive",
    "humans",
    "cautionroguerobots",
    "chess",
    "dangerwillrobinson",
    "malfunction2",
    "nicesoftware2",
    "no5alive",
    "program",
    "selfdestruct",
    "shallweplayagame",
    "silly",
    "stare",
    "world",
    "comewithme",
    "gosomewhere2",
    "lowbattery",
    "robotnotoffended",
    "satisfiedwithmycare",
    "waitbeforeswim",
    "helpme",
]


class RandomSayingBehavior(MaxineBehavior):
    """
    Behavior that animates face and plays sound of a random saying
    """

    def __init__(self):
        super().__init__("Random Saying")
        self.sampled_saying = None

    def select_random_saying(self):
        """
        Randomly samples a saying
        """
        self.sampled_saying = random.randint(0, len(SAYINGS) - 1)

    def initialise(self) -> None:
        """
        When initialising the node, sample a saying
        """
        self.select_random_saying()

    def update(self) -> Status:
        """
        When running the node, make animation and sound managers play the saying
        """
        # get saying
        saying = SAYINGS[self.sampled_saying]

        # get animation and sound managers
        robot = self.get_robot()
        animation_manager = robot.facial_animation_manager
        sound_manager = robot.sound_manager

        # perform the animation
        sound_manager.perform_action(Sound.from_name(saying))
        animation_manager.perform_action(FacialAnimation.from_name(saying))

        # the node always passses
        return Status.SUCCESS

    def terminate(self, new_status: Status) -> None:
        """
        When finished, reset sampled saying
        """
        self.sampled_saying = None


# === ./src/behaviors/__init__.py ===


# === ./src/behaviors/CountBehavior.py ===
import py_trees
from py_trees.common import Status

from src.types.HeadMovementDirection import HeadMovementDirection
from ..action_managers.VelocityManager import VelocityConfig
from .MaxineBehavior import MaxineBehavior


class CountBehavior(MaxineBehavior):
    """
    counts amount of ticks until a certain threshold is reached
    Returns success if count has been increased, and returns failure when count is reached
    """

    def __init__(self, target):
        """
        Initialises the direction

        arguments:
            - direction: the direction to move in
        """
        super().__init__(name=f"count behavoir")
        #self.blackboard.register_key("TICK_COUNTER", access=py_trees.common.Access.WRITE)
        self.target=target
        self.tick_counter=0
        #self.blackboard.set("TICK_COUNTER", tick_counter)


    def update(self) -> Status:
        if self.tick_counter>=self.target:
            self.tick_counter=0
            return Status.FAILURE

        self.tick_counter+=1
        return Status.SUCCESS

# === ./src/behaviors/path_finding/ShowLidarBehavior.py ===
from MiniMax.src.behaviors.MaxineBehavior import MaxineBehavior
from MiniMax.src.path_finding.Position import Position


import cv2
import numpy as np
import py_trees
from matplotlib import pyplot as plt
from py_trees.common import Status


from math import radians


class ShowLidarBehavior(MaxineBehavior):
    def __init__(self):
        super().__init__("Show Lidar Behavior")
        self.blackboard.register_key(
            "TARGET_PERSON", access=py_trees.common.Access.WRITE
        )

    def plot_target(self, ax) -> Position:
        target_object = self.blackboard.get("TARGET_PERSON")

        fov = 110
        angle = target_object.spatialCoordinates.x / fov
        angle = radians(angle)
        depth = target_object.spatialCoordinates.z

        target_plot = ax.scatter([], [], 100, marker="*", color="red")
        target_plot.set_offsets(np.c_[[angle], [depth]])

    def plot_obstacles(self, ax):
        lidar_sensor = self.get_robot().lidar_sensor
        obstacles = lidar_sensor.get_reading()

        angles = [reading.angle for reading in obstacles]
        distances = [reading.distance for reading in obstacles]
        obstacle_plot = ax.scatter([], [], s=10, color="blue")
        obstacle_plot.set_offsets(np.c_[angles, distances])

    def update(self) -> Status:
        plt.close()
        fig, ax = plt.subplots(subplot_kw={"projection": "polar"})

        # angle
        ax.set_theta_zero_location("N")
        ax.set_theta_direction(-1)
        ax.set_xticks(np.radians(np.arange(0, 360, 45)))

        # distance
        range_points = [(i + 1) * self.range / 4 for i in range(4)]
        ax.set_yticks(range_points)
        ax.set_rmax(self.range)
        ax.set_rticks(range_points)

        ax.grid(True)

        self.plot_obstacles(ax)
        self.plot_target(ax)

        fig.canvas.draw()
        img_plot = np.array(fig.canvas.renderer.buffer_rgba())

        cv2.imshow("Lidar", cv2.cvtColor(img_plot, cv2.COLOR_RGBA2BGR))
        cv2.waitKey(1)

        # behavior always passes
        return Status.SUCCESS


class PathfindToTargetBehavior(MaxineBehavior):
    def __init__(self):
        super().__init__("Path find to target")
        self.blackboard.register_key("PATH", access=py_trees.common.Access.WRITE)

    def update(self) -> Status:
        return super().update()


# === ./src/behaviors/path_finding/__init__.py ===



# === ./src/behaviors/discovery_mode/RecogniseObject.py ===
import py_trees
from py_trees.common import Status
from ...behaviors.MaxineBehavior import MaxineBehavior
from ...depth_ai.ObjectDetectionReading import ObjectDetectionReading
from ...types.MovementDirection import MovementDirection
from ...depth_ai.ObjectDetectionPipeline import ObjectDetectionPipeline
import numpy as np

class RecogniseObject(MaxineBehavior):
    def __init__(self):
        super().__init__("Select an object")
        global old_things, update_count
        self.blackboard.register_key("TARGET_OBJECT", access=py_trees.common.Access.WRITE)
        self.blackboard.register_key("UPDATE_COUNTER", access=py_trees.common.Access.WRITE)
        #print("hello")
        closest_object="test"
        old_things=[]
        update_count=1
        self.blackboard.set("TARGET_OBJECT", closest_object)
        self.blackboard.set("UPDATE_COUNTER", update_count)

    def update(self) -> Status:
        global old_things, update_count
        camera = self.get_robot().camera_sensor
        reading: ObjectDetectionReading = camera.get_reading()
        things = reading.get_people_locations()
        
        if len(things) == 0:
            #print("no objects")
            return Status.FAILURE
        else:
            #print(things)
            old_things.append(things)
            #print(old_things)
            new_list=[]
            for one_object in old_things:
                if one_object not in new_list:
                    new_list.append(one_object)
            #print("Reduced List" + str(new_list))
            self.blackboard.set("TARGET_OBJECT", new_list)
            if len(old_things) >6:
                old_things=[]
        update_count=self.blackboard.get("UPDATE_COUNTER") 
        update_count=update_count+1   
        if update_count>20:
            update_count=1
        self.blackboard.set("UPDATE_COUNTER", update_count)    
            
    
            
        #things = sorted(things, key=lambda person: person.spatialCoordinates.z)
        #closest_object = things[0]
        #print(closest_object)
        
        return Status.SUCCESS


# === ./src/behaviors/discovery_mode/AnnounceFoundObject.py ===
import py_trees
from ...behaviors.MaxineBehavior import MaxineBehavior
from ...types.FacialAnimation import FacialAnimation
from ...types.Sound import Sound
from py_trees.common import Status
import depthai as dai

class AnnounceFoundObject(MaxineBehavior):
    """
    Announce that the robot has found a person
    """

    def __init__(self):
        super().__init__(f"Announce found person")
        #print("start")
        self.blackboard.register_key("TARGET_OBJECT", access=py_trees.common.Access.WRITE)
        self.blackboard.register_key("UPDATE_COUNTER", access=py_trees.common.Access.WRITE)

    def update(self) -> Status:
        robot = self.get_robot()
        target_object="Test"
        # play sound
        target_object = self.blackboard.get("TARGET_OBJECT")
        target_object=list(target_object)
        #print(len(target_object))
        update_count = self.blackboard.get("UPDATE_COUNTER")
        #print("Counter .... " +str(update_count))
        if update_count>19:
            robot.speech_manager.perform_action(str(target_object[0]))
            update_count=1
            self.blackboard.set("UPDATE_COUNTER", update_count)
        # always sucessed
        return Status.SUCCESS


# === ./src/behaviors/discovery_mode/__init__.py ===
import py_trees
from src.behaviors.CountBehavior import CountBehavior
from src.behaviors.chase_mode.SelectPerson import SelectPerson
from typing import Tuple
from py_trees.composites import Sequence, Selector

from src.behaviors.discovery_mode import HeadScanningBehavior
from src.behaviors.discovery_mode.HeadScanningBehavior import HeadScanningBehavior
from ...behaviors.MoveBehavior import MoveBehavior
from ...behaviors.SetRobotMode import SetRobotMode
from ...behaviors.discovery_mode.AnnounceFoundObject import AnnounceFoundObject
from ...behaviors.discovery_mode.RecogniseObject import RecogniseObject
from ...types.DirectionSensorLocation import DirectionSensorLocation
from ...types.MovementDirection import MovementDirection
from ...behaviors.ShowCameraFrame import ShowCameraFrame
from ...behaviors.utils import make_press_esc_to_exit_behavior
from ...types.RobotModes import RobotMode
from src.behaviors.HeadTurnBehavior import HeadTurn
from src.types.HeadMovementDirection import HeadMovementDirection
from ...behaviors.MoveBehavior import MoveBehavior
from py_trees.common import Status
from py_trees.behaviour import Behaviour
from src.behaviors.HeadTurnBehavior import HeadTurn
from src.types.HeadMovementDirection import HeadMovementDirection
from ...behaviors.MoveBehavior import MoveBehavior

def make_discovery_sub_tree():
    
    exit_mode_behavior = make_press_esc_to_exit_behavior(RobotMode.IDLE)
    show_frame = ShowCameraFrame()
    recognise_object = RecogniseObject()
    announce_found_object = AnnounceFoundObject()
    turn_behavior= HeadScanningBehavior()

    #Force head to turn after 20 ticks by count behavior returning FALSE when it reaches it's target (20)
    count_behavior= CountBehavior(20)
    
    recog_sequence = Sequence("discovery mode behavior",
        memory=True,children=[count_behavior,recognise_object,announce_found_object])

    recog_turn_selector=Selector("recognise or turn",
        memory=True,children=[recog_sequence,turn_behavior])
    
    discovery_seq = Sequence(
        "discovery mode behavior",
        memory=True,
        children=[
            show_frame,
            recog_turn_selector,    
        ],
    )

    return Selector("Discovery mode", memory=True,children=[exit_mode_behavior,discovery_seq])



# === ./src/behaviors/discovery_mode/HeadScanningBehavior.py ===
import py_trees
from ..MaxineBehavior import MaxineBehavior
from ...types.FacialAnimation import FacialAnimation
from ...types.Sound import Sound
from py_trees.common import Status
import depthai as dai
from src.behaviors.HeadTurnBehavior import HeadTurn
from src.types.HeadMovementDirection import HeadMovementDirection
from ..MoveBehavior import MoveBehavior


class HeadScanningBehavior(MaxineBehavior):
    """
    Move head a little bit to the left then right
    """

    def __init__(self):
        super().__init__(f"Announce found person")
        # start off moving left
        self.direction=HeadMovementDirection.LEFT

    def switch_direction(self):
        if self.direction ==  HeadMovementDirection.LEFT:
            self.direction=HeadMovementDirection.RIGHT
        else:
            self.direction = HeadMovementDirection.LEFT

    def update(self) -> Status:
        # switch head direction if reached the end
        robot = self.get_robot()
        head_move_manager = robot.head_move_manager
        
        
        headposition = head_move_manager.get_head_position()
        if abs(headposition) >= 0.92:
            self.switch_direction()
    
        head_move_manager.perform_action(self.direction)
        head_move_manager.perform_action(self.direction)


        return Status.SUCCESS

       

# === ./src/behaviors/conditions/RobotInModeCondition.py ===
from .Condition import Condition
from ...types.RobotModes import RobotMode


class RobotInModeCondition(Condition):
    """
    Condition based on the robot's current mode
    Will be true when the robot is in a specific mode
    """

    def __init__(self, mode: RobotMode):
        """
        Initialises the condition

        arguments:
            - mode: the mode to target
        """
        super().__init__(f"In {mode.name} mode")
        self.target_mode = mode

    def condition_met(self) -> bool:
        # condition is met if robots current mode is the target mode
        current_mode = self.get_robot().current_mode
        return current_mode == self.target_mode


# === ./src/behaviors/conditions/RandomProbCondition.py ===
import random
from .Condition import Condition


class RandomProbabilityCondition(Condition):
    """
    Condition based on a random probability
    Condition is met with probability P,
    Condition fails with probability 1- P
    """

    def __init__(self, p: float):
        """
        Initilaises the condition

        arguments:
            - p: the probability of condition being met
        """
        super().__init__(f"Random with chance {p}")
        self.p = p

    def condition_met(self) -> bool:
        # condition met if p less than a uniform random var
        # (true p % of the time)
        rnd = random.random()
        return self.p >= rnd


# === ./src/behaviors/conditions/__init__.py ===


# === ./src/behaviors/conditions/KeyPressedCondition.py ===
from .Condition import Condition
from ...types.KeyboardKey import KeyboardKey


class KeyPressedCondition(Condition):
    """
    Condition based on a Key being pressed.
    The condition is true if a certain key is currently being pressed down.
    """

    def __init__(self, key: KeyboardKey):
        """
        Initialises the Condition.

        arguments:
            - key: the key to condition on
            (if key is none condition is true if no keys are being pressed)
        """
        super().__init__(name=f"{key.name} pressed")
        self.target_key = key

    def condition_met(self) -> bool:
        # find keys currently pressed down
        sensor = self.get_robot().keyboard_sensor
        keys_pressed = sensor.get_reading()

        # condition met if key is being pressed
        return self.target_key in keys_pressed


# === ./src/behaviors/conditions/Condition.py ===
from py_trees.common import Status
from ..MaxineBehavior import MaxineBehavior

from abc import ABC, abstractmethod


class Condition(MaxineBehavior, ABC):
    """
    The base class for all conditions.
    A condition is a Node in the tree that will return:
        - SUCESS if a condition is met
        - FAILURE if it is not

    This can be combined with Selector nodes to conditionally follow paths in the tree
    This is an abstract class that cannot be instantated
    """

    def __init__(self, name: str):
        """
        Initialises the condition
        """
        super().__init__(name)

    @abstractmethod
    def condition_met(self) -> bool:
        """
        Checks if the condition is met
        Children must implement this
        """
        pass

    def update(self) -> Status:
        """
        Tree Behavior update function
        Returns SUCESS or FAILURE depending on condition
        """
        if self.condition_met():
            return Status.SUCCESS

        return Status.FAILURE


# === ./src/behaviors/HeadTurnBehavior.py ===
from py_trees.common import Status

from src.types.HeadMovementDirection import HeadMovementDirection
from ..action_managers.VelocityManager import VelocityConfig
from .MaxineBehavior import MaxineBehavior


class HeadTurn(MaxineBehavior):
    """
    Moves the robot in a particular direction
    """

    def __init__(self, direction: HeadMovementDirection):
        """
        Initialises the direction

        arguments:
            - direction: the direction to move in
        """
        super().__init__(name=f"Move head in {direction.value} direction")
        self.direction = direction

    def update(self) -> Status:
        # tell velocity manager to move in direction
        head_move_manager = self.get_robot().head_move_manager
        head_move_manager.perform_action(self.direction)

        # always returns sucess
        return Status.SUCCESS

# === ./src/behaviors/ShowCameraFrame.py ===
import cv2
from py_trees.common import Status
from ..behaviors.MaxineBehavior import MaxineBehavior


class ShowCameraFrame(MaxineBehavior):
    """
    Displays the current camera frame in a CV2 window
    """

    def __init__(self):
        """
        Initilialises the behavior
        """
        super().__init__("Show Camera Frame")

    def update(self) -> Status:
        # get the current frame
        camera = self.get_robot().camera_sensor
        reading = camera.get_reading()
        frame = reading.get_frame()

        # display the current frame
        cv2.imshow("Camera", frame)
        cv2.waitKey(1)

        # behavior always passes
        return Status.SUCCESS


# === ./src/behaviors/MoveBehavior.py ===
import time
import py_trees
from py_trees.common import Status
from ..action_managers.VelocityManager import VelocityConfig
from ..types.MovementDirection import MovementDirection
from .MaxineBehavior import MaxineBehavior


class MoveBehavior(MaxineBehavior):
    """
    Moves the robot in a particular direction
    """

    def __init__(self, direction: MovementDirection):
        """
        Initialises the direction

        arguments:
            - direction: the direction to move in
        """
        super().__init__(name=f"Move in {direction.value} direction")
        self.direction = direction
        self.blackboard.register_key("SPEED", access=py_trees.common.Access.WRITE)

    def update(self) -> Status:
        speed_exists = self.blackboard.exists("SPEED")
        if speed_exists:
            speed= self.blackboard.get("SPEED")
        else:
            speed= 1
        # tell velocity manager to move in direction
        velocity_manager = self.get_robot().velocity_manager
        velocity_manager.perform_action(VelocityConfig(self.direction,speed))

        # always returns sucess
        return Status.SUCCESS


class IncreaseSpeedBehavior(MaxineBehavior):
    def __init__(self):
        super().__init__(name=f"Increase Behavior")
        self.blackboard.register_key("SPEED", access=py_trees.common.Access.WRITE)

    def update(self):
        speed_exists = self.blackboard.exists("SPEED")
        if speed_exists:
            speed= self.blackboard.get("SPEED")
        else:
            speed= 1

        speed = speed + 0.1
        speed = min(speed, 2)

        self.get_robot().speech_manager.perform_action(f"Speed {int(speed*100)} percent")
        time.sleep(1.5)

        self.blackboard.set("SPEED", speed)
        return Status.SUCCESS


class DecreaseSpeedBehavior(MaxineBehavior):
    def __init__(self):
        super().__init__(name=f"Decrease Behavior")
        self.blackboard.register_key("SPEED", access=py_trees.common.Access.WRITE)

    def update(self):
        speed_exists = self.blackboard.exists("SPEED")
        if speed_exists:
            speed= self.blackboard.get("SPEED")
        else:
            speed= 1
                
        speed = speed - 0.1
        speed = max(speed, 0.4)

        self.get_robot().speech_manager.perform_action(f"Speed {int(speed*100)} percent")
        time.sleep(1.5)
        
        self.blackboard.set("SPEED", speed)

        return Status.SUCCESS

# === ./src/behaviors/MaxineBehavior.py ===
from typing import Any, Callable
import py_trees
from py_trees.behaviour import Behaviour
from py_trees.common import Status

from ..robot.Robot import Robot

ROBOT_BLACKBOARD = "ROBOT_BLACKBOARD"
ROBOT_KEY = "ROBOT_KEY"


class MaxineBehavior(Behaviour):
    """
    The base class for all Behaviors in our trees.
    This is an abstract class that cannot be instantiated.
    It just allows all tree nodes to acess the robot object via the get_robot() function.
    This allows all nodes to check sensors and perform actions
    """

    def __init__(self, name: str):
        """
        Initialises the behavior
        """
        super().__init__(name)

        # use py_trees blackboard functionality to share robot across all nodes in the tree
        self.blackboard = self.attach_blackboard_client(name=ROBOT_BLACKBOARD)
        self.blackboard.register_key(ROBOT_KEY, access=py_trees.common.Access.WRITE)

    def get_robot(self) -> Robot:
        """
        Returns the robot object
        """
        return self.blackboard.get(ROBOT_KEY)


# === ./src/behaviors/diagnostic_mode/ReportErrorBehavior.py ===
import random
import time
from py_trees.common import Status
from ...types.FacialAnimation import FacialAnimation
from ...types.Sound import Sound
from ..MaxineBehavior import MaxineBehavior

class ReportErrorBehavior(MaxineBehavior):
    """
    Report errors
    """

    def __init__(self):
        super().__init__("Report Errors")

    def update(self):
        # first, tell the user we are reporting errors occured
        robot = self.get_robot()
        robot.speech_manager.perform_action('Stand by for error report')
        time.sleep(2)
        if len(robot.errors_ocurred) <1:
            robot.speech_manager.perform_action('No errors to report')
            return Status.SUCCESS
        
        for error in range(len(robot.errors_ocurred)):
            robot.speech_manager.perform_action(robot.errors_ocurred[error])
            time.sleep(0.75)
        # for each error occured, read it 
        # out
        
        return Status.SUCCESS

# === ./src/behaviors/diagnostic_mode/__init__.py ===
import py_trees
from src.behaviors.SetRobotMode import SetRobotMode
from src.behaviors.diagnostic_mode.ReportErrorBehavior import ReportErrorBehavior
from src.behaviors.utils import make_press_esc_to_exit_behavior
from src.types.RobotModes import RobotMode
from src.behaviors.CountBehavior import CountBehavior
from src.behaviors.chase_mode.SelectPerson import SelectPerson
from typing import Tuple
from py_trees.composites import Sequence, Selector

def make_diagnostic_sub_tree():
    exit_mode_behavior = make_press_esc_to_exit_behavior(RobotMode.IDLE)

    report_error_behavior = ReportErrorBehavior()

    set_robot_mode = SetRobotMode(RobotMode.IDLE)

     
    diagnostic_sequence = Sequence("Diagnostic sequence", 
                                   memory=True,
                                    children=[report_error_behavior, set_robot_mode, ]
                                    )

    # Diagnostic mode:
    #   Exit if esc pressed 
    #   OR 
    #   run diagnostic mode
    #       - Report Errore AND
    #       - Set robot mode to idle
    return Selector("Diagnostic Mode", memory=True,children=[exit_mode_behavior, diagnostic_sequence])

# === ./src/__init__.py ===


# === ./src/path_finding/new_a_star.py ===
from typing import List
import heapq
import math
from matplotlib import pyplot as plt
import numpy as np
from src.path_finding.Position import Position
import math
import heapq
from typing import List

RESOLUTION = 100
PAD_ROUNDS = 0
DIRECTIONS = [
    (0, 0),
    (0, 1),
    (0, -1),
    (1, 0),
    (1, 1),
    (1, 2),
    (1, -1),
    (1, -2),
    (-1, 0),
    (-1, 1),
    (-1, 2),
    (-1, -1),
    (-1, -2),
]


def test_plt(padded_obstacles, obstacles, goal_cart, path):
    plt.scatter(
        [y[0] for y in padded_obstacles], [y[1] for y in padded_obstacles], color="r"
    )
    plt.scatter([y[0] for y in obstacles], [y[1] for y in obstacles], color="black")
    plt.scatter([y[0] for y in path], [y[1] for y in path], color="b")
    plt.scatter([goal_cart[0]], [goal_cart[1]], color="g")
    plt.show()


def polar_to_cartesian(pos: Position) -> tuple:
    if pos.distance == 0:
        return (0.0, 0.0)
    angle_rad = pos.angle
    x = pos.distance * math.cos(angle_rad)
    y = pos.distance * math.sin(angle_rad)
    return (x, y)


def cartesian_to_polar(x: float, y: float) -> Position:
    distance = math.hypot(x, y)
    if distance == 0:
        return Position(distance=0.0, angle=0.0)
    angle_rad = math.atan2(y, x)
    angle_deg = angle_rad
    return Position(angle=angle_deg, distance=distance * RESOLUTION)


def snap(pos: Position):
    cart = polar_to_cartesian(pos)
    return (int(cart[0] / RESOLUTION), int(cart[1] / RESOLUTION))


def remove_duplicates(coords):
    seen = set()
    unique_coords = []

    for coord in coords:
        if coord not in seen:
            seen.add(coord)
            unique_coords.append(coord)

    return unique_coords


def pad_obstacles(obstacles):
    padded = []
    for x, y in obstacles:
        pads = [(x_p + x, y_p + y) for x_p, y_p in DIRECTIONS]
        padded.extend(pads)
    return remove_duplicates(padded)


def pre_process_obstacles(obstacles: List[Position]):
    obstacles = remove_duplicates([snap(obs) for obs in obstacles])
    for _ in range(PAD_ROUNDS):
        obstacles = pad_obstacles(obstacles)
    return obstacles


def a_star(obstacles: List[Position], goal: Position):
    padded_obstacles = pre_process_obstacles(obstacles)
    goal = snap(goal)

    start = (0, 0)
    obstacles_set = set(padded_obstacles)

    open_set = []  # Priority queue
    heapq.heappush(open_set, (0, start))
    came_from = {}
    g_score = {start: 0}
    f_score = {start: np.linalg.norm(np.array(start) - np.array(goal), ord=1)}

    while open_set:
        _, current = heapq.heappop(open_set)

        if current == goal:
            path = []
            while current in came_from:
                path.append(current)
                current = came_from[current]
            path.append(start)
            path.reverse()
            return [cartesian_to_polar(step[0], step[1]) for step in path], (
                padded_obstacles,
                [polar_to_cartesian(obs) for obs in obstacles],
                goal,
                path,
            )

        for dx, dy in DIRECTIONS:
            neighbor = (current[0] + dx, current[1] + dy)
            if neighbor in obstacles_set:
                continue

            tentative_g_score = g_score[current] + 1
            if neighbor not in g_score or tentative_g_score < g_score[neighbor]:
                came_from[neighbor] = current
                g_score[neighbor] = tentative_g_score
                f_score[neighbor] = tentative_g_score + np.linalg.norm(
                    np.array(neighbor) - np.array(goal), ord=1
                )
                heapq.heappush(open_set, (f_score[neighbor], neighbor))

    return [], (None, None, None, None)  # No path found


# === ./src/path_finding/OccupancyGrid.py ===
from .Position import Position


import numpy as np
from scipy.ndimage import binary_dilation


from typing import List


class OccupancyGrid:
    def __init__(
        self,
        readings: List[Position],
        size: int,
    ) -> None:
        self.size = size
        grid = self.build_grid(readings)
        self.grid = self.post_process_grid(grid)

        self.shape = self.grid.shape

    def build_grid(self, readings: List[Position]):
        grid = np.zeros((self.size, self.size))

        for reading in readings:
            x, y = reading.coordinates()

            if x < 0 or x >= self.size:
                continue

            if y < 0 or y >= self.size:
                continue

            grid[x, y] = 1

        return grid

    def post_process_grid(self, grid):
        structure = np.ones((1, 1))
        dilated_grid = binary_dilation(grid, structure=structure)
        dilated_grid = dilated_grid.astype(int)

        return dilated_grid


# === ./src/path_finding/Position.py ===
import math
import numpy as np


from math import cos, sin
from typing import Tuple


class Position:
    def __init__(self, angle: float = None, distance: float = None) -> None:
        self.angle = angle
        self.distance = distance

    def __eq__(self, other):
        return math.isclose(self.angle, other.angle) and math.isclose(
            self.distance, other.distance
        )

    def __str__(self):
        return f"{math.degrees(self.angle)} {self.distance}"

    def __lt__(self, other):
        """Define less than for Coordinate to work with heapq."""
        return (self.distance, self.angle) < (other.distance, other.angle)

    def __hash__(self):
        return hash((self.angle, self.distance))

    def to_cartesian(self):
        """Convert polar coordinates to Cartesian coordinates."""
        x = self.distance * math.cos(self.angle)
        y = self.distance * math.sin(self.angle)
        return (x, y)


# === ./src/path_finding/LidarPlot.py ===
from .Position import Position


import numpy as np
from matplotlib import pyplot as plt


from typing import List


class LidarPlot:
    def __init__(self, range_mm: int, debug_algo_path=False) -> None:
        self.range = range_mm
        self.debug_algo_path = debug_algo_path

        self.init_plot(debug_algo_path)

    def init_plot(self, debug_algo_path):
        fig = plt.figure()
        ax = plt.subplot(121, projection='polar')
        debug_ax = plt.subplot(122)
        debug_ax.set_xlim(-self.range // 50, self.range// 50)
        debug_ax.set_ylim(-self.range // 50, self.range// 50)
        # debug_ax.invert_yaxis()
        # angle
        ax.set_theta_zero_location("N")
        ax.set_theta_direction(-1)
        ax.set_xticks(np.radians(np.arange(0, 360, 45)))

        # distance
        range_points = [(i + 1) * self.range / 4 for i in range(4)]
        ax.set_yticks(range_points)
        ax.set_rmax(self.range)
        ax.set_rticks(range_points)

        ax.grid(True)

        self.obstacles = ax.scatter([], [], s=10, color="blue")
        self.destination = ax.scatter([], [], 100, marker="*", color="red")
        (self.path,) = ax.plot([], [], "r-")

        if debug_algo_path:            
            self.obstacles_debug = debug_ax.scatter([], [], s=10, color="blue")
            self.obstacles_debug_original = debug_ax.scatter([], [], s=10, color="black")
            self.destination_debug = debug_ax.scatter([], [], 100, marker="*", color="red")
            (self.path_debug,) = debug_ax.plot([], [], "r-")


        fig.set_size_inches((20, 10)) 
        plt.ion()
        plt.show()

    def plot_obstacles(self, readings: List[Position]):
        angles = [reading.angle for reading in readings]
        distances = [reading.distance for reading in readings]

        self.obstacles.set_offsets(np.c_[angles, distances])

    def plot_obstacles_debug(self, readings, original_obs_debug):
        readings = np.array(readings)
        self.obstacles_debug.set_offsets(readings[:, [1, 0]])
        original = np.array(original_obs_debug)

        self.obstacles_debug_original.set_offsets(original[:, [1, 0]])

        
    def plot_destination(self, destination: Position):
        self.destination.set_offsets(np.c_[[destination.angle], [destination.distance]])

    def plot_destination_debug(self, destination):
        self.destination_debug.set_offsets(np.c_[[destination[1]], [destination[0]]])
    
    def plot_path(self, path: List[Position]):
        angles = [reading.angle for reading in path]
        distances = [reading.distance for reading in path]
        gg = np.c_[angles, distances]
        self.path.set_data(gg[:, 0], gg[:, 1])

    def plot_path_debug(self, path):
        path = np.array(path)[:, [1, 0]]
        self.path_debug.set_data(path.swapaxes(0, 1))

    def update_plot(self):
        # plt.draw()
        plt.pause(0.1)

    def close(self):
        plt.cla()
        plt.clf()
        plt.close()


# === ./src/path_finding/__init__.py ===


# === ./src/path_finding/PolarAStarSearchVector.py ===
from heapq import heappop, heappush
import math

import numpy as np

from src.path_finding.Position import Position

STEP_DISTANCE = 50
STEP_ANGLE = math.radians(4)
N_ANGLES = 2 * math.pi // STEP_ANGLE
N_PADDING_ROUNDS = 3


# build neighbour array
neighbours = []
for distance_offset in [-STEP_DISTANCE, 0, STEP_DISTANCE]:
    for angle_offset in [-1, 0, 1]:
        if angle_offset == 0 and distance_offset == 0:
            continue
        neighbours.append((angle_offset, distance_offset))
NEIGHBOURS = np.array(neighbours)


def snap_to_valid_position(coord: np.ndarray):
    if len(coord.shape) == 1:
        coord[0] = coord[0] % N_ANGLES
        coord[1] = (coord[1] / STEP_DISTANCE).round() * STEP_DISTANCE
        return coord

    coord[:, 0] = coord[:, 0] % N_ANGLES
    coord[:, 1] = (coord[:, 1] / STEP_DISTANCE).round() * STEP_DISTANCE

    return coord


def convert_to_search_coord(positions):
    if not isinstance(positions, list):
        positions = [positions]
    positions = np.array([(pos.angle // STEP_ANGLE, pos.distance) for pos in positions])

    snapped_positions = snap_to_valid_position(positions)

    return snapped_positions if snapped_positions.shape[0] > 1 else snapped_positions[0]


def convert_from_search_coord(angle, distance):
    return Position(angle * STEP_ANGLE, distance)


def pad_obstacles(obstacles):
    padded = obstacles[None, :, :]
    for _ in range(N_PADDING_ROUNDS):
        padded = NEIGHBOURS[:, None, :] + padded
    padded = padded.reshape((-1, 2))
    padded = np.unique(padded, axis=0)

    return snap_to_valid_position(padded)


def prebuild_obstacles(obstacles):
    obstacle_list = convert_to_search_coord(obstacles)

    obstacle_set = set()
    for obstacle in pad_obstacles(obstacle_list):
        obstacle_set.add(tuple(obstacle))
    return obstacle_set


def polar_distance(c1: np.ndarray, c2: np.ndarray):
    diff = np.abs(c2 - c1)
    diff[0] *= STEP_ANGLE
    return diff.sum()


def get_neighbors(coord):
    neighbourns = NEIGHBOURS + coord
    neighbourns[:, 0] = neighbourns[:, 0] % N_ANGLES
    return neighbourns


def reconstruct_path(came_from, current):

    path = [convert_from_search_coord(*current)]

    while current in came_from:
        current = tuple(came_from[current])
        path.append(convert_from_search_coord(*current))
    return path[::-1]


def get_distance(from_tuple, from_array, dest_tuple, dest_array, distances):
    dist_tuple = (*from_tuple, *dest_tuple)
    if dist_tuple in distances:
        distance = distances[dist_tuple]
    else:
        distance = polar_distance(from_array, dest_array)
        distances[dist_tuple] = distance

    return distance, distances


def a_star_search(goal: Position, obstacle_set: list[Position]):
    start = np.array((0, 0))
    start_tuple = tuple(start)

    goal = convert_to_search_coord(goal)
    goal_tuple = tuple(goal)
    obstacle_set = prebuild_obstacles(obstacle_set)

    open_set = []
    heappush(open_set, (0, start_tuple))

    came_from = {}
    g_score = {start_tuple: 0}
    start_dist = polar_distance(start, goal)
    distances = {}
    distances[(*start_tuple, *goal_tuple)] = start_dist
    f_score = {start_tuple: start_dist}

    while open_set:
        _, current = heappop(open_set)
        tuple_current = tuple(current)

        if np.abs(current - goal).sum() < 0.1:
            return reconstruct_path(came_from, tuple_current)

        for neighbor in get_neighbors(current):
            tuple_neighbor = tuple(neighbor.round(3))

            if tuple_neighbor in obstacle_set:
                continue

            distance, distances = get_distance(
                tuple_current, current, tuple_neighbor, neighbor, distances
            )
            tentative_g_score = g_score[tuple_current] + distance

            if (
                tuple_neighbor not in g_score
                or tentative_g_score < g_score[tuple_neighbor]
            ):

                came_from[tuple_neighbor] = current
                g_score[tuple_neighbor] = tentative_g_score

                distance, distances = get_distance(
                    tuple_neighbor, neighbor, goal_tuple, goal, distances
                )

                f_score[tuple_neighbor] = tentative_g_score + distance
                heappush(open_set, (f_score[tuple_neighbor], tuple_neighbor))

    return None


# === ./src/path_finding/AStarSearch.py ===
from queue import PriorityQueue
from typing import List

import numpy as np


from .OccupancyGrid import OccupancyGrid
from .Position import Position


class AStarSearch:
    def __init__(self, grid: OccupancyGrid, start: Position, goal: Position) -> None:
        self.grid = grid
        self.start = start
        self.goal = goal

    def find_path(self) -> List[Position]:
        try:
            came_from = self.a_star()
            return self.reconstruct_path(came_from)
        except:
            return []

    def heuristic(self, a):
        return np.linalg.norm(np.array(a) - np.array(self.goal.coordinates()))

    def a_star(self):
        frontier = PriorityQueue()
        frontier.put(self.start.coordinates(), 0)

        came_from = {}
        cost_so_far = {}
        came_from[self.start.coordinates()] = None
        cost_so_far[self.start.coordinates()] = 0

        while not frontier.empty():
            current = frontier.get()

            if current == self.goal.coordinates():
                print("found solution")
                break

            for dx, dy in [
                (-1, 0),
                (1, 0),
                (0, -1),
                (0, 1),
                (1, 1),
                (-1, -1),
                (1, -1),
                (-1, 1),
            ]:
                neighbor = (int(current[0] + dx), int(current[1] + dy))

                if (
                    0 <= neighbor[0] < self.grid.shape[0]
                    and 0 <= neighbor[1] < self.grid.shape[1]
                ):
                    if self.grid.grid[neighbor[0], neighbor[1]] == 0:
                        new_cost = cost_so_far[current] + 1

                        if (
                            neighbor not in cost_so_far
                            or new_cost < cost_so_far[neighbor]
                        ):
                            cost_so_far[neighbor] = new_cost
                            priority = new_cost + self.heuristic(neighbor)
                            frontier.put(neighbor, priority)
                            came_from[neighbor] = current

        return came_from

    def reconstruct_path(self, came_from) -> List[Position]:
        current = self.goal.coordinates()
        path = []
        while current != self.start.coordinates():
            x, y = current
            path.append(Position(x=x, y=y))
            current = came_from[current]

        path.append(self.start)
        path.reverse()

        return path


# === ./src/path_finding/PolarAStarSearch.py ===
from heapq import heappop, heappush
import math

from src.path_finding.Position import Position

STEP_DISTANCE = 100
STEP_ANGLE = math.radians(20)


def euclidean_distance(c1, c2):
    """Calculate the Euclidean distance between two polar coordinates."""
    x1, y1 = c1.to_cartesian()
    x2, y2 = c2.to_cartesian()
    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)


def snap_to_valid_position(coord, step_angle, step_distance):
    rounded_angle = round(coord.angle / step_angle) * step_angle
    rounded_distance = round(coord.distance / step_distance) * step_distance
    return Position(rounded_angle, rounded_distance)


def is_obstacle(position, obstacles, tolerance=0.1):
    for obstacle in obstacles:
        if math.isclose(
            position.angle, obstacle.angle, abs_tol=tolerance
        ) and math.isclose(position.distance, obstacle.distance, abs_tol=tolerance):
            return True
    return False


def prebuild_obstacles(obstacles, step_angle, step_distance):
    """Prebuild obstacles into a set for fast lookup."""
    obstacle_set = set()

    for obstacle in obstacles:
        # Snap each obstacle to the nearest valid position
        snapped_obstacle = snap_to_valid_position(obstacle, step_angle, step_distance)
        # Add the snapped obstacle to the set
        obstacle_set.add((snapped_obstacle.angle, snapped_obstacle.distance))

    return obstacle_set


def is_obstacle(position, obstacle_set, tolerance=1):
    """Check if the position is an obstacle by looking it up in the prebuilt set."""
    # Snap the current position to the nearest valid position
    snapped_position = snap_to_valid_position(position, STEP_ANGLE, STEP_DISTANCE)
    # Check if the snapped position is in the obstacle set
    return (snapped_position.angle, snapped_position.distance) in obstacle_set


def a_star_search(goal: Position, obstacles: list[Position]):
    """Perform A* search in polar coordinates."""
    open_set = []
    start = Position(0, 0)
    goal = snap_to_valid_position(goal, STEP_ANGLE, STEP_DISTANCE)
    obstacles = prebuild_obstacles(obstacles, STEP_ANGLE, STEP_DISTANCE)
    heappush(open_set, (0, start))

    came_from = {}
    g_score = {start: 0}
    f_score = {start: euclidean_distance(start, goal)}

    while open_set:
        _, current = heappop(open_set)

        if current == goal:
            return reconstruct_path(came_from, current)
        for neighbor in get_neighbors(current):
            if is_obstacle(neighbor, obstacles):
                continue

            tentative_g_score = g_score[current] + euclidean_distance(current, neighbor)

            if neighbor not in g_score or tentative_g_score < g_score[neighbor]:
                came_from[neighbor] = current
                g_score[neighbor] = tentative_g_score
                f_score[neighbor] = tentative_g_score + euclidean_distance(
                    neighbor, goal
                )
                heappush(open_set, (f_score[neighbor], neighbor))

    return None  # No path found


def get_neighbors(coord):
    """Generate neighboring coordinates in polar space."""
    step_angle = STEP_ANGLE  # Adjust step size as needed
    step_distance = STEP_DISTANCE  # Adjust step size as needed

    neighbors = []
    for angle_offset in [-step_angle, 0, step_angle]:
        for distance_offset in [-step_distance, 0, step_distance]:
            if angle_offset == 0 and distance_offset == 0:
                continue

            new_angle = coord.angle + angle_offset
            new_distance = coord.distance + distance_offset

            if new_distance > 0:  # Ensure the distance is positive
                neighbors.append(Position(new_angle, new_distance))

    return neighbors


def reconstruct_path(came_from, current):
    """Reconstruct the path from the start to the goal."""
    path = [current]

    while current in came_from:
        print(str(current))
        current = came_from[current]
        path.append(current)
    return path[::-1]


# === ./src/types/MovementDirection.py ===
from enum import Enum


class MovementDirection(Enum):
    """
    Represents the different directions the robot can move in
    """

    FORWARDS = "forwards"
    BACKWARDS = "backwards"

    LEFT = "left"
    RIGHT = "right"

    FORWARDS_LEFT = "forwards_left"
    FORWARDS_RIGHT = "forwards_right"

    BACKWARDS_LEFT = "backwards_left"
    BACKWARDS_RIGHT = "backwards_right"

    NONE = "none"


# === ./src/types/DirectionSensorLocation.py ===
from enum import Enum


class DirectionSensorLocation(Enum):
    FRONT = "front"
    BACK = "back"
    LEFT = "left"
    RIGHT = "right"


# === ./src/types/CameraMode.py ===
from enum import Enum


class CameraMode(Enum):
    """
    The camera modes
    """

    DISABLED = "disabled"
    OBJECT_DETECTION = "object_detection"


# === ./src/types/Sound.py ===
import pygame


from enum import Enum


class Sound(Enum):
    """
    The different sounds the robot can say
    """

    greetings = "DefaultSayings/greetings.wav"
    powerdown = "DefaultSayings/powerdown.wav"
    advised = "DefaultSayings/advised.wav"
    agegender = "DefaultSayings/agegender.wav"
    disagegender = "DefaultSayings/disagegender.wav"
    disemotional = "DefaultSayings/disemotional.wav"
    emotional = "DefaultSayings/emotional.wav"
    found_person = "DefaultSayings/found-person.wav"
    objective = "DefaultSayings/objective.wav"
    searchmode = "DefaultSayings/searchmode-off.wav"
    searchterminated = "DefaultSayings/searchterminated.wav"
    better = "DefaultSayings/better.wav"
    cautionroguerobots = "DefaultSayings/cautionroguerobots.wav"
    chess = "DefaultSayings/chess.wav"
    compute = "DefaultSayings/compute.wav"
    cross = "DefaultSayings/cross.wav"
    danger = "DefaultSayings/danger.wav"
    dangerwillrobinson = "DefaultSayings/dangerwillrobinson.wav"
    directive = "DefaultSayings/directive.wav"
    humans = "DefaultSayings/humans.wav"
    malfunction2 = "DefaultSayings/malfunction2.wav"
    nicesoftware2 = "DefaultSayings/nicesoftware2.wav"
    no5alive = "DefaultSayings/no5alive.wav"
    program = "DefaultSayings/program.wav"
    selfdestruct = "DefaultSayings/selfdestruct.wav"
    shallweplayagame = "DefaultSayings/shallweplayagame.wav"
    comewithme = "DefaultSayings/comewithme.wav"
    gosomewhere2 = "DefaultSayings/gosomewhere2.wav"
    hairybaby = "DefaultSayings/hairybaby.wav"
    lowbattery = "DefaultSayings/lowbattery.wav"
    robotnotoffended = "DefaultSayings/robotnotoffended.wav"
    satisfiedwithmycare = "DefaultSayings/satisfiedwithmycare.wav"
    waitbeforeswim = "DefaultSayings/waitbeforeswim.wav"
    silly = "DefaultSayings/silly.wav"
    stare = "DefaultSayings/stare.wav"
    world = "DefaultSayings/world.wav"
    anger = "DefaultSayings/anger.wav"
    backwards = "DefaultSayings/backwards.wav"
    cautionmovingbackwards = "DefaultSayings/cautionmovingbackwards.wav"
    cautionmovingforward = "DefaultSayings/cautionmovingforward.wav"
    dizzy = "DefaultSayings/dizzy.wav"
    forwards = "DefaultSayings/forwards.wav"
    found_you = "DefaultSayings/found-you.wav"
    happy = "DefaultSayings/happy.wav"
    helpme = "DefaultSayings/helpme.wav"
    inmyway = "DefaultSayings/inmyway.wav"
    left = "DefaultSayings/left.wav"
    movingback = "DefaultSayings/movingback.wav"
    movingforward = "DefaultSayings/movingforward.wav"
    movingleft = "DefaultSayings/movingleft.wav"
    movingright = "DefaultSayings/movingright.wav"
    neutral = "DefaultSayings/neutral.wav"
    right = "DefaultSayings/right.wav"
    sad = "DefaultSayings/sad.wav"
    search_on = "DefaultSayings/search-on.wav"
    search_person = "DefaultSayings/search-person.wav"
    seeyou = "DefaultSayings/seeyou.wav"
    surprise = "DefaultSayings/surprise.wav"
    two = "Sound/2.mp3"
    Powerup = "Sound/Powerup/Powerup_chirp2.mp3"
    questioning_computer = "Sound/Randombeeps/Questioning_computer_chirp.mp3"
    double_beep = "Sound/Randombeeps/Double_beep2.mp3"
    Powerdown = "Sound/Powerdown/Long_power_down.mp3"
    radar_bleep = "Sound/Radarscanning/Radar_bleep_chirp.mp3"
    radar_scanning = "Sound/Radarscanning/Radar_scanning_chirp.mp3"
    celebrate1 = "Sound/celebrate1.mp3"
    da_de_la = "Sound/Randombeeps/Da_de_la.mp3"

    def load_sound(self) -> pygame.mixer.Sound:
        """
        Loads a sound from file
        """
        return pygame.mixer.Sound(f"{self.value}")

    def __str__(self) -> str:
        return f"Play Sound: {self.name}"

    @staticmethod
    def from_name(name: str) -> "Sound":
        """
        Reuturns a sound from a name
        """
        # loop through all sounds looking for matching name
        for sound in Sound:
            if sound.name == name:
                return sound


# === ./src/types/FacialAnimation.py ===
from enum import Enum


class FacialAnimation(Enum):
    """
    All the Facial Animations
    """

    advised = "advised"
    agegender = "agegender"
    anger = "anger"
    backwards = "backwards"
    better = "better"
    cautionmovingbackwards = "cautionmovingbackwards"
    cautionmovingforward = "cautionmovingforward"
    cautionroguerobots = "cautionroguerobots"
    chess = "chess"
    comewithme = "comewithme"
    compute = "compute"
    cross = "cross"
    danger = "danger"
    dangerwillrobinson = "dangerwillrobinson"
    directive = "directive"
    disagegender = "disagegender"
    disemotional = "disemotional"
    dizzy = "dizzy"
    emotional = "emotional"
    forwards = "forwards"
    found_person = "found-person"
    found_you = "found-you"
    gosomewhere2 = "gosomewhere2"
    greetings = "greetings"
    hairybaby = "hairybaby"
    happy = "happy"
    helpme = "helpme"
    humans = "humans"
    inmyway = "inmyway"
    left = "left"
    lowbattery = "lowbattery"
    malfunction2 = "malfunction2"
    movingback = "movingback"
    movingforward = "movingforward"
    movingleft = "movingleft"
    movingright = "movingright"
    neutral = "neutral"
    nicesoftware2 = "nicesoftware2"
    no5alive = "no5alive"
    objective = "objective"
    powerdown = "powerdown"
    program = "program"
    right = "right"
    robotnotoffended = "robotnotoffended"
    sad = "sad"
    satisfiedwithmycare = "satisfiedwithmycare"
    search_on = "search-on"
    search_person = "search-person"
    searchmode_off = "searchmode-off"
    searchterminated = "searchterminated"
    seeyou = "seeyou"
    selfdestruct = "selfdestruct"
    shallweplayagame = "shallweplayagame"
    silly = "silly"
    stare = "stare"
    surprise = "surprise"
    waitbeforeswim = "waitbeforeswim"
    world = "world"

    def __str__(self) -> str:
        """
        Returns a string version of the animation
        """
        return f"Animating: {self.name}"

    @staticmethod
    def from_name(name: str) -> "FacialAnimation":
        """
        Loads an animation from a string name
        """

        # loop over all animations looking for this name
        for animation in FacialAnimation:
            if animation.name == name:
                return animation


# === ./src/types/KeyboardKey.py ===
import pygame


from enum import Enum


class KeyboardKey(Enum):
    """
    Represents keyboard keys
    """

    # velocity movement
    Q = pygame.K_q
    E = pygame.K_e
    W = pygame.K_w
    A = pygame.K_a
    S = pygame.K_s
    D = pygame.K_d
    Z = pygame.K_z
    X = pygame.K_x
    
    # head control
    O = pygame.K_o
    P = pygame.K_p
    
    # mode control
    ESC = pygame.K_ESCAPE
    K = pygame.K_k
    C = pygame.K_c
    L = pygame.K_l
    R = pygame.K_r
    H = pygame.K_h
    G = pygame.K_g

    Plus = pygame.K_EQUALS
    Minus = pygame.K_MINUS
    
    




# === ./src/types/HeadMovementDirection.py ===
from enum import Enum


class HeadMovementDirection(Enum):
    """
    Represents the different directions the robot can move in
    """
    LEFT = "left"
    RIGHT = "right"
    NONE = "none"


# === ./src/types/__init__.py ===


# === ./src/types/RobotModes.py ===
from enum import Enum


class RobotMode(Enum):
    """
    Represents the different robot modes
    """

    # Exit state is used to exit the program
    # once set to exit, the program will exit on the next tick
    EXIT = 0

    # Idle state is the default state of the robot
    # It will randomly run animations and sayings
    IDLE = 1

    # keyboard control mode allows movement using the keyboard
    KEYBOARD_CONTROL = 2

    # will travel towards a person
    CHASE = 3 

    # lidar chase mode 
    LIDARCHASE = 4

    # Discovery mode 
    DISCOVERY = 5

    # Head Turn Mode
    HEADTURN = 6

    # Diagnostic MOde
    DIAGNOSTIC = 7

    # Game Mode
    PLAYGAME = 8
    
    

# === ./src/sensors/KeyboardSensor.py ===
from typing import List
import pygame

from ..types.KeyboardKey import KeyboardKey
from ..sensors.RobotSensor import RobotSensor


class KeyboardReading:
    """
    Snapshot of all the keys currently pressed down.
    We can obtain this from the KeyboardSensor's get_reading() function
    """

    # stores the keys pressed down
    keys_pressed_down: List[KeyboardKey]

    def __init__(self, keys_pressed_down: List[KeyboardKey] = []) -> None:
        """
        Initialises the Keyboard reading class

        arguments:
            - keys_pressed_down: the list of keys pressed down
        """
        self.keys_pressed_down = keys_pressed_down

    def __contains__(self, key: KeyboardKey):
        """
        Checks if a key is currently pressed down.

        This function allows us to use the 'in' keyword in python
        Ie:
            if KeyboardKey.A in keyboard_readings:
        """
        if type(key) != KeyboardKey:
            return False

        return key.value in self.keys_pressed_down

    def __len__(self):
        """
        Returns the number of keys being pressed down
        """
        return len(self.keys_pressed_down)


class KeyboardSensor(RobotSensor[KeyboardReading]):
    """
    The sensor for the Keyboard connected to the robot.

    Note that due to limitiations with pygame, this class CANNOT be run in a separate thread
    """

    def __init__(self) -> None:
        """
        Initialises the Keyboard Sensor
        """
        super().__init__("Keyboard Sensor")
        # initialise readings to empty reading
        self.current_reading = KeyboardReading()

    def update_reading(self):
        """
        Will update the current readings by:
            - removing any keys that have been 'keyed-up' since we last checked
            - adding keys that have been 'keyed-down' since we last checked
        """
        # copy previous keys
        pressed_down_keys = self.current_reading.keys_pressed_down.copy()

        # remove keys from list when KEYUP event happens
        key_up_events = pygame.event.get(eventtype=[pygame.KEYUP])
        for event in key_up_events:
            if event.key in pressed_down_keys:
                pressed_down_keys.remove(event.key)

        # add keys in list when KEYDOWN event happens
        key_down_events = pygame.event.get(eventtype=[pygame.KEYDOWN])
        for event in key_down_events:
            if event.key not in pressed_down_keys:
                pressed_down_keys.append(event.key)

        # set current reading to be new reading
        self.current_reading = KeyboardReading(pressed_down_keys)

    def get_reading(self) -> KeyboardReading:
        """
        Returns the current state of the keyboard readings.
        Returns a KeyboardReading instance which contains all the keys currently being pressed down.
        """
        self.update_reading()
        return self.current_reading

    def flush_readings(self) -> None:
        """
        Resets the keyboard reading to have no keys pressed.
        This seems to be necesarry after switching modes because of pygame limitations.
        """
        self.current_reading = KeyboardReading()


# === ./src/sensors/I2CSensor.py ===
from typing import Generic, TypeVar


# the type of readings from this sensor
TReading = TypeVar("TReading")


class I2CSensor(Generic[TReading]):
    """
    Represents a single I2C sensor
    The I2C update thread will update these continuouly using the set_latest_reading method
    """

    def __init__(self, i2c_adress: int, port_index: int) -> None:
        """
        Initialises the sensor

        arguments:
            - i2c_adress: the i2c adress of the sensor
            - port_index: the index on the ultraborg board this sensor is on
        """
        self.i2c_adress = i2c_adress
        self.port_index = port_index
        self.latest_reading: TReading = None

    def get_latest_reading(self) -> TReading:
        """
        returns the latest reading of the sensor
        """
        return self.latest_reading

    def set_latest_reading(self, reading: TReading):
        """
        Sets the latest reading
        """
        self.latest_reading = reading


# === ./src/sensors/RobotSensor.py ===
from abc import ABC, abstractmethod
from typing import Generic, TypeVar

# the type of readings from this sensor
TReading = TypeVar("TReading")


class RobotSensor(Generic[TReading], ABC):
    """
    Base Class for all Sensors attached to the robot.
    This is an abstract class that cannot be instantiated
    """

    def __init__(self, name) -> None:
        self.name = name

    @abstractmethod
    def get_reading(self) -> TReading:
        """
        Returns a reading for the sensor.
        All behaviour trees nodes will call this function when they want the status of the sensor
        """
        pass


# === ./src/sensors/DistanceSensor.py ===
from typing import Dict
from ..multithreading.I2CSensorThread import I2CSensorThread
from ..sensors.I2CSensor import I2CSensor
from ..sensors.RobotSensor import RobotSensor
from ..types.DirectionSensorLocation import DirectionSensorLocation


class DistanceReading:
    """
    Represents a reading from multiple distance sensors in different directions
    """

    def __init__(self, readings: Dict[DirectionSensorLocation, float]) -> None:
        self.readings = readings

    def __str__(self):
        return ", ".join(
            [
                f"{direction.value}: {round(value, 3) if value is not None else 'none'}"
                for direction, value in self.readings.items()
            ]
        )

    def __get_item__(self, direction):
        if direction not in self.readings:
            raise ValueError(f"Direction {direction} not in readings!")

        return self.readings[direction]


class DistanceSensor(RobotSensor[DistanceReading]):
    """
    The distance sensor detects distances in multiple directions from many I2C sensors
    """

    def __init__(self, sensors: Dict[DirectionSensorLocation, I2CSensor]) -> None:
        """
        Initialises the sensor

        arguments:
            - sensors: dictionary of sensors mapping from direciton -> sensor
        """
        super().__init__("Distance Sensor")
        self.sensors = sensors

        # run polling thread
        # self.polling_thread = I2CSensorThread(self.sensors.values())
        # self.polling_thread.start()

    def get_reading(self) -> DistanceReading:
        """
        Returns the reading
        """
        # readings = {
        #     direction: sensor.get_latest_reading()
        #     for direction, sensor in self.sensors.items()
        # }
        # return DistanceReading(readings)
        return None

# === ./src/sensors/CameraSensor.py ===
from ..depth_ai.CameraReading import CameraReading
from ..depth_ai.ObjectDetectionReading import ObjectDetectionReading
from ..types.CameraMode import CameraMode
from ..depth_ai.DepthAiPipeline import DepthAiPipeline
from ..depth_ai.ObjectDetectionPipeline import ObjectDetectionPipeline
from ..sensors.RobotSensor import RobotSensor
import depthai as dai


class CameraSensor(RobotSensor):
    """
    The camera sensor runs different pipelines on the oak camera based on the current mode
    """

    # mappings between camera mode and the pipeline/reading types
    MODE_PIPELINES = {CameraMode.OBJECT_DETECTION: ObjectDetectionPipeline()}
    MODE_READINGS = {CameraMode.OBJECT_DETECTION: ObjectDetectionReading}

    def __init__(self, starting_mode: CameraMode = CameraMode.DISABLED) -> None:
        """
        Initialises the camera
        """
        super().__init__("Camera")
        self.device = None
        self.switch_mode(starting_mode)

    def stop_camera(self):
        """
        Stops the camera pipeline from running
        """
        if self.device is not None:
            self.device.close()
            self.device = None

    def start_camera(self, pipeline: DepthAiPipeline):
        """
        Starts a pipeline on the camera
        """
        if self.device is not None:
            raise Exception(
                "Trying to start camera but camera is already started (did you forget to close the previous camera connection?)"
            )

        self.device = dai.Device()
        self.device.startPipeline(pipeline.pipeline)

    def switch_mode(self, mode: CameraMode):
        """
        Switch the camera mode
        """
        # stop the current pipeline
        self.stop_camera()

        # if camera is disabled nothing to do
        if mode == CameraMode.DISABLED:
            self.current_mode = CameraMode.DISABLED
            return

        # start the relevant mode
        pipeline = self.MODE_PIPELINES[mode]
        self.start_camera(pipeline)

        self.current_mode = mode

        # set the queues to match this mode
        self.output_queues = pipeline.get_output_queues(self.device)

    def get_reading(self) -> CameraReading:
        """
        Returns the current reading from the pipeline
        """

        # disabled mode has no readings
        if self.current_mode == CameraMode.DISABLED:
            return None

        # get the lastest reading from each queue for this mode
        readings = {}
        for name, queue in self.output_queues.items():
            values = queue.getAll()
            if len(values) == 0:
                readings[name] = None

            readings[name] = values[-1]

        # build a CameraReading object from the queue readings
        return self.MODE_READINGS[self.current_mode](readings)


# === ./src/sensors/__init__.py ===



    

# === ./src/sensors/LidarSensor.py ===
from ..multithreading.LidarSensorThread import LidarSensorThread
from ..sensors.RobotSensor import RobotSensor


class LidarSensor(RobotSensor):
    def __init__(self) -> None:
        super().__init__("Lidar Sensor")
        self.obstacles = []
        thread = LidarSensorThread(self.obstacles)#, ms_delay=50)
        thread.start()
        

    def get_reading(self):
        return self.obstacles


# === ./src/old/operational_modes.py ===
import time
import operator
import depthai as dai

# import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
from .operational_mode import OperationalMode
import UltraBorg_old
import board
import adafruit_bno055
import threading

# Board #1, address 10
# UB1 = UltraBorg.UltraBorg()
# UB1.i2cAddress = 10
# UB1.Init()
# Board #2, address 11
UB2 = UltraBorg_old.UltraBorg()
# UB2.i2cAddress = 11
UB2.Init()

# Board #3, address 12
# UB3 = UltraBorg.UltraBorg()
# UB3.i2cAddress = 12
# UB3.Init()
# Board #4, address 13
# UB4 = UltraBorg.UltraBorg()
# UB4.i2cAddress = 13
# UB4.Init()
"""UB1.SetWithRetry(UB1.SetServoMaximum1, UB1.GetServoMaximum1, 4520, 5)
UB1.SetWithRetry(UB1.SetServoMinimum1, UB1.GetServoMinimum1, 1229, 5)
UB1.SetWithRetry(UB1.SetServoStartup1, UB1.GetServoStartup1, 3100, 5)
UB1.SetWithRetry(UB1.SetServoMaximum2, UB1.GetServoMaximum2, 3026, 5)
UB1.SetWithRetry(UB1.SetServoMinimum2, UB1.GetServoMinimum2, 1592, 5)
UB1.SetWithRetry(UB1.SetServoStartup2, UB1.GetServoStartup2, 2408, 5)
UB2.SetWithRetry(UB2.SetServoMaximum1, UB2.GetServoMaximum1, 4951, 5)
UB2.SetWithRetry(UB2.SetServoMinimum1, UB2.GetServoMinimum1, 1076, 5)
UB2.SetWithRetry(UB2.SetServoStartup1, UB2.GetServoStartup1, 2949, 5)
UB2.SetWithRetry(UB2.SetServoMaximum2, UB2.GetServoMaximum2, 5247, 5)
UB2.SetWithRetry(UB2.SetServoMinimum2, UB2.GetServoMinimum2, 1136, 5)
UB2.SetWithRetry(UB2.SetServoStartup2, UB2.GetServoStartup2, 3130, 5)
UB2.SetWithRetry(UB2.SetServoMaximum3, UB2.GetServoMaximum3, 4561, 5)
UB2.SetWithRetry(UB2.SetServoMinimum3, UB2.GetServoMinimum3, 989, 5)
UB2.SetWithRetry(UB2.SetServoStartup3, UB2.GetServoStartup3, 2737, 5)

i2c = board.I2C()  # uses board.SCL and board.SDA
sensor = adafruit_bno055.BNO055_I2C(i2c)
last_val = 0xFFFF

def temperature():
    global last_val  # pylint: disable=global-statement
    result = sensor.temperature
    if abs(result - last_val) == 128:
        result = sensor.temperature
        if abs(result - last_val) == 128:
            return 0b00111111 & result
    last_val = result
    return result

calibrated = False
print("Cal status (S,G,A,M):{}".format(sensor.calibration_status))
print("Temperature: {} degrees C".format(temperature()))  # Uncomment if using a Raspberry Pi
print("Euler angle: {}".format(sensor.euler))
"""

global stop_distance, obdis, colour, stop_flag
colour = (255, 255, 255)
stop_distance, obdis, stop_threads, distance = 300, 300, False, 9999


def sensor_scan(stop, distance):
    while True:
        distance[0] = int(UB2.GetDistance1())
        distance[1] = int(UB2.GetDistance2())
        distance[2] = int(UB2.GetDistance3())
        distance[3] = int(UB2.GetDistance4())
        if stop():
            break


# *****************************************************************************************************************
class RemoteControlMode(OperationalMode):
    # ALIGNMENT  attaching servo horns the screw is positioned over the sticker on the servo
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config
            # logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        # logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)

    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE

    def get_key(self) -> str:
        return "x"

    def get_state(self) -> RobotState:
        return RobotState.REMOTE_CONTROL

    def _start(self, robot):
        time.sleep(0.1)
        robot.say("Remote control mode enabled")
        print("Remote control enabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        robot.play_sound("Double_beep2")

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        robot.say("Remote Control Mode has been disabled")
        print("Remote control disabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        time.sleep(1.6)
        robot.play_sound("Long_power_down")

    def _main(self, robot):
        Direction = "neutral"
        keyup, x, y, running = 0, 0, 0, True
        # UB2.SetServoPosition3(0) #start robot head/neck in netral centered position
        hp = 0
        pantilt = 0
        k_w, k_a, k_s, k_d, k_q, k_e, k_z, k_x, k_l, k_r, k_o, k_p = (
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
        )
        while running:
            events = pygame.event.get()
            for event in events:
                # print('this is keyup at start of loop ',keyup)
                if event.type == pygame.KEYUP:
                    keyup = 1
                    if event.key == pygame.K_w:
                        k_w = 0
                    if event.key == pygame.K_a:
                        k_a = 0
                    if event.key == pygame.K_s:
                        k_s = 0
                    if event.key == pygame.K_d:
                        k_d = 0
                    if event.key == pygame.K_q:
                        k_q = 0
                    if event.key == pygame.K_e:
                        k_e = 0
                    if event.key == pygame.K_z:
                        k_z = 0
                    if event.key == pygame.K_x:
                        k_x = 0
                    if event.key == pygame.K_l:
                        k_l = 0
                    if event.key == pygame.K_r:
                        k_r = 0
                if event.type == pygame.QUIT:
                    _exit()
                    pygame.quit()
                    running = False
                    # UB2.SetServoPosition1(0);UB2.SetServoPosition2(0);UB2.SetServoPosition3(0);UB1.SetServoPosition1(0)    #set servos to default positions on exit
                if event.type == pygame.KEYDOWN:
                    keyup = 0
                    if event.key == pygame.K_ESCAPE:
                        # UB2.SetServoPosition1(0);UB2.SetServoPosition2(0);UB2.SetServoPosition3(0);UB1.SetServoPosition1(0) #set servos to default positions on exit
                        running = False
                    if event.key == pygame.K_q:
                        k_q = 1
                    if event.key == pygame.K_e:
                        k_e = 1
                    if event.key == pygame.K_w:
                        k_w = 1
                    if event.key == pygame.K_a:
                        k_a = 1
                    if event.key == pygame.K_s:
                        k_s = 1
                    if event.key == pygame.K_d:
                        k_d = 1
                    if event.key == pygame.K_z:
                        k_z = 1
                    if event.key == pygame.K_x:
                        k_x = 1
                    if event.key == pygame.K_b:
                        k_b = 1
                    if event.key == pygame.K_f:
                        k_f = 1
                    if event.key == pygame.K_l:
                        k_l = 1
                    if event.key == pygame.K_r:
                        k_r = 1
                if k_w:
                    print("w")
                    Direction = "forward"
                    x = -0.95
                    y = -0.12
                if k_a:
                    print("a")
                    Direction = "left"  # anti-clockwise euler goes down
                    x = 0.1
                    y = 0.8
                if k_d:
                    print("d")
                    Direction = "right"  # clockwise euler goes up
                    x = -0.26
                    y = -1.0
                if k_q:
                    print("q")
                    Direction = "left-forward"
                    x = -0.9
                    y = 0.1
                if k_e:
                    print("e")
                    Direction = "right-forward"
                    x = -0.9
                    y = -0.3
                if k_s:
                    print("s")
                    Direction = "back"
                    x = 0.98
                    y = 0.24
                if k_z:
                    print("z")
                    Direction = "left-reverse"
                    x = 0.98
                    y = -0.02
                if k_x:
                    print("x")
                    Direction = "right-reverse"
                    x = 0.98
                    y = 0.4
                if k_l:
                    print("Head Left")
                    Direction = "Head left"
                    UB2.SetServoPosition4(0)
                    if hp < -0.95:
                        hp = -0.95
                    else:
                        hp = hp - 0.02
                # print(hp)
                if k_r:
                    print("Head Right")
                    Direction = "Head right"
                    if hp > 0.95:
                        hp = 0.95
                    else:
                        hp = hp + 0.02
                if keyup == 1:
                    Direction = "neutral"
                    x = 0
                    y = 0
                # UB2.SetServoPosition2(x);UB2.SetServoPosition1(y);UB2.SetServoPosition3(hp)
                # Move robot by moving wheelchair joystick


class ObjectSearchOperationMode(OperationalMode):
    def __init__(
        self, label: str, biscuit_mode=True, config: RobotConfig = None
    ) -> None:
        super().__init__()
        _supported_labels = [
            "person",
            "cup",
        ]
        assert (
            label in _supported_labels
        ), f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == "person":
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == "cup":
            self.model_id = 3
            self.model_threshold = 0.3
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config
        # logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        # logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)

    def obstacleAvoid(self, avoided):
        mainloop = True
        rx = 0
        ry = 0
        forward_count = 0
        turn = ""
        while mainloop:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    mainloop = False
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        # UB2.SetServoPosition2(0)# make motors stop
                        # UB2.SetServoPosition1(0)# make motors stop
                        mainloop = False
                        break
            us1 = my_distances[0]
            us2 = my_distances[1]
            us3 = my_distances[2]
            us4 = my_distances[3]
            if us1 == 0:
                us1 = 99999
            if us2 == 0:
                us2 = 99999
            if us3 == 0:
                us3 = 99999
            if us4 == 0:
                us4 = 99999
            # usr1,usr2,usr3,usr4=int(UB3.GetDistance1()),int(UB3.GetDistance2()),int(UB3.GetDistance3()),int(UB3.GetDistance4())
            print(us1, "  ", us2, " ", us3, " ", us4, " Front sensors")
            # print (usr1, "  ",usr2, " ", usr3, " ", usr4," Rear sensors")
            if us1 < obdis or us2 < obdis or us3 < obdis or us4 < obdis:
                if (
                    us1 < obdis and us2 > obdis and us3 > obdis and us4 > obdis
                ):  # turn left,right sensor close to object
                    rx = 0.1
                    ry = 0.98
                    turn = "left"  # ant-clockwise
                if (
                    us1 < obdis and us2 < obdis and us3 > obdis and us4 > obdis
                ):  # turn left,right 2 sensors close to object
                    rx = 0.1
                    ry = 0.98
                    turn = "left"  # ant-clockwise
                if (
                    us1 < obdis and us2 < obdis and us3 < obdis and us4 > obdis
                ):  # turn left,right 3 sensors close to object
                    rx = 0.1
                    ry = 0.98
                    turn = "left"  # ant-clockwise
                if (
                    us1 > obdis and us2 < obdis and us3 > obdis and us4 > obdis
                ):  # turn left,right 3 sensors close to object
                    rx = 0.1
                    ry = 0.98
                    turn = "left"  # ant-clockwise
                if (
                    us4 < obdis and us3 > obdis and us2 > obdis and us1 > obdis
                ):  # turn right,left sensor close to object
                    rx = -0.26
                    ry = -1
                    turn = "right"  # clockwise
                if (
                    us4 < obdis and us3 < obdis and us2 > obdis and us1 > obdis
                ):  # turn right,left 2 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn = "right"  # clockwise
                if (
                    us4 < obdis and us3 < obdis and us2 < obdis and us1 > obdis
                ):  # turn right,left 3 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn = "right"  # clockwise
                if (
                    us4 > obdis and us3 < obdis and us2 > obdis and us1 > obdis
                ):  # turn right,left 3 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn = "right"  # clockwise
                if (
                    us1 < obdis and us2 < obdis and us3 < obdis and us4 < obdis
                ):  # back out of corner
                    rx = 0.98
                    ry = 0.24
                    turn = "back"
                if (
                    us2 < obdis and us3 < obdis and us1 > obdis and us4 > obdis
                ):  # front sensors triggered without other 2,turn right
                    rx = 0.1
                    ry = 0.98
                    turn = "left"  # ant-clockwise
            else:
                if turn == "left":  # turn 1 last time before going forward
                    rx = 0.1
                    ry = 0.98
                    turn = ""
                if turn == "right":  # turn 1 last time before going forward
                    rx = -0.26
                    ry = -0.1
                    turn = ""
                if turn == "back":  # go back 1 last time
                    rx = 0.98
                    ry = 0.24
                    turn = ""
                # UB2.SetServoPosition2(rx)
                # UB2.SetServoPosition1(ry)
                time.sleep(0.2)
                rx = -0.95
                ry = -0.12  # drive forward as NO objects have been detected
                # UB2.SetServoPosition2(rx)
                # UB2.SetServoPosition1(ry)
                print("moving forward")
                time.sleep(0.2)
                avoided = True
                forward_count += 1
                if forward_count > 5:
                    mainloop = False
                    forward_count = 0
            # UB2.SetServoPosition2(rx)
            # UB2.SetServoPosition1(ry)
            time.sleep(0.15)

    def avoid(self, avoided):
        z_str = str(z)
        current_z_str = str(current_z)
        us1_str = str(us1)
        us2_str = str(us2)
        us3_str = str(us3)
        us4_str = str(us4)
        print("z = " + z_str + " this is current_z = " + current_z_str)
        print(
            "Ultrasonic us1= "
            + us1_str
            + " us2 = "
            + us2_str
            + " us3 = "
            + us3_str
            + " us4 = "
            + us4_str
        )
        # UB2.SetServoPosition1(0)# make motors stop
        # UB2.SetServoPosition2(0)# make motors stop
        print("........................... obstacle detected")
        print("attempting to avoid obstacle")
        # robot.say("Obstacle detected")
        images = load_images("/home/raspberrypi/MiniMax/Animations/inmyway/")
        self.sound_manager.play_sound("inmyway")
        # preObsHeading, roll, pitch = sensor.euler #left euler goes down. Righr euler goes up.
        preObsHeading = 0  # place holder as no IMU sensor connected!
        print("Before obstacle avoided heading", preObsHeading)
        self.obstacleAvoid(avoided)
        # afterObsHeading, roll, pitch = sensor.euler
        afterObsHeading = 0  # place holder as no IMU sensor connected!
        print("after obstacle avoided heading", afterObsHeading)
        diff1 = int(preObsHeading - afterObsHeading)
        diff2 = abs(diff1)
        while diff2 > 1:
            # afterObsHeading, roll, pitch = sensor.euler#aproximate same heading
            afterObsHeading = 0  # place holder as no IMU sensor connected!
            diff1 = int(preObsHeading - afterObsHeading)  #
            diff2 = abs(diff1)  #
            if preObsHeading <= 180:
                if (
                    afterObsHeading > preObsHeading
                    and (preObsHeading + 180) > afterObsHeading
                ):
                    # turn Left or anti-clockwise to approach destination in shortest way
                    x = 0.1
                    y = 0.98
                else:  # right clockwise
                    x = -0.26
                    y = -1.0
            else:
                if (
                    preObsHeading > afterObsHeading
                    and (afterObsHeading + 180) > preObsHeading
                ):
                    # turn Right
                    x = -0.26
                    y = -1.0
                else:  # shortest way around a 360 degree circle to the pre obstacle angle
                    x = 0.1
                    y = 0.98
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        break
            # UB2.SetServoPosition2(x)
            # UB2.SetServoPosition1(y)
        print("back to original angle +- 1 degrees")
        # UB2.SetServoPosition2(0)# make motors stop
        # UB2.SetServoPosition1(0)# make motors stop
        # cv2.destroyAllWindows()
        r_person = 0

    def _exit():
        # UB2.SetServoPosition1(0)# make motors stop
        # UB2.SetServoPosition2(0)# make motors stop
        print("exit")

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)

    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(
            robot,
            success=success,
            give_biscuit_on_success=robot.config.ps_give_biscuit_on_success,
        )

    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            images = load_images("/home/raspberrypi/MiniMax/Animations/search-on/")
            self.sound_manager.play_sound("search-on")
            self.animate(images)  # (robot.config.animations[18])
            time.sleep(0.1)
            images = load_images("/home/raspberrypi/MiniMax/Animations/search-person/")
            self.sound_manager.play_sound(
                "search-person"
            )  # THIS WAS USED, TO SAY "TO GIVE A BISCUIT TO"
            self.animate(images)  # robot.config.animations[19])
        else:
            images = load_images("/home/raspberrypi/MiniMax/Animations/search-on/")
            self.sound_manager.play_sound("search-on")
            self.animate(images)  # (robot.config.animations[18])
            time.sleep(0.1)
            images = load_images("/home/raspberrypi/MiniMax/Animations/search-person/")
            self.sound_manager.play_sound(
                "search-person"
            )  # This is used if normal search person mode is activated.
            self.animate(images)  # robot.config.animations[19])
        time.sleep(0.1)
        robot.play_sound("Radar_bleep_chirp")
        time.sleep(0.1)
        # UB2.SetServoPosition1(0)  # make motors stop
        # UB2.SetServoPosition2(0)  # make motors stop

    def stop(self, robot):
        print("First stop check")
        z_str = str(z)
        current_z_str = str(current_z)
        us1_str, us2_str, us3_str, us4_str = str(us1), str(us2), str(us3), str(us4)
        print("z = " + z_str + " this is current_z = " + current_z_str)
        print(
            "Ultrasonic us1= "
            + us1_str
            + " us2 = "
            + us2_str
            + " us3 = "
            + us3_str
            + " us4 = "
            + us4_str
        )
        # UB2.SetServoPosition1(0)  # make motors stop
        # UB2.SetServoPosition2(0)  # make motors stop
        print("........................... Objective Reached")
        robot.say("Objective reached")
        cv2.destroyAllWindows()
        r_person = 0

    def _main(self, robot):
        global z, current_z, current_z_str, us1_str, us2_str, us3_str, us4_str, rx, ry, us1, us2, us3, us4, my_distances
        z, current_z, stop_distance, rx, ry = 9999, 9999, 700, 0, 0
        nnPath = "/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob"
        # MobilenetSSD label texts
        labelMap = [
            "background",
            "aeroplane",
            "bicycle",
            "bird",
            "boat",
            "bottle",
            "bus",
            "car",
            "cat",
            "chair",
            "cow",
            "diningtable",
            "dog",
            "horse",
            "motorbike",
            "person",
            "pottedplant",
            "sheep",
            "sofa",
            "train",
            "tvmonitor",
        ]
        syncNN = True
        pipeline = self._create_pipeline2(robot)
        cv2.namedWindow("rectified right", cv2.WINDOW_GUI_NORMAL)
        # Connect to device and start pipeline
        with dai.Device(pipeline) as device:
            # Output queues will be used to get the rgb frames and nn data from the outputs defined above
            previewQueue = device.getOutputQueue(
                name="right", maxSize=8, blocking=False
            )
            detectionNNQueue = device.getOutputQueue(
                name="detections", maxSize=8, blocking=False
            )
            depthQueue = device.getOutputQueue(name="depth", maxSize=8, blocking=False)
            rectifiedRight = None
            found_people = [{"objxcenter": 150, "z_depth": 99999}]
            sorted_x = {}
            detections = []
            startTime = time.monotonic()
            counter = 0
            fps = 0
            color = (255, 255, 255)
            us1, us2, us3, us4 = 9999, 9999, 9999, 9999
            my_distances = [7777, 8888, 9999, 5555]
            current_z = 99999
            stop_threads = False
            t1 = threading.Thread(
                target=sensor_scan,
                args=(
                    lambda: stop_threads,
                    my_distances,
                ),
            )
            t1.start()
            while True:
                us1 = my_distances[0]
                us2 = my_distances[1]
                us3 = my_distances[2]
                us4 = my_distances[3]
                inRectified = previewQueue.get()
                inDet = detectionNNQueue.get()
                counter += 1
                currentTime = time.monotonic()
                if (currentTime - startTime) > 1:
                    fps = counter / (currentTime - startTime)
                    counter = 0
                    startTime = currentTime
                rectifiedRight = inRectified.getCvFrame()
                detections = inDet.detections
                height = rectifiedRight.shape[0]
                width = rectifiedRight.shape[1]
                if current_z < stop_distance:
                    self.stop(robot)
                    stop_threads = True
                    t1.join()
                    return True
                if (
                    us1 < obdis or us2 < obdis or us3 < obdis or us4 < obdis
                ):  # forign object is close
                    avoided = False
                    self.avoid(avoided)
                    # UB2.SetServoPosition2(0)# make motors stop
                    # UB2.SetServoPosition1(0)# make motors stop
                    # do everything needed when robot finds obstacle
                found_peeps = 0
                n_of_track_peeps = 0
                for detection in detections:
                    label = labelMap[detection.label]
                    if label != "person":
                        continue
                    current_z = int(detection.spatialCoordinates.z)
                    roiData = detection.boundingBoxMapping
                    roi = roiData.roi
                    x1, x2, y1 = (
                        int(detection.xmin * width),
                        int(detection.xmax * width),
                        int(detection.ymin * height),
                    )
                    x_diff = x2 - x1
                    obj_x_center = int(x1 + (x_diff / 2))
                    dict = {"objxcenter": obj_x_center, "z_depth": current_z}
                    found_people.append(dict)
                    cv2.putText(
                        rectifiedRight,
                        f"Z: {current_z} ",
                        (x1 + 10, y1 + 80),
                        cv2.FONT_HERSHEY_TRIPLEX,
                        0.75,
                        color,
                    )
                    cv2.putText(
                        rectifiedRight,
                        f"c {obj_x_center} ",
                        (x1 + 10, y1 + 120),
                        cv2.FONT_HERSHEY_TRIPLEX,
                        0.75,
                        color,
                    )
                sorted_x = min(
                    (x for x in found_people if x["z_depth"] != "9999"),
                    key=lambda x: x["z_depth"],
                    default=None,
                )
                found_people = [
                    {"objxcenter": 150, "z_depth": 99999}
                ]  # reset found_people
                obj_x_center = sorted_x[
                    "objxcenter"
                ]  # get x co-ordinate of closest object
                current_z = sorted_x[
                    "z_depth"
                ]  # get z-depth co-ordinate of closest object
                x_deviation = int(robot.config.ps_xres / 2) - obj_x_center
                if (
                    abs(x_deviation) < robot.config.ps_tolerance
                ):  # is object in the middle of screen?
                    if current_z < stop_distance:
                        self.stop(robot)
                        stop_threads = True
                        t1.join()
                        return True
                    if (
                        us1 < obdis or us2 < obdis or us3 < obdis or us4 < obdis
                    ):  # object is very close
                        print("Second stop check")
                        avoided = False
                        self.avoid(avoided)
                        # UB2.SetServoPosition2(0)# make motors stop
                        # UB2.SetServoPosition1(0)# make motors stop
                    else:
                        rx = -0.95
                        ry = -0.12  # move forward
                        # UB2.SetServoPosition2(rx)
                        # UB2.SetServoPosition1(ry)
                        # print("........................... moving robot FORWARD")
                else:
                    if x_deviation > robot.config.ps_tolerance:
                        if x_deviation < robot.config.ps_far_boundry:
                            rx = -0.9
                            ry = 0.1
                            # UB2.SetServoPosition2(rx)
                            # UB2.SetServoPosition1(ry)
                            print("........... turning left while moving forward")
                        if x_deviation >= robot.config.ps_far_boundry:
                            rx = 0.1
                            ry = 0.98
                            # UB2.SetServoPosition2(rx)
                            # UB2.SetServoPosition1(ry)
                            print("..... turning left on the spot")
                    elif (x_deviation * -1) > robot.config.ps_tolerance:
                        if abs(x_deviation) < robot.config.ps_far_boundry:
                            rx = -0.9
                            ry = -0.3
                            # UB2.SetServoPosition2(rx)
                            # UB2.SetServoPosition1(ry)
                            print("............ turning right while moving forward")
                        if abs(x_deviation) >= robot.config.ps_far_boundry:
                            rx = -0.26
                            ry = -1.0
                            # UB2.SetServoPosition2(rx)
                            # UB2.SetServoPosition1(ry)
                            print("..... turning right on the spot")
                # cv2.rectangle(rectifiedRight, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)
                cv2.putText(
                    rectifiedRight,
                    f"US: {(my_distances)} ",
                    (1, 180),
                    cv2.FONT_HERSHEY_TRIPLEX,
                    0.6,
                    color,
                )
                cv2.putText(
                    rectifiedRight,
                    "fps: {:.2f}".format(fps),
                    (2, rectifiedRight.shape[0] - 4),
                    cv2.FONT_HERSHEY_TRIPLEX,
                    0.4,
                    color,
                )
                key_pressed = cv2.waitKey(1)
                if key_pressed == ord("z"):
                    # UB2.SetServoPosition2(0)# make motors stop
                    # UB2.SetServoPosition1(0)# make motors stop
                    print("Aborting search z ")
                    cv2.destroyAllWindows()
                    images = load_images(
                        "/home/raspberrypi/MiniMax/Animations/searchterminated/"
                    )
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(images)  # robot.config.animations[11])
                    print("menu waiting for keyboard input")
                    stop_threads = True
                    t1.join()
                    return False
                # rectifiedRight=cv2.resize(rectifiedRight,(600,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                cv2.imshow("rectified right", rectifiedRight)

    def _create_pipeline2(self, robot):
        nnPath = "/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob"
        # MobilenetSSD label texts
        labelMap = [
            "background",
            "aeroplane",
            "bicycle",
            "bird",
            "boat",
            "bottle",
            "bus",
            "car",
            "cat",
            "chair",
            "cow",
            "diningtable",
            "dog",
            "horse",
            "motorbike",
            "person",
            "pottedplant",
            "sheep",
            "sofa",
            "train",
            "tvmonitor",
        ]
        syncNN = True
        # Create pipeline
        pipeline = dai.Pipeline()
        # Define sources and outputs
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        spatialDetectionNetwork = pipeline.create(
            dai.node.MobileNetSpatialDetectionNetwork
        )
        imageManip = pipeline.create(dai.node.ImageManip)
        xoutManip = pipeline.create(dai.node.XLinkOut)
        nnOut = pipeline.create(dai.node.XLinkOut)
        xoutDepth = pipeline.create(dai.node.XLinkOut)
        xoutManip.setStreamName("right")
        nnOut.setStreamName("detections")
        xoutDepth.setStreamName("depth")
        # Properties
        imageManip.initialConfig.setResize(300, 300)
        # The NN model expects BGR input. By default ImageManip output type would be same as input (gray in this case)
        imageManip.initialConfig.setFrameType(dai.ImgFrame.Type.BGR888p)
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoLeft.setFps(21)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoRight.setFps(21)
        monoRight.setCamera("right")
        # StereoDepth
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        stereo.setSubpixel(False)
        # Define a neural network that will make predictions based on the source frames
        spatialDetectionNetwork.setConfidenceThreshold(0.55)
        spatialDetectionNetwork.setBlobPath(nnPath)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)
        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        imageManip.setKeepAspectRatio(False)
        imageManip.out.link(spatialDetectionNetwork.input)

        if syncNN:
            spatialDetectionNetwork.passthrough.link(xoutManip.input)
        else:
            imageManip.out.link(xoutManip.input)
        spatialDetectionNetwork.out.link(nnOut.input)
        stereo.rectifiedRight.link(imageManip.inputImage)
        # stereo.rectifiedRight.setPreviewKeepAspectRatio(False)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        spatialDetectionNetwork.passthroughDepth.link(xoutDepth.input)
        return pipeline

    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                images = load_images(
                    "/home/raspberrypi/MiniMax/Animations/found-person/"
                )
                self.sound_manager.play_sound("found-person")
                self.animate(images)  # robot.config.animations[8])
                # robot.say(f'I think I found a {self.label}')
            else:
                images = load_images(
                    "/home/raspberrypi/MiniMax/Animations/searchterminated/"
                )
                self.sound_manager.play_sound("searchterminated")
                self.animate(images)  # robot.config.animations[11])
                robot.say("what is going on")

            # robot.animate(1)
            print(f"well, hello there, I think I found a {self.label}")
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                images = load_images(
                    "/home/raspberrypi/MiniMax/Animations/searchterminated/"
                )
                self.sound_manager.play_sound("searchterminated")
                self.animate(images)  # robot.config.animations[11])
                robot.play_sound("Radar_bleep_chirp")
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                images = load_images("/home/raspberrypi/MiniMax/Animations/objective/")
                self.sound_manager.play_sound("objective")
                self.animate(images)  # robot.config.animations[9])
                time.sleep(0.5)
                robot.play_sound("celebrate1")
                time.sleep(0.2)
                robot.play_sound("Da_de_la")
                time.sleep(0.2)
                robot.play_sound("celebrate1")
                print("focus on terminal")
                time.sleep(0.1)
                return RobotState.PERSON_SEARCH

    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say("If you want a jelly bean, take one from my tray")
        # robot.animate(1)
        time.sleep(0.1)
        start = time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))
            time.sleep(1)
        robot.say("The jelly beans are leaving now bye bye")
        # robot.animate(1)
        # robot.write_serial('9z')  # make robot do a 180 degree turn
        # robot.write_serial('2z')  # stop robot
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == "person":
            return "b"
        elif not self.biscuit_mode and self.label == "person":
            return "p"
        elif self.biscuit_mode and self.label == "cup":
            return "v"
        elif not self.biscuit_mode and self.label == "cup":
            return "c"

    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == "person":
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == "person":
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == "cup":
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == "cup":
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT


# === ./src/old/MultiMsgSync.py ===
# Color frames (ImgFrame), object detection (ImgDetections) and recognition (NNData)
# messages arrive to the host all with some additional delay.
# For each ImgFrame there's one ImgDetections msg, which has multiple detections, and for each
# detection there's a NNData msg which contains recognition results.
# How it works:
# Every ImgFrame, ImgDetections and NNData message has it's own sequence number, by which we can sync messages.

class TwoStageHostSeqSync:
    def __init__(self):
        self.msgs = {}
    # name: color, detection, or recognition
    def add_msg(self, msg, name):
        seq = str(msg.getSequenceNum())
        if seq not in self.msgs:
            self.msgs[seq] = {} # Create directory for msgs
        if "recognition" not in self.msgs[seq]:
            self.msgs[seq]["recognition"] = [] # Create recognition array

        if name == "recognition":
            # Append recognition msgs to an array
            self.msgs[seq]["recognition"].append(msg)
            # print(f'Added recognition seq {seq}, total len {len(self.msgs[seq]["recognition"])}')

        elif name == "detection":
            # Save detection msg in the directory
            self.msgs[seq][name] = msg
            self.msgs[seq]["len"] = len(msg.detections)
            # print(f'Added detection seq {seq}')

        elif name == "color": # color
            # Save color frame in the directory
            self.msgs[seq][name] = msg
            # print(f'Added frame seq {seq}')


    def get_msgs(self):
        seq_remove = [] # Arr of sequence numbers to get deleted

        for seq, msgs in self.msgs.items():
            seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair

            # Check if we have both detections and color frame with this sequence number
            if "color" in msgs and "len" in msgs:

                # Check if all detected objects (faces) have finished recognition inference
                if msgs["len"] == len(msgs["recognition"]):
                    # print(f"Synced msgs with sequence number {seq}", msgs)

                    # We have synced msgs, remove previous msgs (memory cleaning)
                    for rm in seq_remove:
                        del self.msgs[rm]

                    return msgs # Returned synced msgs

        return None # No synced msgs

# === ./src/old/sound_manager_original.py ===
from typing import List, Optional
import pygame
import pathlib

# I don't think sounds in pygame block, so should be fine to run in the main process
# If the sounds do block, then will probably want to split this all into a seperate
# process

#TODO: want to change all the filenames to be more descriptive, given that we will be
# playing sounds by filename

_default_filepaths = [
    "Sound/DefaultSayings/greetings.wav",
    "Sound/DefaultSayings/powerdown.wav",
    "Sound/DefaultSayings/advised.wav",
    "Sound/DefaultSayings/agegender.wav",
    "Sound/DefaultSayings/dis-agegender.wav",
    "Sound/DefaultSayings/dis-emotional.wav",
    "Sound/DefaultSayings/emotional.wav",
    "Sound/DefaultSayings/found-person.wav",
    "Sound/DefaultSayings/objective.wav",
    "Sound/DefaultSayings/searchmode-off.wav",
    "Sound/DefaultSayings/searchterminated.wav",
    
    "Sound/MouthSayings/better.wav",
    "Sound/MouthSayings/cautionroguerobots.wav",
    "Sound/MouthSayings/chess.wav",
    "Sound/Sayings/compute.wav",
    "Sound/Sayings/cross.wav",
    "Sound/Sayings/danger.wav",
    "Sound/Sayings/dangerwillrobinson.wav",
    "Sound/Sayings/directive.wav",
    "Sound/Sayings/humans.wav",
    "Sound/Sayings/malfunction.wav",
    "Sound/Sayings/nicesoftware.wav",
    "Sound/Sayings/no5alive.wav",
    "Sound/Sayings/program.wav",
    "Sound/Sayings/selfdestruct.wav",
    "Sound/Sayings/shallweplayagame.wav",
    
    "Sound/Sayings/comewithme.wav",
    "Sound/Sayings/gosomewhere.wav",
    "Sound/Sayings/hairybaby.wav",
    "Sound/Sayings/lowbattery.wav",
    "Sound/Sayings/robotnotoffended.wav",
    "Sound/Sayings/satisfiedwithmycare.wav",
    "Sound/Sayings/waitbeforeswim.wav",
    
    "Sound/Sayings/silly.wav",
    "Sound/Sayings/stare.wav",
    "Sound/Sayings/world.wav",
    
    
       
    "Sound/2.mp3",
    "Sound/Powerup/Powerup_chirp2.mp3",
    "Sound/Randombeeps/Questioning_computer_chirp.mp3",
    "Sound/Randombeeps/Double_beep2.mp3",
    "Sound/Powerdown/Long_power_down.mp3",
    "Sound/Radarscanning/Radar_bleep_chirp.mp3",
    "Sound/Radarscanning/Radar_scanning_chirp.mp3",
    "Sound/celebrate1.mp3",
    "Sound/Randombeeps/Da_de_la.mp3",
]

class SoundManger:
    def __init__(self, config, file_paths: Optional[List[str]] = None) -> None:
        pygame.mixer.pre_init(48000, -16, 8, 8192)# initialise music,sound mixer
        pygame.mixer.init()
    
        if file_paths is not None:
            file_paths += _default_filepaths
        else:
            file_paths = _default_filepaths
        
        self.sounds = {
            pathlib.Path(f).stem: pygame.mixer.Sound(f) for f in file_paths
        }
        self.channel = pygame.mixer.Channel(1)
        
        
        
    def play_sound(self, sound):
       self.channel.play(self.sounds[sound])
       #self.sounds[sound].play()

# === ./src/old/operational_modes-old.py ===
from abc import ABC, abstractmethod
import time
import operator
import depthai as dai
import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
import UltraBorg_old

UB = UltraBorg_old.UltraBorg()      # Create a new UltraBorg object
UB.Init()
 

class OperationalMode(ABC):
    @abstractmethod
    def run(self, robot):
        pass

    @abstractmethod
    def get_key(self) -> str:
        pass

    @abstractmethod
    def get_state(self) -> RobotState:
        pass

class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        images=load_images('/home/pi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(images)
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 2)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/pi/MiniMax/Animations/disagegender/')
        self.sound_manager.play_sound("disagegender")
        self.animate(images)
        #robot.say("Age and Gender Detection disabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        images=load_images('/home/pi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(images)
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        cv2.putText(frame, emotion_name, (bbox[0]+20, y-100), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        cv2.putText(frame, emotion_name, (bbox[0]+20, y-100), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/pi/MiniMax/Animations/disemotional/')
        self.sound_manager.play_sound("disemotional")
        self.animate(images)
        #robot.say("Emotion Detection state disabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS


class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = [
            'person',
            'cup',
        ]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode

        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
            
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)  

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)
    
    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            #images=load_images('/home/pi/MiniMax/Results/searchterminated/')
            #self.sound_manager.play_sound("searchterminated")
            #self.animate(images)
            robot.say(f"Search mode enabled. Searching for {self.label} who like jellybeans")
        else:
            robot.say(f"Search mode enabled. Searching for {self.label}")
        #robot.animate(1)
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)

    def _main(self, robot):
        pipeline = self._create_pipeline2(robot)

        pin = '2z'
        r_person = 0

        with dai.Device(pipeline) as device:
            preview = device.getOutputQueue("preview", 4, False)
            tracklets = device.getOutputQueue("tracklets", 4, False)
            startTime = time.monotonic()
            counter = 0
            fps = 0
            frame = None
            found_people = {}
            sorted_tracked= {}
            while True:
                usm1 = int(UB.GetDistance1())
                usm2 = int(UB.GetDistance2())
                usm3 = int(UB.GetDistance3())
                if usm1 == 0:
                    usm1=999
                if usm2 == 0:
                    usm2=999
                if usm3 == 0:
                    usm3=999
                imgFrame = preview.get()
                track = tracklets.get()
                counter+=1
                current_time = time.monotonic()
                if (current_time - startTime) > 1 :
                    fps = counter / (current_time - startTime)
                    counter = 0
                    startTime = current_time
                color = (255, 255, 255)
                frame = imgFrame.getCvFrame()
                frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                trackletsData = track.tracklets
                old_pin=pin
                for t in trackletsData:
                    roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
                    x1 = int(roi.topLeft().x)
                    y1 = int(roi.topLeft().y)
                    x2 = int(roi.bottomRight().x)
                    y2 = int(roi.bottomRight().y)
                    xmin, ymin = x1, y1
                    xmax, ymax = x2, y2
                    x_diff, y_diff = (xmax-xmin), (ymax-ymin)
                    obj_x_center = int(xmin+(x_diff/2))
                    obj_y_center = int(ymin+(y_diff/2))
                    center_coordinates = (obj_x_center, obj_y_center)
                    # Put info in dictionary for detected / tracked object ONLY if detected object is being successfully tracked #
                    found_people[t.id] = (t.id, obj_x_center, obj_y_center, ymax, t.status.name)
                    if t.status.name!="TRACKED":
                        found_people.popitem()
                    radius = 2
                    #cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.putText(frame, f"ID: {[t.id]}", (x1 + 10, y1 + 45), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.putText(frame, t.status.name, (x1 + 10, y1 + 70), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)
                    cv2.circle(frame, center_coordinates, radius, color, 1)
                sorted_x = sorted(found_people.items(), key=operator.itemgetter(0))# key sort only items that are tracked, lowest key first #
                if len(sorted_x)<1:
                    robot.write_serial("5z")
                if len(sorted_x)>0:
                    for q in sorted_x:
                        listy=(q[1])
                        index=listy[0]
                        status=listy[4]
                        if status=="TRACKED":
                            sorted_tracked[index]=listy
                if len(sorted_tracked)>0: # if the dictionary is not empty....
                    for w in sorted_tracked:
                        newlist=(sorted_tracked[w])
                    obj_x_center= newlist[1] # get x co-ordinate of lowest ID
                    obj_y_center= newlist[2] # get y co-ordinate of lowest ID
                    ymax= newlist[3] # get ymax of lowest ID
                    x_deviation = (int(robot.config.ps_xres/2)-obj_x_center)
                    # calculate the deviation from the center of the screen
                    if(abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                        if abs(ymax>(600-robot.config.ps_bottom_buffer)):     # is object close to the bottom of the frame?
                            robot.write_serial('2z')
                            r_person=r_person+1
                            print('Old pin '+old_pin +' new pin '+pin)
                            if (r_person>3):
                                pin="2z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +' new pin '+pin)
                                print('........................... reached objective')
                                print('waiting at objective reached')
                                #robot.say("stop")
                                cv2.destroyAllWindows()
                                return True
                        else:
                            #if old_pin != pin:
                            pin="1z"
                            robot.write_serial(pin)
                            print('Old pin '+old_pin +' new pin '+pin)
                            print("........................... moving robot FORWARD")
                            #robot.say("go")
                            r_person=0
                    else:
                        if (x_deviation>robot.config.ps_tolerance):
                            if x_deviation<175:
                                pin="3z"
                                #robot.write_serial(pin)
                                print('Old pin '+old_pin +'  '+pin+'........................... turning left' )
                                #robot.say("left")
                                r_person=0
                            if x_deviation>=175:
                                pin="7z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +'  '+pin+'....... turning left on the spot' )
                                #robot.say("spot left")
                                r_person=0
                        elif ((x_deviation*-1)>robot.config.ps_tolerance):
                            if abs(x_deviation)<175:
                                pin="4z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +'  '+pin+'........................... turning right' )
                                #robot.say("right")
                                r_person=0
                            if abs(x_deviation)>=175:
                                pin="8z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +'  '+pin+'....... turning right on the spot' )
                                #robot.say("spot right")
                                r_person=0
                cv2.putText(frame, "fps: {:.2f}".format(fps), (2, frame.shape[0] - 7), cv2.FONT_HERSHEY_TRIPLEX, 0.6, color)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    print('menu waiting for keyboard input')
                    pin='5z'
                    robot.write_serial(pin)
                    return False
                sorted_tracked.clear()
                #cv2.imshow("tracker", frame)

    def _create_pipeline2(self, robot):
        # Create pipeline
        pipeline = dai.Pipeline()
        
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        
        detectionNetwork = pipeline.create(dai.node.MobileNetDetectionNetwork)
        objectTracker = pipeline.create(dai.node.ObjectTracker)
        xlinkOut = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)
        xlinkOut.setStreamName("preview")
        trackerOut.setStreamName("tracklets")
        
        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P) #THE_1080_P
        camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        camRgb.setInterleaved(False)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        camRgb.setFps(15)
        
        # testing MobileNet DetectionNetwork
        detectionNetwork.setBlobPath(robot.config.ps_nn_path)
        detectionNetwork.setConfidenceThreshold(self.model_threshold)
        detectionNetwork.input.setBlocking(False)
        objectTracker.setDetectionLabelsToTrack([self.model_id])  # track only person
        
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        #objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)
        #objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        camRgb.preview.link(detectionNetwork.input)
        objectTracker.passthroughTrackerFrame.link(xlinkOut.input)
        if robot.config.ps_full_frame_tracking:
            camRgb.setPreviewKeepAspectRatio(False)
            camRgb.video.link(objectTracker.inputTrackerFrame)
        else:
            detectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)
        detectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        detectionNetwork.out.link(objectTracker.inputDetections)
        objectTracker.out.link(trackerOut.input)
        return pipeline

    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                images=load_images('/home/pi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animator.animate(images)
                #robot.say(f'I think I found a {self.label}')
            else:
                images=load_images('/home/pi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animator.animate(images)
                #robot.say("Search mode terminated")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                images=load_images('/home/pi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images)
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                images=load_images('/home/pi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(images)
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)

                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)

        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))

            time.sleep(1)

        robot.say('The jelly beans are leaving now bye bye')

        #robot.animate(1)

        robot.write_serial('9z')  #do a 180 degree turn
        robot.write_serial('2z')
        
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT
        



# === ./src/old/animation_manager_exp.py ===
import time
import pygame
import os
global resx, resy, yoffset
resx=1280
resy=800
yoffset=70

def load_images(path):
    images = []
    for file_name in os.listdir(path):
        image = pygame.image.load(path + os.sep + file_name).convert()
        images.append(image)
    return images

class AnimationManager:
    def __init__(self, config) -> None:
        pygame.init()
        self.display = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
        self.background = pygame.Surface(self.display.get_size())
        self.clock = pygame.time.Clock()
        self.animate_delay = config.animate_delay
    
    def animate(self, ticks: int):
        for _ in range(ticks):
            clock = pygame.time.Clock()
            value = 0
            run = Tru
            for loops in range(0,len(images)):      
                clock.tick(11) 
                if value >= len(images):
                    value = 0
                    break
                image = images[value]
                y = 20
                # Displaying the image in our game window
                window.blit(image, (x, y))
                # Updating the display surface
                pygame.display.update()
                # Filling the window with black color
                window.fill((0, 0, 0))
                # Increasing the value of value variable by 1
                # after every iteration
                value += 4
                
window = pygame.display.set_mode((1280, 800))
pygame.init()
print("set mode")
images=load_images('/home/pi/MiniMax/AnimatedFace')
print("loaded")
time.sleep(1)
print('go go go go')

self.animate(1)
pygame.quit()



# === ./src/old/working_animation_manager2.py ===
import time
import os
import pygame
global resx, resy, yoffset, images
resx=1280
resy=800
yoffset=70
images=[]
def load_images(path):
    
    for file_name in os.listdir(path):
        image = pygame.image.load(path + os.sep + file_name).convert()
        images.append(image)
    return images

class AnimationManager:
    def __init__(self, config) -> None:
        pygame.init()

        self.display = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
        self.background = pygame.Surface(self.display.get_size())
        self.clock = pygame.time.Clock()
        self.animate_delay = config.animate_delay
    
    def animate(self, ticks: int):
        for _ in range(ticks):
            clock = pygame.time.Clock()
            value = 0
            run = True
            for loops in range(0,len(images)):      
                clock.tick(13) 
                if value >= len(images):
                    value = 0
                    break
                image = images[value]
                y = 20
                x = 20
                # Displaying the image in our game window
                self.display.blit(image, (x, y))
                # Updating the display surface
                pygame.display.update()
                # Filling the window with black color
                #self.display.fill((0, 0, 0))
                self.display.blit(self.background, (0, 0)) 
                # Increasing the value of value variable by 1
                # after every iteration
                value += 4

    def draw_robot_big_mouth(self):
        color=(200,50,50)
        black=(0,0,0)
        
        size1=(int(resx*0.33), int(resy*0.5)+yoffset, int(resx*0.35), int(resy*0.16)+yoffset)
        size2=(int(resx*0.345), int(resy*0.52)+yoffset, int(resx*0.32), int(resy*0.12)+yoffset)
        pygame.draw.ellipse(self.display, color, size1)
        pygame.draw.ellipse(self.display, black, size2)

        pygame.draw.rect(self.display, (50,50,150), pygame.Rect(resx*0.1, resy*0.04+yoffset, resx*0.8, resy*0.70+yoffset),  8) #head outline

        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.3), int(resy*0.22)+yoffset], 80, 4) #outer eye
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.7), int(resy*0.22)+yoffset], 80, 4)
        pygame.draw.circle(self.display,(5,5,200),[int(resx*0.3), int(resy*0.22)+yoffset], 70, 0) #blue eye
        pygame.draw.circle(self.display,(5,5,200),[int(resx*0.7), int(resy*0.22)+yoffset], 70, 0)
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.49), int(resy*0.37)+yoffset], 9, 5) #nothrals
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.51), int(resy*0.37)+yoffset], 9, 5)
        pygame.draw.circle(self.display,(0,0,0),[int(resx*0.3), int(resy*0.22)+yoffset], 29, 0) #iner eye
        pygame.draw.circle(self.display,(0,0,0),[int(resx*0.7), int(resy*0.22)+yoffset], 29, 0)
    
    def draw_robot_small_mouth(self):
        color=(200,50,50)
        black=(0,0,0)
        
        size1 = (int(resx*0.3), int(resy*0.5)+yoffset, int(resx*0.4), int(resy*0.067)+yoffset)
        size2 = (int(resx*0.35), int(resy*0.51+yoffset), int(resx*0.3), int(resy*0.05)+yoffset)

        pygame.draw.ellipse(self.display, color, size1)
        pygame.draw.ellipse(self.display, black, size2)
        pygame.draw.rect(self.display, (50,50,150), pygame.Rect(resx*0.1, resy*0.04+yoffset, resx*0.8, resy*0.70+yoffset),  8) # head outline

        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.3), int(resy*0.22)+yoffset], 80, 4) #outer eye
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.7), int(resy*0.22)+yoffset], 80, 4)
        pygame.draw.circle(self.display,(5,5,200),[int(resx*0.3), int(resy*0.22)+yoffset], 50, 0) #blue eye
        pygame.draw.circle(self.display,(5,5,200),[int(resx*0.7), int(resy*0.22)+yoffset], 50, 0)
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.49), int(resy*0.37)+yoffset], 9, 5) #nothrals
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.51), int(resy*0.37)+yoffset], 9, 5)
        pygame.draw.circle(self.display,(0,0,0),[int(resx*0.3), int(resy*0.22)+yoffset], 22, 0) #inner eye
        pygame.draw.circle(self.display,(0,0,0),[int(resx*0.7), int(resy*0.22)+yoffset], 22, 0)

# === ./src/old/operational_modes_working-26-07-24.py ===
import time
import operator
import depthai as dai
#import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images 
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
from .operational_mode import OperationalMode
import UltraBorg_old
import board
import adafruit_bno055
#from threading import Thread, Event
import threading

# Board #1, address 10
UB1 = UltraBorg_old.UltraBorg()
UB1.i2cAddress = 10
UB1.Init()
# Board #2, address 11
UB2 = UltraBorg_old.UltraBorg()
UB2.i2cAddress = 11
UB2.Init()
# Board #3, address 12
UB3 = UltraBorg_old.UltraBorg()
UB3.i2cAddress = 12
UB3.Init()
# Board #4, address 13
#UB4 = UltraBorg.UltraBorg()
#UB4.i2cAddress = 13
#UB4.Init()
UB1.SetWithRetry(UB1.SetServoMaximum1, UB1.GetServoMaximum1, 4520, 5)
UB1.SetWithRetry(UB1.SetServoMinimum1, UB1.GetServoMinimum1, 1229, 5)
UB1.SetWithRetry(UB1.SetServoStartup1, UB1.GetServoStartup1, 3100, 5)
UB1.SetWithRetry(UB1.SetServoMaximum2, UB1.GetServoMaximum2, 3026, 5)
UB1.SetWithRetry(UB1.SetServoMinimum2, UB1.GetServoMinimum2, 1592, 5)
UB1.SetWithRetry(UB1.SetServoStartup2, UB1.GetServoStartup2, 2408, 5)
UB2.SetWithRetry(UB2.SetServoMaximum1, UB2.GetServoMaximum1, 4951, 5)
UB2.SetWithRetry(UB2.SetServoMinimum1, UB2.GetServoMinimum1, 1076, 5)
UB2.SetWithRetry(UB2.SetServoStartup1, UB2.GetServoStartup1, 2949, 5)
UB2.SetWithRetry(UB2.SetServoMaximum2, UB2.GetServoMaximum2, 5247, 5)
UB2.SetWithRetry(UB2.SetServoMinimum2, UB2.GetServoMinimum2, 1136, 5)
UB2.SetWithRetry(UB2.SetServoStartup2, UB2.GetServoStartup2, 3130, 5)
UB2.SetWithRetry(UB2.SetServoMaximum3, UB2.GetServoMaximum3, 4561, 5)
UB2.SetWithRetry(UB2.SetServoMinimum3, UB2.GetServoMinimum3, 989, 5)
UB2.SetWithRetry(UB2.SetServoStartup3, UB2.GetServoStartup3, 2737, 5)

i2c = board.I2C()  # uses board.SCL and board.SDA
sensor = adafruit_bno055.BNO055_I2C(i2c)
last_val = 0xFFFF

def temperature():
    global last_val  # pylint: disable=global-statement
    result = sensor.temperature
    if abs(result - last_val) == 128:
        result = sensor.temperature
        if abs(result - last_val) == 128:
            return 0b00111111 & result
    last_val = result
    return result

calibrated = False
print("Cal status (S,G,A,M):{}".format(sensor.calibration_status))
print("Temperature: {} degrees C".format(temperature()))  # Uncomment if using a Raspberry Pi
print("Euler angle: {}".format(sensor.euler))

global stop_distance,obdis, colour, stop_flag
colour=(255,255,255)
stop_distance=400
obdis=400
stop_threads=False
distance=9999

def stop(distance,stop):
    global stop_threads
    while True:
        if stop:
            break
        try:
            distance[0]=int(UB2.GetDistance1())
            distance[1]=int(UB2.GetDistance2())
            distance[2]=int(UB2.GetDistance3())
            distance[3]=int(UB2.GetDistance4())
            print('running')
            time.sleep(0.5)
        except:
            distance[0]=9999
            distance[1]=9999
            distance[2]=9999
            distance[3]=9999
            print('Finished')
            print(stop)
            break
        
def sensor_scan(stop, distance):
    while True:
        distance[0]=int(UB2.GetDistance1())
        distance[1]=int(UB2.GetDistance2())
        distance[2]=int(UB2.GetDistance3())
        distance[3]=int(UB2.GetDistance4())
        if stop():
            break  

class RemoteControlMode(OperationalMode):
    # ALIGNMENT  attaching servo horns the screw is positioned over the sticker on the servo
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def get_key(self) -> str:
        return 'x'
    
    def get_state(self) -> RobotState:
        return RobotState.REMOTE_CONTROL
    
    def _start(self, robot):
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        #self.sound_manager.play_sound("agegender")
        #self.animate(robot.config.animations[4])
        robot.say("Remote control mode enabled")
        print("Remote control enabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Double_beep2")

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/obdisagegender/')
        #self.sound_manager.play_sound("obdisagegender")
        #self.animate(robot.config.animations[5])
        robot.say("Remote Control Mode has been disabled")
        print("Remote control disabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        #robot.animate(1)
        #engine.runAndWait()
        time.sleep(1.6)
        robot.play_sound("Long_power_down")
        
    def _main(self, robot):
        Direction='neutral'
        keyup=0
        x=0
        y=0
        running = True
        UB2.SetServoPosition3(0)
        hp=0
        pantilt=0
        k_w,k_a,k_s,k_d,k_q,k_e,k_z,k_x,k_l,k_r,k_o,k_p = 0,0,0,0,0,0,0,0,0,0,0,0
        while running:
            events = pygame.event.get()
            for event in events:
                #print('this is keyup at start of loop ',keyup)
                if event.type == pygame.KEYUP:
                    keyup=1
                    if event.key == pygame.K_w:
                        k_w = 0
                    if event.key == pygame.K_a:
                        k_a = 0
                    if event.key == pygame.K_s:
                        k_s = 0
                    if event.key == pygame.K_d:
                        k_d = 0
                    if event.key == pygame.K_q:
                        k_q = 0
                    if event.key == pygame.K_e:
                        k_e = 0
                    if event.key == pygame.K_z:
                        k_z = 0
                    if event.key == pygame.K_x:
                        k_x = 0
                    if event.key == pygame.K_l:
                        k_l = 0
                    if event.key == pygame.K_r:
                        k_r = 0
                if event.type == pygame.QUIT:
                    _exit()
                    pygame.quit()
                    running = False
                    UB2.SetServoPosition1(0)    #set servos to default positions on exit     
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition3(0)
                    UB1.SetServoPosition1(0)
                if event.type == pygame.KEYDOWN:
                    keyup=0
                    if event.key == pygame.K_ESCAPE:
                        UB2.SetServoPosition1(0)   #set servos to default positions on exit  
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition3(0)
                        UB1.SetServoPosition1(0)
                        running = False
                    if event.key == pygame.K_q:
                        k_q = 1
                    if event.key == pygame.K_e:
                        k_e = 1
                    if event.key == pygame.K_w:
                        k_w = 1
                    if event.key == pygame.K_a:
                        k_a = 1
                    if event.key == pygame.K_s:
                        k_s = 1
                    if event.key == pygame.K_d:
                        k_d = 1
                    if event.key == pygame.K_z:
                        k_z = 1
                    if event.key == pygame.K_x:
                        k_x = 1
                    if event.key == pygame.K_b:
                        k_b = 1
                    if event.key == pygame.K_f:
                        k_f = 1
                    if event.key == pygame.K_l:
                        k_l = 1
                    if event.key == pygame.K_r:
                        k_r = 1
                if k_w:
                    #print("w")
                    Direction ='forward'
                    x=-0.95
                    y=-0.12
                if k_a:
                    #print("a")
                    Direction ='left'# anti-clockwise euler goes down
                    x=0.1
                    y=0.98
                if k_d:
                    #print("d")
                    Direction='right'# clockwise euler goes up
                    x=-0.26
                    y=-1.0
                if k_q:
                    #print("q")
                    Direction ='left-forward'
                    x=-0.9
                    y=0.1
                if k_e:
                    #print("e")
                    Direction ='right-forward'
                    x=-0.9
                    y=-0.3
                if k_s:
                    #print("s")
                    Direction='back'
                    x=0.98
                    y=0.24
                if k_z:
                    #print("z")
                    Direction ='left-reverse'
                    x=0.98
                    y=-0.02
                if k_x:
                    #print("x")
                    Direction ='right-reverse'
                    x=0.98
                    y=0.4
                if k_l:
                    #print("Head Left")
                    Direction='Head left'
                    UB2.SetServoPosition4(0)
                    if hp <-0.95:
                        hp=-0.95
                    else:
                        hp=hp-0.02
                #print(hp)
                if k_r:
                    #print("Head Right")
                    Direction='Head right'
                    if hp > 0.95:
                        hp=0.95
                    else:
                        hp=hp+0.02
                if keyup==1:
                    Direction='neutral'
                    x=0
                    y=0
                UB2.SetServoPosition2(x)
                UB2.SetServoPosition1(y)
                UB2.SetServoPosition3(hp)
                #UB1.SetServoPosition1(pantilt)
                
class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(images)
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+20, y-96), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 1)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/obdisagegender/')
        self.sound_manager.play_sound("obdisagegender")
        self.animate(images)
        #robot.say("Age and Gender Detection obdisabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        #cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will obdisplay spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't obdisplay spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(images)
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            responses = ['neutral', 'happy', 'sad', 'surprise', 'anger']
            neutral,happy,sad,surprise,anger=0,0,0,0,0
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]
                    
                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 1)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, emotion_name, (bbox[0]+10, y-110), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 7)
                        cv2.putText(frame, emotion_name, (bbox[0], y-90), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 1)
                        if emotion_name=="neutral":
                            neutral=neutral +1
                        if emotion_name=="happy":
                            happy=happy +1
                        if emotion_name=="sad":
                            sad=sad +1
                        if emotion_name=="surprise":
                            surprise=surprise +1
                        if emotion_name=="anger":
                            anger=anger +1
                        #print(emotion_name)    
                        
                            #self.animate(animations[7])
                            #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    if neutral>20 or happy>20 or sad>20 or surprise>20 or anger>20:
                            max_response = max(zip(responses, (map(eval, responses))), key=lambda tuple: tuple[1])[0]
                            self.sound_manager.play_sound(max_response)
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/obdisemotional/')
        self.sound_manager.play_sound("obdisemotional")
        self.animate(images)
        #robot.say("Emotion Detection state obdisabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        #cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will obdisplay spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't obdisplay spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS

class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = ['person', 'cup',]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
    
    def obstacleAvoid(self, avoided):
        mainloop=True
        rx=0
        ry=0
        forward_count=0
        turn=''
        us1,us2,us3,us4,usr1,usr2,usr3,usr4=9999,9999,9999,9999,9999,9999,9999,9999
        while mainloop:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    mainloop = False
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        mainloop = False
                        break
            us1,us2,us3,us4=int(UB2.GetDistance1()),int(UB2.GetDistance2()),int(UB2.GetDistance3()),int(UB2.GetDistance4())
            usr1,usr2,usr3,usr4=int(UB3.GetDistance1()),int(UB3.GetDistance2()),int(UB3.GetDistance3()),int(UB3.GetDistance4())
            
            print (us1, "  ",us2, " ", us3, " ", us4," Front sensors")
            print (usr1, "  ",usr2, " ", usr3, " ", usr4," Rear sensors") 
            if us1<obdis or us2<obdis or us3<obdis or us4<obdis:  
                if us1<obdis and us2>obdis and us3>obdis and us4>obdis:#turn left,right sensor close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us1<obdis and us2<obdis and us3>obdis and us4>obdis:#turn left,right 2 sensors close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us1<obdis and us2<obdis and us3<obdis and us4>obdis:#turn left,right 3 sensors close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us1>obdis and us2<obdis and us3>obdis and us4>obdis:#turn left,right 3 sensors close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us4<obdis and us3>obdis and us2>obdis and us1>obdis:#turn right,left sensor close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us4<obdis and us3<obdis and us2>obdis and us1>obdis:#turn right,left 2 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us4<obdis and us3<obdis and us2<obdis and us1>obdis:#turn right,left 3 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us4>obdis and us3<obdis and us2>obdis and us1>obdis:#turn right,left 3 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us1<obdis and us2<obdis and us3<obdis and us4<obdis:#back out of corner
                    rx = 0.98
                    ry = 0.24
                    turn='back'
                if us2<obdis and us3<obdis and us1>obdis and us4>obdis:#front sensors triggered without other 2,turn right
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
            else:
                if turn=='left':# turn 1 last time before going forward
                    rx=0.1
                    ry=0.98
                    turn=''
                if turn=='right':# turn 1 last time before going forward
                    rx = -0.26
                    ry = -0.1
                    turn=''
                if turn=='back':# go back 1 last time 
                    rx = 0.98
                    ry = 0.24
                    turn=''
                UB2.SetServoPosition2(rx)
                UB2.SetServoPosition1(ry)
                time.sleep(0.2)
                rx = -0.95
                ry = -0.12# drive forward as NO objects have been detected
                #robot.say("forward")
                #x=-0.95
                #y=-0.08
                UB2.SetServoPosition2(rx)
                UB2.SetServoPosition1(ry)
                print("moving forward")
                time.sleep(0.2)
                avoided = True
                forward_count += 1
                if forward_count>6:
                    mainloop = False
                    forward_count=0

            UB2.SetServoPosition2(rx)
            UB2.SetServoPosition1(ry)
            time.sleep(0.15)
 
    def avoid(self, avoided):
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str=str(us1)
        us2_str=str(us2)
        us3_str=str(us3)
        us4_str=str(us4)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        print('........................... obstacle detected')
        print('attempting to avoid obstacle')
        #robot.say("Obstacle detected")
        images=load_images('/home/raspberrypi/MiniMax/Animations/inmyway/')
        self.sound_manager.play_sound("inmyway")
        preObsHeading, roll, pitch = sensor.euler #left euler goes down. Righr euler goes up.
        print("Before obstacle avoided heading",preObsHeading)
        self.obstacleAvoid(avoided)
        afterObsHeading, roll, pitch = sensor.euler
        print("after obstacle avoided heading",afterObsHeading)
        diff1 = int(preObsHeading - afterObsHeading)
        diff2 = abs(diff1)
        while diff2>1:
            afterObsHeading, roll, pitch = sensor.euler#aproximate same heading 
            diff1 = int(preObsHeading - afterObsHeading) #
            diff2 = abs(diff1) #
            if preObsHeading<=180:
                if afterObsHeading > preObsHeading and (preObsHeading+180) > afterObsHeading:
                    #turn Left or anti-clockwise to approach destination in shortest way
                    x=0.1
                    y=0.98
                else: # right clockwise 
                    x=-0.26
                    y=-1.0
            else:
                if preObsHeading > afterObsHeading and (afterObsHeading+180) > preObsHeading:
                    #turn Right
                    x=-0.26
                    y=-1.0
                else: # shortest way around a 360 degree circle to the pre obstacle angle
                    x=0.1
                    y=0.98
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        break
            UB2.SetServoPosition2(x)
            UB2.SetServoPosition1(y)
        print("back to original angle +- 1 degrees")
        UB2.SetServoPosition2(0)
        UB2.SetServoPosition1(0)
        #cv2.destroyAllWindows()
        r_person=0  
    
    def _exit():
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)

    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #THIS WAS USED, TO SAY "TO GIVE A BISCUIT TO"
            self.animate(images) #robot.config.animations[19])
        else:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #This is used if normal search person mode is activated.
            self.animate(images) #robot.config.animations[19])
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        
    def stop(self, robot):
        print('First stop check')
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str,us2_str,us3_str,us4_str=str(us1),str(us2),str(us3),str(us4)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        print('........................... Objective Reached')
        robot.say("Objective reached")
        cv2.destroyAllWindows()
        r_person=0
    
    def _main(self, robot):
        global z, current_z, current_z_str,us1_str,us2_str,us3_str,us4_str,rx,ry,us1,us2,us3,us4
        z, current_z, stop_distance,rx,ry = 9999,9999,700,0,0
        nnPath = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob'
        # MobilenetSSD label texts
        labelMap = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
                    "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
        syncNN = True
        pipeline = self._create_pipeline2(robot)
        cv2.namedWindow('rectified right', cv2.WINDOW_GUI_NORMAL)
        # Connect to device and start pipeline
        with dai.Device(pipeline) as device:
            # Output queues will be used to get the rgb frames and nn data from the outputs defined above
            previewQueue = device.getOutputQueue(name="right", maxSize=8, blocking=False)
            detectionNNQueue = device.getOutputQueue(name="detections", maxSize=8, blocking=False)
            depthQueue = device.getOutputQueue(name="depth", maxSize=8, blocking=False)
            rectifiedRight = None
            found_people=[{'objxcenter': 60,'z_depth':9999}]
            sorted_x={}
            detections = []
            startTime = time.monotonic()
            counter = 0
            fps = 0
            color = (255, 255, 255)
            us1,us2,us3,us4=9999,9999,9999,9999
            my_distances = [7777,8888,9999,5555]
            current_z=9999
            stop_threads = False
            t1 = threading.Thread(target = sensor_scan, args =(lambda : stop_threads, my_distances, ))
            t1.start()
            while True:
                inRectified = previewQueue.get()
                inDet = detectionNNQueue.get()
                counter += 1
                currentTime = time.monotonic()
                if (currentTime - startTime) > 1:
                    fps = counter / (currentTime - startTime)
                    counter = 0
                    startTime = currentTime
                rectifiedRight = inRectified.getCvFrame()
                detections = inDet.detections
                height = rectifiedRight.shape[0]
                width = rectifiedRight.shape[1]
                if current_z < stop_distance:
                    self.stop(robot)
                    stop_threads=True
                    t1.join()
                    return True
                if us1 < 450 or us2 < 450 or us3 < 450 or us4 <450: # forign object is close  
                    avoided=False
                    self.avoid(avoided)
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    # do everything needed when robot finds obstacle
                found_peeps=0    
                n_of_track_peeps=0
                for detection in detections:
                    label = labelMap[detection.label]
                    if label!='person':
                        continue
                    current_z=int(detection.spatialCoordinates.z)
                    roiData = detection.boundingBoxMapping
                    roi = roiData.roi
                    #roi = roi.denormalize(depthFrameColor.shape[1], depthFrameColor.shape[0])
                    #topLeft = roi.topLeft()
                    #bottomRight = roi.bottomRight()
                    #xmin,ymin,xmax,ymax, = int(topLeft.x),int(topLeft.y),int(bottomRight.x),int(bottomRight.y)
                    x1,x2,y1 = int(detection.xmin * width),int(detection.xmax * width),int(detection.ymin * height)
                    x_diff = (x2-x1)
                    obj_x_center = int(x1+(x_diff/2))
                    dict = {'objxcenter': obj_x_center, 'z_depth': current_z}
                    found_people.append(dict)
                    #cv2.putText(rectifiedRight, f"Z: {current_z} ", (x1 + 10, y1 + 80), cv2.FONT_HERSHEY_TRIPLEX, 0.75, color)
                    cv2.putText(rectifiedRight, f"c {obj_x_center} ", (x1 + 10, y1 + 120), cv2.FONT_HERSHEY_TRIPLEX, 0.75, color)
                sorted_x=min((x for x in found_people if x['z_depth'] != "9999"), key=lambda x:x['z_depth'], default=None)
                found_people=[{'objxcenter': 60,'z_depth':9999}]
                obj_x_center= sorted_x['objxcenter'] # get x co-ordinate of closest object
                current_z = sorted_x['z_depth']# get z-depth co-ordinate of closest object
                x_deviation = int(robot.config.ps_xres/2)-obj_x_center
                if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                    if current_z < stop_distance:
                        self.stop(robot)
                        stop_threads=True
                        t1.join()
                        return True 
                    if us1<450 or us2<450 or us3<450 or us4<450:#object is very close 
                        print('Second stop check')
                        avoided=False
                        self.avoid(avoided)
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                    else:
                        rx = -0.95
                        ry = -0.12 # move forward
                        UB2.SetServoPosition2(rx)
                        UB2.SetServoPosition1(ry)
                        #print("........................... moving robot FORWARD")
                else:
                    if (x_deviation>robot.config.ps_tolerance):
                        if x_deviation< robot.config.ps_far_boundry:
                            rx=-0.9
                            ry=0.1
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('........... turning left while moving forward' )
                        if x_deviation>=robot.config.ps_far_boundry:
                            rx=0.1
                            ry=0.98
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('..... turning left on the spot' )
                    elif ((x_deviation*-1)>robot.config.ps_tolerance):
                        if abs(x_deviation)<robot.config.ps_far_boundry:
                            rx=-0.9
                            ry=-0.3
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('............ turning right while moving forward' )
                        if abs(x_deviation)>=robot.config.ps_far_boundry:
                            rx=-0.26
                            ry=-1.0
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('..... turning right on the spot' )
                #cv2.rectangle(rectifiedRight, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)
                #cv2.putText(rectifiedRight, f"US: {(my_distances)} ", (1, 180), cv2.FONT_HERSHEY_TRIPLEX, 0.6, color)
                cv2.putText(rectifiedRight, "fps: {:.2f}".format(fps), (2, rectifiedRight.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(images) #robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    stop_threads=True
                    t1.join()
                    return False
                #rectifiedRight=cv2.resize(rectifiedRight,(600,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                
                cv2.imshow("rectified right", rectifiedRight)
       
    def _create_pipeline2(self, robot):
        nnPath = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob'
        # MobilenetSSD label texts
        labelMap = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
                    "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
        syncNN = True
        # Create pipeline
        pipeline = dai.Pipeline()
        # Define sources and outputs
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        imageManip = pipeline.create(dai.node.ImageManip)
        xoutManip = pipeline.create(dai.node.XLinkOut)
        nnOut = pipeline.create(dai.node.XLinkOut)
        xoutDepth = pipeline.create(dai.node.XLinkOut)
        xoutManip.setStreamName("right")
        nnOut.setStreamName("detections")
        xoutDepth.setStreamName("depth")
        # Properties
        imageManip.initialConfig.setResize(300, 300)
        # The NN model expects BGR input. By default ImageManip output type would be same as input (gray in this case)
        imageManip.initialConfig.setFrameType(dai.ImgFrame.Type.BGR888p)
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
        monoLeft.setFps(23)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
        monoRight.setFps(23)
        monoRight.setCamera("right")
        # StereoDepth
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        stereo.setSubpixel(False)
        # Define a neural network that will make predictions based on the source frames
        spatialDetectionNetwork.setConfidenceThreshold(0.6)
        spatialDetectionNetwork.setBlobPath(nnPath)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)
        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        imageManip.setKeepAspectRatio(False)
        imageManip.out.link(spatialDetectionNetwork.input)
        
        if syncNN:
            spatialDetectionNetwork.passthrough.link(xoutManip.input)
        else:
            imageManip.out.link(xoutManip.input)
        spatialDetectionNetwork.out.link(nnOut.input)
        stereo.rectifiedRight.link(imageManip.inputImage)
        #stereo.rectifiedRight.setPreviewKeepAspectRatio(False)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        spatialDetectionNetwork.passthroughDepth.link(xoutDepth.input)
        return pipeline
   
    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                images=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animate(images) #robot.config.animations[8])
                #robot.say(f'I think I found a {self.label}')
            else:
                images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images) #robot.config.animations[11])
                robot.say("what is going on")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images) #robot.config.animations[11])
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                images=load_images('/home/raspberrypi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(images) #robot.config.animations[9])
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)
                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)
        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))
            time.sleep(1)
        robot.say('The jelly beans are leaving now bye bye')
        #robot.animate(1)
        robot.write_serial('9z')  # make robot do a 180 degree turn
        robot.write_serial('2z')  # stop robot
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT

# === ./src/old/utils.py ===
from dataclasses import dataclass
from enum import Enum
import logging
from .animation_manager import AnimationManager, load_images
import numpy as np
import pygame

# All the configs for the robot will be stored in this RobotConfig dataclass. Dataclasses are an 
# easy way to store configuration in Python - expecially useful when we have lots of named parameters
# that we will be using all over our code. They a) make code much more readable, b) make the code
# much easier to modify, and c) make it much easier to pass in arguments from the commandline or
# other places if we want to e.g. modify a single parameter for a single run.
@dataclass
class RobotConfig:
    # serial port params
    # prefix: port_
    port_baudrate: int = 115200

    # navigation params
    # prefix: nav_
    nav_xres: int = 800
    nav_yres: int = 600
    nav_x_deviation: int = 0
    nav_ymax: int = 0

    # TTS params
    # prefix: tts_
    tts_rate: int = 145
    tts_voice: str = 'english+f4'
    tts_volume: float = 1

    # Biscuit Operation Mode params
    # prefic: biscuit_
    biscuit_wait_time: float = 4

    # Animation params
    # prefix: animate_
    animate_delay: float = 0.08

    # Person search params
    # prefix: ps_
    ps_nn_path: str = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_6shave.blob'
    ps_full_frame_tracking: bool = True
    ps_xres: int = 300
    ps_yres: int = 300
    ps_tolerance: int = 36
    ps_bottom_buffer: int = 2
    ps_give_biscuit_on_success: bool = True
    ps_far_boundry = 70


# Enum's are a way to create a type with statically set values. They're an idea stolen from more
# strongly-type languages (e.g. Java) but even though their implementation in Python is reasonably
# bare-bones, they do make help us make code more readable when we have lots of different states
# or types that an object could exist in.
class RobotState(Enum):
    # Special Cases
    EXIT = 0
    IDLE = 1

    # Operational Modes
    PERSON_SEARCH_GIVE_BISCUIT = 2
    PERSON_SEARCH_NO_GIVE_BISCUIT = 3
    CUP_SEARCH_GIVE_BISCUIT = 4
    CUP_SEARCH_NO_GIVE_BISCUIT = 5
    AGEGENDER = 6
    EMOTIONS = 7
    REMOTE_CONTROL = 8

lower_case_letters = [
    pygame.K_a,
    pygame.K_b,
    pygame.K_c,
    pygame.K_d,
    pygame.K_e,
    pygame.K_f,
    pygame.K_g,
    pygame.K_h,
    pygame.K_i,
    pygame.K_j,
    pygame.K_k,
    pygame.K_l,
    pygame.K_m,
    pygame.K_n,
    pygame.K_o,
    pygame.K_p,
    pygame.K_q,
    pygame.K_r,
    pygame.K_s,
    pygame.K_t,
    pygame.K_u,
    pygame.K_v,
    pygame.K_w,
    pygame.K_x,
    pygame.K_y,
    pygame.K_z,
]

# InputObject is currently only used to detect keystrokes - if more user input
# is required in the future, it should be added here.
class InputObject:
    def __init__(self) -> None:
        self.pressed_keys = self._get_pressed_keys() 
    
    def _get_pressed_keys(self):
        pressed = []
        #TODO: is this a bottleneck??
        for event in pygame.event.get():
            if event.type == pygame.KEYDOWN:
                if event.key in lower_case_letters:
                    pressed.append(pygame.key.name(event.key))
        
        # logging.debug(pressed)
        
        return pressed


def frame_norm(frame, bbox):
    normVals = np.full(len(bbox), frame.shape[0])
    normVals[::2] = frame.shape[1]
    return (np.clip(np.array(bbox), 0, 1) * normVals).astype(int)


# === ./src/old/avoid_obstacles.py ===
from .operational_mode import OperationalMode
from .utils import RobotConfig, RobotState
from .animation_manager import AnimationManager
from .sound_manager import SoundManger # LOL

class AvoidObstaclesMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def get_key(self) -> str:
        return 'x'
    
    def get_state(self) -> RobotState:
        return RobotState.AVOID_OBSTACLES



# === ./src/old/operational_modes_testing_2.py ===
from abc import ABC, abstractmethod
import time
import operator
import depthai as dai
import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images 
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
import UltraBorg_old

UB = UltraBorg_old.UltraBorg()      # Create a new UltraBorg object
UB.Init()
global stop_distance
stop_distance=220

class OperationalMode(ABC):

    @abstractmethod
    def run(self, robot):
        pass

    @abstractmethod
    def get_key(self) -> str:
        pass

    @abstractmethod
    def get_state(self) -> RobotState:
        pass

class AvoidObstaclesMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def get_key(self) -> str:
        return 'x'
    
    def get_state(self) -> RobotState:
        return RobotState.AVOID_OBSTACLES

class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(robot.config.animations[4])
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+20, y-96), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 1)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disagegender/')
        self.sound_manager.play_sound("disagegender")
        self.animate(robot.config.animations[5])
        #robot.say("Age and Gender Detection disabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(robot.config.animations[7])
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            responses = ['neutral', 'happy', 'sad', 'surprise', 'anger']
            neutral,happy,sad,surprise,anger=0,0,0,0,0
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]
                    
                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 1)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, emotion_name, (bbox[0]+10, y-110), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 7)
                        cv2.putText(frame, emotion_name, (bbox[0], y-90), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 1)
                        if emotion_name=="neutral":
                            neutral=neutral +1
                        if emotion_name=="happy":
                            happy=happy +1
                        if emotion_name=="sad":
                            sad=sad +1
                        if emotion_name=="surprise":
                            surprise=surprise +1
                        if emotion_name=="anger":
                            anger=anger +1
                        #print(emotion_name)    
                        
                            #self.animate(animations[7])
                            #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    if neutral>20 or happy>20 or sad>20 or surprise>20 or anger>20:
                            max_response = max(zip(responses, (map(eval, responses))), key=lambda tuple: tuple[1])[0]
                            self.sound_manager.play_sound(max_response)
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disemotional/')
        self.sound_manager.play_sound("disemotional")
        self.animate(robot.config.animations[6])
        #robot.say("Emotion Detection state disabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS


class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = [
            'person',
            'cup',
        ]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
            
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)  

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)
    
    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            #images=load_images('/home/raspberrypi/MiniMax/Results/searchterminated/')
            #self.sound_manager.play_sound("searchterminated")
            #self.animate(images)
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label} who like jellybeans")
        else:
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label}")
        #robot.animate(1)
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)

    def _main(self, robot):
        pipeline = self._create_pipeline2(robot)

        pin = '2z'
        r_person = 0
        z=1000
        stop_distance=600
        with dai.Device(pipeline) as device:
            preview = device.getOutputQueue("preview", 4, False)
            tracklets = device.getOutputQueue("tracklets", 4, False)
            startTime = time.monotonic()
            counter = 0
            fps = 0
            frame = None
            found_people = {}
            sorted_tracked= {}
            f_number=0
            
            while True:
                f_number=f_number+1
                us1 = int(UB.GetDistance1())
                us2 = int(UB.GetDistance2())
                us3 = int(UB.GetDistance3())
                if us1 == 0:
                    us1=99999
                if us2 == 0:
                    us2=99999
                if us3 == 0:
                    us3=99999
                #print(us1,us2,us3)
                imgFrame = preview.get()
                track = tracklets.get()
                counter+=1
                current_time = time.monotonic()
                if (current_time - startTime) > 1 :
                    fps = counter / (current_time - startTime)
                    counter = 0
                    startTime = current_time
                color = (255, 255, 255)
                frame = imgFrame.getCvFrame()
                frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST) # this line slows down everything!
                trackletsData = track.tracklets
                old_pin=pin
                # us = Ultrasonic sensor , z = distance to object measured by depth camera.
                if us1 < 400 or us2 < 400 or us2 < 400 or z < stop_distance:# object is very close.  
                    r_person=r_person+1
                    #print('Old pin '+old_pin +' new pin '+pin)
                    if (r_person>=2):
                        pin="2z"
                        robot.write_serial(pin)
                        print('Old pin '+old_pin +' new pin '+pin)
                        print('........................... reached objective')
                        print('waiting at objective reached')
                        robot.say("stop stop")
                        cv2.destroyAllWindows()
                        r_person=0
                        return True
                else:
                    r_person=0
                for t in trackletsData:
                    roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
                    x1 = int(roi.topLeft().x)
                    y1 = int(roi.topLeft().y)
                    x2 = int(roi.bottomRight().x)
                    y2 = int(roi.bottomRight().y)
                    z=int(t.spatialCoordinates.z)
                    xmin, ymin = x1, y1
                    xmax, ymax = x2, y2
                    x_diff, y_diff = (xmax-xmin), (ymax-ymin)
                    obj_x_center = int(xmin+(x_diff/2))
                    obj_y_center = int(ymin+(y_diff/2))
                    center_coordinates = (obj_x_center, obj_y_center)
                    # Put info in dict for detected / tracked object ONLY if detected object is being currently tracked. #
                    found_people[t.id] = (t.id, obj_x_center, obj_y_center, ymax, t.status.name, z)
                    if t.status.name!="TRACKED":
                        found_people.popitem() # remove all people who are not being currently tracked. (lost, deleted)
                    # found_people = {id: data for id, data in found_people.items() if t.status.name != "TRACKED"}
                    #radius = 2
                    #cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    #cv2.putText(frame, f"ID: {[t.id]}", (x1 + 10, y1 + 45), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.putText(frame, t.status.name, (x1 + 10, y1 + 70), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)
                    cv2.putText(frame, f"Z: {z} mm", (x1 + 10, 195), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
                    #cv2.circle(frame, center_coordinates, radius, color, 1)
                sorted_x = sorted(found_people.items(), key=operator.itemgetter(0))# key sort only items that are tracked, lowest key first #
                if len(sorted_x)<1:
                    robot.write_serial("5z") # no tracked people in current frame
                if len(sorted_x)>0: # found at least 1 tracked person in current frame
                    for q in sorted_x: # devide sorted_x dictionary into component variables
                        listy=(q[1])
                        index=listy[0]
                        status=listy[4] # status indicates if person is tracked
                        if status=="TRACKED":
                            sorted_tracked[index]=listy #creates a dictionary of ONLY tracked persons with co-ords
                if len(sorted_tracked)>0: # if the dictionary is not empty....
                    for w in sorted_tracked:
                        newlist=(sorted_tracked[w])
                    obj_x_center= newlist[1] # get x co-ordinate of lowest ID
                    obj_y_center= newlist[2]
                    current_z = newlist[5]# get y co-ordinate of lowest ID
                    ymax= newlist[3] # get ymax of lowest ID
                    x_deviation = (int(robot.config.ps_xres/2)-obj_x_center)
                            
                    # calculate the deviation from the center of the screen
                    if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                        if us1 < 400 or us2 < 400 or us2 < 400 or current_z < stop_distance:# object is very close 
                            print('Old pin '+old_pin +' new pin '+pin)
                            if (r_person>=2):
                                pin="2z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +' new pin '+pin)
                                print('........................... reached objective')
                                print('waiting at objective reached')
                                robot.say("stop stop stop")
                                cv2.destroyAllWindows()
                                return True
                        else:
                            #if old_pin != pin:
                            pin="1z"
                            robot.write_serial(pin)
                            #print('Old pin '+old_pin +' new pin '+pin)
                            #print("........................... moving robot FORWARD")
                            #robot.say("go")
                            r_person=0
                    else:
                        if (x_deviation>robot.config.ps_tolerance):
                            if x_deviation<175:
                                pin="3z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'........................... turning left' )
                                #robot.say("left")
                                r_person=0
                            if x_deviation>=175:
                                pin="7z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'....... turning left on the spot' )
                                #robot.say("spot left")
                                r_person=0
                        elif ((x_deviation*-1)>robot.config.ps_tolerance):
                            if abs(x_deviation)<175:
                                pin="4z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'........................... turning right' )
                                #robot.say("right")
                                r_person=0
                            if abs(x_deviation)>=175:
                                pin="8z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'....... turning right on the spot' )
                                #robot.say("spot right")
                                r_person=0
                cv2.putText(frame, "fps: {:.2f}".format(fps), (2, frame.shape[0] - 7), cv2.FONT_HERSHEY_TRIPLEX, 0.6, color)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    pin='5z'
                    robot.write_serial(pin)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    return False
                sorted_tracked.clear()
                #frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                cv2.imshow("tracker", frame)

    def _create_pipeline2(self, robot):
        # Create pipeline
        pipeline = dai.Pipeline()
        fullFrameTracking = False
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        objectTracker = pipeline.create(dai.node.ObjectTracker)

        xoutRgb = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)

        xoutRgb.setStreamName("preview")
        trackerOut.setStreamName("tracklets")

        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        camRgb.setInterleaved(False)
        #camRgb.setVideoSize(800, 800)
        #camRgb.setIspScale(16,27)
        #pipeline.setXLinkChunkSize(0)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        camRgb.setPreviewKeepAspectRatio(False)
        camRgb.setFps(9)
        camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoRight.setCamera("right")

        # setting node configs
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        # Align depth map to the perspective of RGB camera, on which inference is done
        stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
        stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())

        spatialDetectionNetwork.setBlobPath('/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')
        spatialDetectionNetwork.setConfidenceThreshold(0.6)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)

        objectTracker.setDetectionLabelsToTrack([15])  # track only person
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)

        camRgb.preview.link(spatialDetectionNetwork.input)
        objectTracker.passthroughTrackerFrame.link(xoutRgb.input)
        objectTracker.out.link(trackerOut.input)

        if fullFrameTracking:
            camRgb.setPreviewKeepAspectRatio(False)
            camRgb.video.link(objectTracker.inputTrackerFrame)
            objectTracker.inputTrackerFrame.setBlocking(False)
            # do not block the pipeline if it's too slow on full frame
            objectTracker.inputTrackerFrame.setQueueSize(1)
        else:
            spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)

        spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        spatialDetectionNetwork.out.link(objectTracker.inputDetections)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        return pipeline
   
    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animate(robot.config.animations[8])
                #robot.say(f'I think I found a {self.label}')
            else:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say("Search mode terminated")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(robot.config.animations[9])
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)

                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)

        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))

            time.sleep(1)

        robot.say('The jelly beans are leaving now bye bye')

        #robot.animate(1)

        robot.write_serial('9z')  # make robot do a 180 degree turn
        robot.write_serial('2z')  # stop robot
        
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT
        



# === ./src/old/stop_thread.py ===
import threading
import time

def run(stop):
    while True:
        print('r')
        if stop():
                break
                
def main():
        stop_threads = False
        t1 = threading.Thread(target = run, args =(lambda : stop_threads, ))
        t1.start()
        time.sleep(0.1)
        stop_threads = True
        t1.join()
        print('thread killed')
main()


# === ./src/old/operational_modes_back_up1.py ===
import time
import operator
import depthai as dai
import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images 
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
from .operational_mode import OperationalMode
import UltraBorg_old

UB = UltraBorg_old.UltraBorg()      # Create a new UltraBorg object
UB.Init()
global stop_distance
stop_distance=220

class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(robot.config.animations[4])
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+20, y-96), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 1)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disagegender/')
        self.sound_manager.play_sound("disagegender")
        self.animate(robot.config.animations[5])
        #robot.say("Age and Gender Detection disabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(robot.config.animations[7])
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            responses = ['neutral', 'happy', 'sad', 'surprise', 'anger']
            neutral,happy,sad,surprise,anger=0,0,0,0,0
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]
                    
                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 1)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, emotion_name, (bbox[0]+10, y-110), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 7)
                        cv2.putText(frame, emotion_name, (bbox[0], y-90), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 1)
                        if emotion_name=="neutral":
                            neutral=neutral +1
                        if emotion_name=="happy":
                            happy=happy +1
                        if emotion_name=="sad":
                            sad=sad +1
                        if emotion_name=="surprise":
                            surprise=surprise +1
                        if emotion_name=="anger":
                            anger=anger +1
                        #print(emotion_name)    
                        
                            #self.animate(animations[7])
                            #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    if neutral>20 or happy>20 or sad>20 or surprise>20 or anger>20:
                            max_response = max(zip(responses, (map(eval, responses))), key=lambda tuple: tuple[1])[0]
                            self.sound_manager.play_sound(max_response)
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disemotional/')
        self.sound_manager.play_sound("disemotional")
        self.animate(robot.config.animations[6])
        #robot.say("Emotion Detection state disabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS


class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = [
            'person',
            'cup',
        ]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
            
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)  

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)
    
    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            #images=load_images('/home/raspberrypi/MiniMax/Results/searchterminated/')
            #self.sound_manager.play_sound("searchterminated")
            #self.animate(images)
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label} who like jellybeans")
        else:
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label}")
        #robot.animate(1)
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)

    def _main(self, robot):
        pipeline = self._create_pipeline2(robot)

        pin = '2z'
        r_person = 0
        z=1000
        stop_distance=600
        with dai.Device(pipeline) as device:
            preview = device.getOutputQueue("preview", 4, False)
            tracklets = device.getOutputQueue("tracklets", 4, False)
            startTime = time.monotonic()
            counter = 0
            fps = 0
            frame = None
            found_people = {}
            sorted_tracked= {}
            f_number=0
            
            while True:
                f_number=f_number+1
                us1 = int(UB.GetDistance1())
                us2 = int(UB.GetDistance2())
                us3 = int(UB.GetDistance3())
                if us1 == 0:
                    us1=99999
                if us2 == 0:
                    us2=99999
                if us3 == 0:
                    us3=99999
                #print(us1,us2,us3)
                imgFrame = preview.get()
                track = tracklets.get()
                counter+=1
                current_time = time.monotonic()
                if (current_time - startTime) > 1 :
                    fps = counter / (current_time - startTime)
                    counter = 0
                    startTime = current_time
                color = (255, 255, 255)
                frame = imgFrame.getCvFrame()
                #frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST) # this line slows down everything!
                trackletsData = track.tracklets
                old_pin=pin
                # us = Ultrasonic sensor , z = distance to object measured by depth camera.
                if us1 < 400 or us2 < 400 or us2 < 400 or z < stop_distance:# object is very close.  
                    r_person=r_person+1
                    #print('Old pin '+old_pin +' new pin '+pin)
                    if (r_person>=2):
                        pin="2z"
                        robot.write_serial(pin)
                        print('Old pin '+old_pin +' new pin '+pin)
                        print('........................... reached objective')
                        print('waiting at objective reached')
                        robot.say("stop stop")
                        cv2.destroyAllWindows()
                        r_person=0
                        return True
                else:
                    r_person=0
                for t in trackletsData:
                    roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
                    x1 = int(roi.topLeft().x)
                    y1 = int(roi.topLeft().y)
                    x2 = int(roi.bottomRight().x)
                    y2 = int(roi.bottomRight().y)
                    z=int(t.spatialCoordinates.z)
                    xmin, ymin = x1, y1
                    xmax, ymax = x2, y2
                    x_diff, y_diff = (xmax-xmin), (ymax-ymin)
                    obj_x_center = int(xmin+(x_diff/2))
                    obj_y_center = int(ymin+(y_diff/2))
                    center_coordinates = (obj_x_center, obj_y_center)
                    # Put info in dict for detected / tracked object ONLY if detected object is being currently tracked. #
                    found_people[t.id] = (t.id, obj_x_center, obj_y_center, t.status.name, z)
                    if t.status.name!="TRACKED":
                        found_people.popitem() # remove all people who are not being currently tracked. (lost, deleted)
                    # found_people = {id: data for id, data in found_people.items() if t.status.name != "TRACKED"}
                    #radius = 2
                    #cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    #cv2.putText(frame, f"ID: {[t.id]}", (x1 + 10, y1 + 45), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.putText(frame, t.status.name, (x1 + 10, y1 + 70), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)
                    cv2.putText(frame, f"Z: {z} mm", (x1 + 10, 195), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
                    #cv2.circle(frame, center_coordinates, radius, color, 1)
                sorted_x = sorted(found_people.items(), key=operator.itemgetter(0))# key sort only items that are tracked, lowest key first #
                if len(sorted_x)<1:
                    robot.write_serial("5z") # no tracked people in current frame
                if len(sorted_x)>0: # found at least 1 tracked person in current frame
                    for q in sorted_x: # devide sorted_x dictionary into component variables
                        listy=(q[1])
                        index=listy[0]  # the tracking ID of the found person
                        status=listy[3] # status indicates if person is tracked
                        if status=="TRACKED":
                            sorted_tracked[index]=listy #creates a dictionary of ONLY tracked persons with co-ords
                if len(sorted_tracked)>0: # if the dictionary is not empty....
                    for w in sorted_tracked:
                        newlist=(sorted_tracked[w])
                    obj_x_center= newlist[1] # get x co-ordinate of lowest ID
                    obj_y_center= newlist[2] # get x co-ordinate of lowest ID
                    current_z = newlist[4] # get z co-ordinate of lowest ID
                    #ymax= newlist[3] # get ymax of lowest ID
                    x_deviation = (int(robot.config.ps_xres/2)-obj_x_center)
                            
                    # calculate the deviation from the center of the screen
                    if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                        if us1 < 400 or us2 < 400 or us2 < 400 or current_z < stop_distance:# object is very close 
                            print('Old pin '+old_pin +' new pin '+pin)
                            if (r_person>=2):
                                pin="2z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +' new pin '+pin)
                                print('........................... reached objective')
                                print('waiting at objective reached')
                                robot.say("stop stop stop")
                                cv2.destroyAllWindows()
                                return True
                        else:
                            #if old_pin != pin:
                            pin="1z"
                            robot.write_serial(pin)
                            #print('Old pin '+old_pin +' new pin '+pin)
                            #print("........................... moving robot FORWARD")
                            #robot.say("go")
                            r_person=0
                    else:
                        if (x_deviation>robot.config.ps_tolerance):
                            if x_deviation< robot.config.ps_far_boundry:
                                pin="3z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'........................... turning left' )
                                #robot.say("left")
                                r_person=0
                            if x_deviation>=robot.config.ps_far_boundry:
                                pin="7z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'....... turning left on the spot' )
                                #robot.say("spot left")
                                r_person=0
                        elif ((x_deviation*-1)>robot.config.ps_tolerance):
                            if abs(x_deviation)<robot.config.ps_far_boundry:
                                pin="4z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'........................... turning right' )
                                #robot.say("right")
                                r_person=0
                            if abs(x_deviation)>=robot.config.ps_far_boundry:
                                pin="8z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'....... turning right on the spot' )
                                #robot.say("spot right")
                                r_person=0
                cv2.putText(frame, "fps: {:.2f}".format(fps), (2, frame.shape[0] - 7), cv2.FONT_HERSHEY_TRIPLEX, 0.6, color)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    pin='5z'
                    robot.write_serial(pin)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    return False
                sorted_tracked.clear()
                frame=cv2.resize(frame,(600,400),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                cv2.imshow("tracker", frame)

    def _create_pipeline2(self, robot):
        # Create pipeline
        pipeline = dai.Pipeline()
        fullFrameTracking = False
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        objectTracker = pipeline.create(dai.node.ObjectTracker)

        xoutRgb = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)

        xoutRgb.setStreamName("preview")
        trackerOut.setStreamName("tracklets")

        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        camRgb.setInterleaved(False)
        #camRgb.setVideoSize(800, 800)
        #camRgb.setIspScale(16,27)
        #pipeline.setXLinkChunkSize(0)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        camRgb.setPreviewKeepAspectRatio(False)
        camRgb.setFps(15)
        
        camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoRight.setCamera("right")

        # setting node configs
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        # Align depth map to the perspective of RGB camera, on which inference is done
        stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
        stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())

        spatialDetectionNetwork.setBlobPath('/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')
        spatialDetectionNetwork.setConfidenceThreshold(0.6)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)

        objectTracker.setDetectionLabelsToTrack([15])  # track only person
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        camRgb.preview.link(spatialDetectionNetwork.input)

        objectTracker.passthroughTrackerFrame.link(xoutRgb.input)
        objectTracker.out.link(trackerOut.input)

       # if fullFrameTracking:
           # camRgb.setPreviewKeepAspectRatio(False)
           # camRgb.video.link(objectTracker.inputTrackerFrame)
            #objectTracker.inputTrackerFrame.setBlocking(False)
            # do not block the pipeline if it's too slow on full frame
            #objectTracker.inputTrackerFrame.setQueueSize(1)
        #else:
        spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)
        objectTracker.inputTrackerFrame.setBlocking(False) #testing can remove
        spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        spatialDetectionNetwork.out.link(objectTracker.inputDetections)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        return pipeline
   
    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animate(robot.config.animations[8])
                #robot.say(f'I think I found a {self.label}')
            else:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say("Search mode terminated")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(robot.config.animations[9])
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)

                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)

        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))

            time.sleep(1)

        robot.say('The jelly beans are leaving now bye bye')

        #robot.animate(1)

        robot.write_serial('9z')  # make robot do a 180 degree turn
        robot.write_serial('2z')  # stop robot
        
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT
        



# === ./src/old/operational_modes_testing.py ===
from abc import ABC, abstractmethod
import time
import operator
import depthai as dai
import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images 
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
import UltraBorg_old

UB = UltraBorg_old.UltraBorg()      # Create a new UltraBorg object
UB.Init()
global stop_distance
stop_distance=220

class OperationalMode(ABC):

    @abstractmethod
    def run(self, robot):
        pass

    @abstractmethod
    def get_key(self) -> str:
        pass

    @abstractmethod
    def get_state(self) -> RobotState:
        pass

class AvoidObstaclesMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def get_key(self) -> str:
        return 'x'
    
    def get_state(self) -> RobotState:
        return RobotState.AVOID_OBSTACLES

class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(robot.config.animations[4])
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+20, y-96), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 1)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disagegender/')
        self.sound_manager.play_sound("disagegender")
        self.animate(robot.config.animations[5])
        #robot.say("Age and Gender Detection disabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(robot.config.animations[7])
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            responses = ['neutral', 'happy', 'sad', 'surprise', 'anger']
            neutral,happy,sad,surprise,anger=0,0,0,0,0
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]
                    
                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 1)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, emotion_name, (bbox[0]+10, y-110), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 7)
                        cv2.putText(frame, emotion_name, (bbox[0], y-90), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 1)
                        if emotion_name=="neutral":
                            neutral=neutral +1
                        if emotion_name=="happy":
                            happy=happy +1
                        if emotion_name=="sad":
                            sad=sad +1
                        if emotion_name=="surprise":
                            surprise=surprise +1
                        if emotion_name=="anger":
                            anger=anger +1
                        #print(emotion_name)    
                        
                            #self.animate(animations[7])
                            #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    if neutral>20 or happy>20 or sad>20 or surprise>20 or anger>20:
                            max_response = max(zip(responses, (map(eval, responses))), key=lambda tuple: tuple[1])[0]
                            self.sound_manager.play_sound(max_response)
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disemotional/')
        self.sound_manager.play_sound("disemotional")
        self.animate(robot.config.animations[6])
        #robot.say("Emotion Detection state disabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS


class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = [
            'person',
            'cup',
        ]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
            
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)  

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)
    
    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            #images=load_images('/home/raspberrypi/MiniMax/Results/searchterminated/')
            #self.sound_manager.play_sound("searchterminated")
            #self.animate(images)
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label} who like jellybeans")
        else:
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label}")
        #robot.animate(1)
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)

    def _main(self, robot):
        pipeline = self._create_pipeline2(robot)

        pin = '2z'
        r_person = 0
        z=1000
        stop_distance=600
        with dai.Device(pipeline) as device:
            preview = device.getOutputQueue("preview", 4, False)
            tracklets = device.getOutputQueue("tracklets", 4, False)
            startTime = time.monotonic()
            counter = 0
            fps = 0
            frame = None
            found_people = {}
            sorted_tracked= {}
            f_number=0
            
            while True:
                f_number=f_number+1
                us1 = int(UB.GetDistance1())
                us2 = int(UB.GetDistance2())
                us3 = int(UB.GetDistance3())
                if us1 == 0:
                    us1=99999
                if us2 == 0:
                    us2=99999
                if us3 == 0:
                    us3=99999
                #print(us1,us2,us3)
                imgFrame = preview.get()
                track = tracklets.get()
                counter+=1
                current_time = time.monotonic()
                if (current_time - startTime) > 1 :
                    fps = counter / (current_time - startTime)
                    counter = 0
                    startTime = current_time
                color = (255, 255, 255)
                frame = imgFrame.getCvFrame()
                frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST) # this line slows down everything!
                trackletsData = track.tracklets
                old_pin=pin
                # us = Ultrasonic sensor , z = distance to object measured by depth camera.
                if us1 < 400 or us2 < 400 or us2 < 400 or z < stop_distance:# object is very close.  
                    r_person=r_person+1
                    #print('Old pin '+old_pin +' new pin '+pin)
                    if (r_person>=2):
                        pin="2z"
                        robot.write_serial(pin)
                        print('Old pin '+old_pin +' new pin '+pin)
                        print('........................... reached objective')
                        print('waiting at objective reached')
                        robot.say("stop stop")
                        cv2.destroyAllWindows()
                        r_person=0
                        return True
                else:
                    r_person=0
                for t in trackletsData:
                    roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
                    x1 = int(roi.topLeft().x)
                    y1 = int(roi.topLeft().y)
                    x2 = int(roi.bottomRight().x)
                    y2 = int(roi.bottomRight().y)
                    z=int(t.spatialCoordinates.z)
                    xmin, ymin = x1, y1
                    xmax, ymax = x2, y2
                    x_diff, y_diff = (xmax-xmin), (ymax-ymin)
                    obj_x_center = int(xmin+(x_diff/2))
                    obj_y_center = int(ymin+(y_diff/2))
                    center_coordinates = (obj_x_center, obj_y_center)
                    # Put info in dict for detected / tracked object ONLY if detected object is being currently tracked. #
                    found_people[t.id] = (t.id, obj_x_center, obj_y_center, ymax, t.status.name, z)
                    if t.status.name!="TRACKED":
                        found_people.popitem() # remove all people who are not being currently tracked. (lost, deleted)
                    # found_people = {id: data for id, data in found_people.items() if t.status.name != "TRACKED"}
                    #radius = 2
                    #cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    #cv2.putText(frame, f"ID: {[t.id]}", (x1 + 10, y1 + 45), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.putText(frame, t.status.name, (x1 + 10, y1 + 70), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)
                    cv2.putText(frame, f"Z: {z} mm", (x1 + 10, 195), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
                    #cv2.circle(frame, center_coordinates, radius, color, 1)
                sorted_x = sorted(found_people.items(), key=operator.itemgetter(0))# key sort only items that are tracked, lowest key first #
                if len(sorted_x)<1:
                    robot.write_serial("5z") # no tracked people in current frame
                if len(sorted_x)>0: # found at least 1 tracked person in current frame
                    for q in sorted_x: # devide sorted_x dictionary into component variables
                        listy=(q[1])
                        index=listy[0]
                        status=listy[4] # status indicates if person is tracked
                        if status=="TRACKED":
                            sorted_tracked[index]=listy #creates a dictionary of ONLY tracked persons with co-ords
                if len(sorted_tracked)>0: # if the dictionary is not empty....
                    for w in sorted_tracked:
                        newlist=(sorted_tracked[w])
                    obj_x_center= newlist[1] # get x co-ordinate of lowest ID
                    obj_y_center= newlist[2]
                    current_z = newlist[5]# get y co-ordinate of lowest ID
                    ymax= newlist[3] # get ymax of lowest ID
                    x_deviation = (int(robot.config.ps_xres/2)-obj_x_center)
                            
                    # calculate the deviation from the center of the screen
                    if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                        if us1 < 400 or us2 < 400 or us2 < 400 or current_z < stop_distance:# object is very close 
                            print('Old pin '+old_pin +' new pin '+pin)
                            if (r_person>=2):
                                pin="2z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +' new pin '+pin)
                                print('........................... reached objective')
                                print('waiting at objective reached')
                                robot.say("stop stop stop")
                                cv2.destroyAllWindows()
                                return True
                        else:
                            #if old_pin != pin:
                            pin="1z"
                            robot.write_serial(pin)
                            #print('Old pin '+old_pin +' new pin '+pin)
                            #print("........................... moving robot FORWARD")
                            #robot.say("go")
                            r_person=0
                    else:
                        if (x_deviation>robot.config.ps_tolerance):
                            if x_deviation<175:
                                pin="3z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'........................... turning left' )
                                #robot.say("left")
                                r_person=0
                            if x_deviation>=175:
                                pin="7z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'....... turning left on the spot' )
                                #robot.say("spot left")
                                r_person=0
                        elif ((x_deviation*-1)>robot.config.ps_tolerance):
                            if abs(x_deviation)<175:
                                pin="4z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'........................... turning right' )
                                #robot.say("right")
                                r_person=0
                            if abs(x_deviation)>=175:
                                pin="8z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'....... turning right on the spot' )
                                #robot.say("spot right")
                                r_person=0
                cv2.putText(frame, "fps: {:.2f}".format(fps), (2, frame.shape[0] - 7), cv2.FONT_HERSHEY_TRIPLEX, 0.6, color)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    pin='5z'
                    robot.write_serial(pin)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    return False
                sorted_tracked.clear()
                #frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                cv2.imshow("tracker", frame)

    def _create_pipeline2(self, robot):
        # Create pipeline
        pipeline = dai.Pipeline()
        fullFrameTracking = False
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        objectTracker = pipeline.create(dai.node.ObjectTracker)

        xoutRgb = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)

        xoutRgb.setStreamName("preview")
        trackerOut.setStreamName("tracklets")

        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        camRgb.setInterleaved(False)
        #camRgb.setVideoSize(800, 800)
        #camRgb.setIspScale(16,27)
        #pipeline.setXLinkChunkSize(0)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        camRgb.setPreviewKeepAspectRatio(False)
        camRgb.setFps(9)
        camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoRight.setCamera("right")

        # setting node configs
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        # Align depth map to the perspective of RGB camera, on which inference is done
        stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
        stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())

        spatialDetectionNetwork.setBlobPath('/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')
        spatialDetectionNetwork.setConfidenceThreshold(0.6)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)

        objectTracker.setDetectionLabelsToTrack([15])  # track only person
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)

        camRgb.preview.link(spatialDetectionNetwork.input)
        objectTracker.passthroughTrackerFrame.link(xoutRgb.input)
        objectTracker.out.link(trackerOut.input)

        if fullFrameTracking:
            camRgb.setPreviewKeepAspectRatio(False)
            camRgb.video.link(objectTracker.inputTrackerFrame)
            objectTracker.inputTrackerFrame.setBlocking(False)
            # do not block the pipeline if it's too slow on full frame
            objectTracker.inputTrackerFrame.setQueueSize(1)
        else:
            spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)

        spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        spatialDetectionNetwork.out.link(objectTracker.inputDetections)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        return pipeline
   
    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animate(robot.config.animations[8])
                #robot.say(f'I think I found a {self.label}')
            else:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say("Search mode terminated")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(robot.config.animations[9])
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)

                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)

        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))

            time.sleep(1)

        robot.say('The jelly beans are leaving now bye bye')

        #robot.animate(1)

        robot.write_serial('9z')  # make robot do a 180 degree turn
        robot.write_serial('2z')  # stop robot
        
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT
        



# === ./src/old/animate_jpg.py ===
import os
import pygame
pygame.init()

SIZE = WIDTH, HEIGHT = 1280, 800
BACKGROUND_COLOR = pygame.Color('black')
FPS = 30
screen = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
clock = pygame.time.Clock()

def load_images(path):
    """
    Loads all images in directory. The directory must only contain images.
    Args:
        path: The relative or absolute path to the directory to load images from.
    Returns:
        List of images.
    """
    images = []
    for file_name in os.listdir(path):
        image = pygame.image.load(path + os.sep + file_name).convert()
        images.append(image)
    return images

class AnimatedSprite(pygame.sprite.Sprite):

    def __init__(self, position, images):
        """
        Animated sprite object.
        Args:
            position: x, y coordinate on the screen to place the AnimatedSprite.
            images: Images to use in the animation.
        """
        super(AnimatedSprite, self).__init__()
        size = (1280, 800)  # This should match the size of the images.
        self.rect = pygame.Rect(position, size)
        self.images = images
        self.images_right = images
        self.images_left = [pygame.transform.flip(image, True, False) for image in images]  # Flipping every image.
        self.index = 0
        self.image = images[self.index]  # 'image' is the current image of the animation.

        self.velocity = pygame.math.Vector2(0, 0)

        self.animation_time = 0.1
        self.current_time = 0

        self.animation_frames = 6
        self.current_frame = 0

    def update_time_dependent(self, dt):
        """
        Updates the image of Sprite approximately every 0.1 second.
        Args:
            dt: Time elapsed between each frame.
        """
        if self.velocity.x > 0:  # Use the right images if sprite is moving right.
            self.images = self.images_right
        elif self.velocity.x < 0:
            self.images = self.images_left

        self.current_time += dt
        if self.current_time >= self.animation_time:
            self.current_time = 0
            self.index = (self.index + 1) % len(self.images)
            self.image = self.images[self.index]

        self.rect.move_ip(*self.velocity)

    def update_frame_dependent(self):
        """
        Updates the image of Sprite every 6 frame (approximately every 0.1 second if frame rate is 60).
        """
        if self.velocity.x > 0:  # Use the right images if sprite is moving right.
            self.images = self.images_right
        elif self.velocity.x < 0:
            self.images = self.images_left

        self.current_frame += 1
        if self.current_frame >= self.animation_frames:
            self.current_frame = 0
            self.index = (self.index + 1) % len(self.images)
            self.image = self.images[self.index]

        self.rect.move_ip(*self.velocity)

    def update(self, dt):
        """This is the method that's being called when 'all_sprites.update(dt)' is called."""
        # Switch between the two update methods by commenting/uncommenting.
        self.update_time_dependent(dt)
        # self.update_frame_dependent()
def main():
    images = load_images(path='/home/pi/MiniMax/AnimatedFace')  # Make sure to provide the relative or full path to the images directory.
    player = AnimatedSprite(position=(20, 20), images=images)
    all_sprites = pygame.sprite.Group(player)  # Creates a sprite group and adds 'player' to it.

    running = True
    while running:
        dt = clock.tick(FPS) / 1000  # Amount of seconds between each loop.
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False
            elif event.type == pygame.KEYDOWN:
                if event.key == pygame.K_q:
                    running = False
                    pygame.quit()
                    break
                if event.key == pygame.K_RIGHT:
                    player.velocity.x = 4
                elif event.key == pygame.K_LEFT:
                    player.velocity.x = -4
                elif event.key == pygame.K_DOWN:
                    player.velocity.y = 4
                elif event.key == pygame.K_UP:
                    player.velocity.y = -4
            elif event.type == pygame.KEYUP:
                if event.key == pygame.K_RIGHT or event.key == pygame.K_LEFT:
                    player.velocity.x = 0
                elif event.key == pygame.K_DOWN or event.key == pygame.K_UP:
                    player.velocity.y = 0
        all_sprites.update(dt)  # Calls the 'update' method on all sprites in the list (currently just the player).

        screen.fill(BACKGROUND_COLOR)
        all_sprites.draw(screen)
        pygame.display.update()

if __name__ == '__main__':
    main()
    pygame.quit()

# === ./src/old/animation_manager.py ===
import time
import os
import pygame
from .sound_manager import SoundManger
from natsort import natsorted
import numpy as np

global resx, resy, yoffset, imp, initPygameOnce, animations
resx, resy, yoffset, initPygameOnce = 1280, 1024, 70, 0
animations = []

pygame.init()
# display = pygame.display.set_mode((1024, 768))
# animations.append(pygame.image.load('/home/raspberrypi/MiniMax/Animations/powerdown/out_001.jpg'))
display = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)


# display = pygame.display.set_mode((1024, 1024))
def load_images(path):
    store = []
    q = 0
    for file_name in os.listdir(path):
        temp = file_name
        q = q + 1
        if q >= 2:
            store.append(temp)
            q = 0
    store = natsorted(store)
    images = []
    image_array = []
    for xyz in store:
        pic = path + xyz
        image_array.append(pic)
    for names in image_array:
        imagine = pygame.image.load(names)
        images.append(imagine)
    store.clear()
    imagine = []
    image_array.clear()
    return images


class AnimationManager:
    def __init__(self, config) -> None:
        self.display = display  # Reuse display from the call to pygame.display.set_mode() on import
        # self.display = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
        # self.background = pygame.Surface(self.display.get_size())
        # self.clock = pygame.time.Clock()
        self.animate_delay = config.animate_delay
        # self.sound_manager = SoundManger
        self.channel = pygame.mixer.Channel(1)

    def animate(self, images):
        closedmouth = pygame.image.load(
            "/home/raspberrypi/MiniMax/Animations/advised/outputFile_001.jpg"
        )
        loops = 0
        run = True
        length = len(images)
        # print('starting the loop',length)
        while loops < length:
            # frt=show_fps()
            # print(loops)
            intloops = int(loops)
            image = images[intloops]
            self.display.blit(image, (0, 0))
            pygame.display.flip()
            loops = loops + 0.5
        # time.sleep(0.05)
        self.display.blit(closedmouth, (0, 0))  # end animation with closed mouth .jpg
        pygame.display.flip()


# === ./src/old/operational_modes2.py ===
from abc import ABC, abstractmethod
import time
import operator
import depthai as dai
import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig

class OperationalMode(ABC):
    @abstractmethod
    def run(self, robot):
        pass

    @abstractmethod
    def get_key(self) -> str:
        pass

    @abstractmethod
    def get_state(self) -> RobotState:
        pass

class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        images=load_images('/home/pi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(images)
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 2)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/pi/MiniMax/Animations/disagegender/')
        self.sound_manager.play_sound("disagegender")
        self.animate(images)
        #robot.say("Age and Gender Detection disabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        images=load_images('/home/pi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(images)
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        cv2.putText(frame, emotion_name, (bbox[0]+20, y-100), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        cv2.putText(frame, emotion_name, (bbox[0]+20, y-100), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/pi/MiniMax/Animations/disemotional/')
        self.sound_manager.play_sound("disemotional")
        self.animate(images)
        #robot.say("Emotion Detection state disabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS


class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = [
            'person',
            'cup',
        ]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode

        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
            
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)  

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)
    
    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            #images=load_images('/home/pi/MiniMax/Results/searchterminated/')
            #self.sound_manager.play_sound("searchterminated")
            #self.animate(images)
            robot.say(f"Search mode enabled. Searching for {self.label} who like jellybeans")
        else:
            robot.say(f"Search mode enabled. Searching for {self.label}")
        #robot.animate(1)
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)

    def _main(self, robot):
        pipeline = self._create_pipeline2(robot)

        pin = '2z'
        r_person = 0

        with dai.Device(pipeline) as device:
            preview = device.getOutputQueue("preview", 4, False)
            tracklets = device.getOutputQueue("tracklets", 4, False)
            startTime = time.monotonic()
            counter = 0
            fps = 0
            frame = None
            found_people = {}
            sorted_tracked= {}
            while True:
                imgFrame = preview.get()
                track = tracklets.get()
                counter+=1
                current_time = time.monotonic()
                if (current_time - startTime) > 1 :
                    fps = counter / (current_time - startTime)
                    counter = 0
                    startTime = current_time
                color = (255, 255, 255)
                frame = imgFrame.getCvFrame()
                frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                trackletsData = track.tracklets
                old_pin=pin
                for t in trackletsData:
                    roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
                    x1 = int(roi.topLeft().x)
                    y1 = int(roi.topLeft().y)
                    x2 = int(roi.bottomRight().x)
                    y2 = int(roi.bottomRight().y)
                    xmin, ymin = x1, y1
                    xmax, ymax = x2, y2
                    x_diff, y_diff = (xmax-xmin), (ymax-ymin)
                    obj_x_center = int(xmin+(x_diff/2))
                    obj_y_center = int(ymin+(y_diff/2))
                    center_coordinates = (obj_x_center, obj_y_center)
                    # Put info in dictionary for detected / tracked object ONLY if detected object is being successfully tracked #
                    found_people[t.id] = (t.id, obj_x_center, obj_y_center, ymax, t.status.name)
                    if t.status.name!="TRACKED":
                        found_people.popitem()
                    radius = 2
                    #cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.putText(frame, f"ID: {[t.id]}", (x1 + 10, y1 + 45), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.putText(frame, t.status.name, (x1 + 10, y1 + 70), cv2.FONT_HERSHEY_TRIPLEX, 0.7, color)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)
                    cv2.circle(frame, center_coordinates, radius, color, 1)
                sorted_x = sorted(found_people.items(), key=operator.itemgetter(0))# key sort only items that are tracked, lowest key first #
                if len(sorted_x)<1:
                    robot.write_serial("5z")
                    
                if len(sorted_x)>0:
                    for q in sorted_x:
                        listy=(q[1])
                        index=listy[0]
                        status=listy[4]
                        if status=="TRACKED":
                            sorted_tracked[index]=listy
                if len(sorted_tracked)>0: # if the dictionary is not empty....
                    for w in sorted_tracked:
                        newlist=(sorted_tracked[w])
                    obj_x_center= newlist[1] # get x co-ordinate of lowest ID
                    obj_y_center= newlist[2] # get y co-ordinate of lowest ID
                    ymax= newlist[3] # get ymax of lowest ID
                    x_deviation = (int(robot.config.ps_xres/2)-obj_x_center)
                    # calculate the deviation from the center of the screen
                    if(abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                        if abs(ymax<(0+robot.config.ps_bottom_buffer)):     # is object close to the bottom of the frame?
                            robot.write_serial('2z')
                            r_person=r_person+1
                            print('Old pin '+old_pin +' new pin '+pin)
                            if (r_person>3):
                                pin="2z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +' new pin '+pin)
                                print('........................... reached objective')
                                print('waiting at objective reached')
                                robot.say("stop")
                                cv2.destroyAllWindows()
                                return True
                        else:
                            #if old_pin != pin:
                            pin="1z"
                            robot.write_serial(pin)
                            print('Old pin '+old_pin +' new pin '+pin)
                            print("........................... moving robot FORWARD")
                            robot.say("go")
                            r_person=0
                    else:
                        if (x_deviation>robot.config.ps_tolerance):
                            if old_pin !="3z" and x_deviation<175:
                                pin="3z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +'  '+pin+'........................... turning left' )
                                robot.say("left")
                                r_person=0
                            if old_pin !="7z" and x_deviation>=175:
                                pin="7z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +'  '+pin+'....... turning left on the spot' )
                                robot.say("spot left")
                                r_person=0
                        elif ((x_deviation*-1)>robot.config.ps_tolerance):
                            if old_pin !="4z" and abs(x_deviation)<175:
                                pin="4z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +'  '+pin+'........................... turning right' )
                                robot.say("right")
                                r_person=0
                            if old_pin !="8z" and abs(x_deviation)>=175:
                                pin="8z"
                                robot.write_serial(pin)
                                print('Old pin '+old_pin +'  '+pin+'....... turning right on the spot' )
                                robot.say("spot right")
                                r_person=0
                cv2.putText(frame, "fps: {:.2f}".format(fps), (2, frame.shape[0] - 7), cv2.FONT_HERSHEY_TRIPLEX, 0.6, color)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    print('menu waiting for keyboard input')
                    pin='5z'
                    robot.write_serial(pin)
                    return False
                sorted_tracked.clear()
                cv2.imshow("tracker", frame)

    def _create_pipeline2(self, robot):
        # Create pipeline
        pipeline = dai.Pipeline()
        
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        
        detectionNetwork = pipeline.create(dai.node.MobileNetDetectionNetwork)
        objectTracker = pipeline.create(dai.node.ObjectTracker)
        xlinkOut = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)
        xlinkOut.setStreamName("preview")
        trackerOut.setStreamName("tracklets")
        
        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P) #THE_1080_P
        camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        camRgb.setInterleaved(False)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        camRgb.setFps(15)
        
        # testing MobileNet DetectionNetwork
        detectionNetwork.setBlobPath(robot.config.ps_nn_path)
        detectionNetwork.setConfidenceThreshold(self.model_threshold)
        detectionNetwork.input.setBlocking(False)
        objectTracker.setDetectionLabelsToTrack([self.model_id])  # track only person
        
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        #objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)
        #objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        camRgb.preview.link(detectionNetwork.input)
        objectTracker.passthroughTrackerFrame.link(xlinkOut.input)
        if robot.config.ps_full_frame_tracking:
            camRgb.setPreviewKeepAspectRatio(False)
            camRgb.video.link(objectTracker.inputTrackerFrame)
        else:
            detectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)
        detectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        detectionNetwork.out.link(objectTracker.inputDetections)
        objectTracker.out.link(trackerOut.input)
        return pipeline

    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                images=load_images('/home/pi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animator.animate(images)
                #robot.say(f'I think I found a {self.label}')
            else:
                images=load_images('/home/pi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animator.animate(images)
                #robot.say("Search mode terminated")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                images=load_images('/home/pi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images)
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                images=load_images('/home/pi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(images)
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)

                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)

        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))

            time.sleep(1)

        robot.say('The jelly beans are leaving now bye bye')

        #robot.animate(1)

        robot.write_serial('9z')  #do a 180 degree turn
        robot.write_serial('2z')
        
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT
        



# === ./src/old/222Obstacle_avoid101.py ===

#!/usr/bin/env python
# coding: Latin-1

# Load library functions we want
import us3traBorg
UB = us3traBorg.us3traBorg()
UB.Init()
import time
import os
import sys
import pygame
import ThunderBorg

# Re-direct ous2 output to standard error, we need to ignore standard out to hide some nasty print statements from pygame
sys.stdout = sys.stderr

# Setup the ThunderBorg
TB = ThunderBorg.ThunderBorg()
#TB.i2cAddress = 0x15                  # Uncomment and change the value if you have changed the board address
TB.Init()


# Power settings
voltageIn = 4 * 3.7                     # Total battery voltage to the ThunderBorg
voltageOut = 12.0 * 0.95                                                                                                  # Maximum motor voltage, we limit it to 95% to allow the RPi to get uninterrupted power


# Setup the power limits
if voltageOut > voltageIn:
    maxPower = 1.0
else:
    maxPower = voltageOut / float(voltageIn)

# .................................................................................................#
us1 = UB.GetDistance1()
us2 = UB.GetDistance2()
us3 = UB.GetDistance3()
us4 = UB.GetDistance4()
dr = 0
dl = 0
pygame.init()
window = pygame.display.set_mode((100, 100))
pygame.display.set_caption("Pygame Demonstration")

mainloop=True
while mainloop:
    
    us1 = int(UB.GetDistance1())
    us2 = int(UB.GetDistance2())
    us3 = int(UB.GetDistance3())
    us4 = UB.GetDistance4()
    print (us1, "  ",us2, " ", us3, " ", us4)
    dis = 300
    if us1 < 350 and us1 !=0 or us2 < dis and us2 !=0 or us3 < dis and us3 !=0:
        if us3 < dis and us2 > dis: # tus2n right cause Left sensor is close to object 
            dr = -0.4
            dl = 0.4
        elif us2 < dis and us3 > dis: # tus2n left cause Right sensor is close to object
            dr = 0.4
            dl = -0.4
        elif us2 < dis and us3 < dis: # back out of corner
            dr = -0.6
            dl = -0.6
        elif us1 < dis and us1 !=0: # if front sensor is triggered without the other 2 then just tus2n right
            dr = -0.4
            dl = 0.4
    else:                     # just keep driving forward as NO objects have been detected
        dr = .7
        dl = .7
    
    for event in pygame.event.get():

        if event.type == pygame.QUIT:
            mainloop = False
        
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_w:
                mainloop = False
        
    

    TB.SetMotor1(dl  * maxPower)
    TB.SetMotor2(dr * maxPower)
    time.sleep(0.3)
# .................................................................................................#

# Disable all drives
TB.MotorsOff()
TB.SetCommsFailsafe(False)
TB.SetLedShowBattery(False)
TB.SetLeds(0,0,0)
print ("")

pygame.quit()


# === ./src/old/mono-no-depthmap-FAST.py ===
import cv2
import depthai as dai
import numpy as np
import time

nnPath = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob'
# MobilenetSSD label texts
labelMap = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
            "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
syncNN = True
# Create pipeline
pipeline = dai.Pipeline()
# Define sources and outputs
monoLeft = pipeline.create(dai.node.MonoCamera)
monoRight = pipeline.create(dai.node.MonoCamera)
stereo = pipeline.create(dai.node.StereoDepth)

spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
imageManip = pipeline.create(dai.node.ImageManip)
imageManip.setKeepAspectRatio(False)
xoutManip = pipeline.create(dai.node.XLinkOut)
nnOut = pipeline.create(dai.node.XLinkOut)
xoutDepth = pipeline.create(dai.node.XLinkOut)
xoutManip.setStreamName("right")
nnOut.setStreamName("detections")
xoutDepth.setStreamName("depth")
# Properties
imageManip.initialConfig.setResize(300, 300)
# The NN model expects BGR input. By default ImageManip output type would be same as input (gray in this case)
imageManip.initialConfig.setFrameType(dai.ImgFrame.Type.BGR888p)
monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
monoLeft.setFps(24)
monoLeft.setCamera("left")
monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
monoRight.setFps(24)
monoRight.setCamera("right")
# StereoDepth
stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
stereo.setSubpixel(False)
# Define a neural network that will make predictions based on the source frames
spatialDetectionNetwork.setConfidenceThreshold(0.7)
spatialDetectionNetwork.setBlobPath(nnPath)
spatialDetectionNetwork.input.setBlocking(False)
spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
spatialDetectionNetwork.setDepthLowerThreshold(100)
spatialDetectionNetwork.setDepthUpperThreshold(5000)
# Linking
monoLeft.out.link(stereo.left)
monoRight.out.link(stereo.right)
imageManip.out.link(spatialDetectionNetwork.input)
if syncNN:
    spatialDetectionNetwork.passthrough.link(xoutManip.input)
else:
    imageManip.out.link(xoutManip.input)
spatialDetectionNetwork.out.link(nnOut.input)
stereo.rectifiedRight.link(imageManip.inputImage)
#stereo.rectifiedRight.setPreviewKeepAspectRatio(False)
stereo.depth.link(spatialDetectionNetwork.inputDepth)
spatialDetectionNetwork.passthroughDepth.link(xoutDepth.input)
# Connect to device and start pipeline
with dai.Device(pipeline) as device:
    # Output queues will be used to get the rgb frames and nn data from the outputs defined above
    previewQueue = device.getOutputQueue(name="right", maxSize=8, blocking=False)
    detectionNNQueue = device.getOutputQueue(name="detections", maxSize=8, blocking=False)
    depthQueue = device.getOutputQueue(name="depth", maxSize=8, blocking=False)
    rectifiedRight = None
    detections = []
    startTime = time.monotonic()
    counter = 0
    fps = 0
    color = (255, 255, 255)
    while True:
        inRectified = previewQueue.get()
        inDet = detectionNNQueue.get()
        counter += 1
        currentTime = time.monotonic()
        if (currentTime - startTime) > 1:
            fps = counter / (currentTime - startTime)
            counter = 0
            startTime = currentTime
        rectifiedRight = inRectified.getCvFrame()
        detections = inDet.detections
        height = rectifiedRight.shape[0]
        width = rectifiedRight.shape[1]
        for detection in detections:
            label = labelMap[detection.label]
            #print(label)
            if label!='person':
                continue
            roiData = detection.boundingBoxMapping
            roi = roiData.roi
            #roi = roi.denormalize(depthFrameColor.shape[1], depthFrameColor.shape[0])
            #topLeft = roi.topLeft()
            #bottomRight = roi.bottomRight()
            #xmin = int(topLeft.x)
            #print(xmin)
            #ymin = int(topLeft.y)
            #xmax = int(bottomRight.x)
            #print(xmax)
            #ymax = int(bottomRight.y)
            x1 = int(detection.xmin * width)
            x2 = int(detection.xmax * width)
            y1 = int(detection.ymin * height)
            y2 = int(detection.ymax * height)
            cv2.putText(rectifiedRight, f"Z: {int(detection.spatialCoordinates.z)} mm", (x1 + 10, y1 + 80), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            cv2.putText(rectifiedRight, f"C: {int(x1+(x2-x2)/2)} " ,(x1 + 6, y1 + 130), cv2.FONT_HERSHEY_TRIPLEX, 0.8, color)
        cv2.putText(rectifiedRight, "NN fps: {:.2f}".format(fps), (2, rectifiedRight.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color)
        cv2.imshow("rectified right", rectifiedRight)
        if cv2.waitKey(1) == ord('q'):
            break


# === ./src/old/spatial_object_tracker.py ===
#!/usr/bin/env python3

import cv2
import depthai as dai
import numpy as np
import time

labelMap = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
            "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]

nnPath = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob'
fullFrameTracking = False
# Create pipeline
pipeline = dai.Pipeline()
# Define sources and outputs
camRgb = pipeline.create(dai.node.ColorCamera)
#camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
monoLeft = pipeline.create(dai.node.MonoCamera)
monoRight = pipeline.create(dai.node.MonoCamera)
stereo = pipeline.create(dai.node.StereoDepth)
objectTracker = pipeline.create(dai.node.ObjectTracker)

xoutRgb = pipeline.create(dai.node.XLinkOut)
trackerOut = pipeline.create(dai.node.XLinkOut)

xoutRgb.setStreamName("preview")
trackerOut.setStreamName("tracklets")

# Properties
camRgb.setPreviewSize(300, 300)
camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_2024X1520)
camRgb.setInterleaved(False)
camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
camRgb.setPreviewKeepAspectRatio(False)
#camRgb.setFps(20)
monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
monoLeft.setCamera("left")
monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
monoRight.setCamera("right")

# setting node configs
stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
# Align depth map to the perspective of RGB camera, on which inference is done
stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())
stereo.setSubpixel(False)

spatialDetectionNetwork.setBlobPath(nnPath)
spatialDetectionNetwork.setConfidenceThreshold(0.6)
spatialDetectionNetwork.input.setBlocking(False)
spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
spatialDetectionNetwork.setDepthLowerThreshold(100)
spatialDetectionNetwork.setDepthUpperThreshold(5000)

objectTracker.setDetectionLabelsToTrack([15])  # track only person
# possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
# take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

# Linking
monoLeft.out.link(stereo.left)
monoRight.out.link(stereo.right)

camRgb.preview.link(spatialDetectionNetwork.input)
objectTracker.passthroughTrackerFrame.link(xoutRgb.input)
objectTracker.out.link(trackerOut.input)

if fullFrameTracking:
    camRgb.setPreviewKeepAspectRatio(False)
    camRgb.video.link(objectTracker.inputTrackerFrame)
    objectTracker.inputTrackerFrame.setBlocking(False)
    # do not block the pipeline if it's too slow on full frame
    objectTracker.inputTrackerFrame.setQueueSize(4)
else:
    spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)

spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
spatialDetectionNetwork.out.link(objectTracker.inputDetections)
stereo.depth.link(spatialDetectionNetwork.inputDepth)

# Connect to device and start pipeline
with dai.Device(pipeline) as device:

    preview = device.getOutputQueue("preview", 4, False)
    tracklets = device.getOutputQueue("tracklets", 4, False)

    startTime = time.monotonic()
    counter = 0
    fps = 0
    color = (255, 255, 255)

    while(True):
        imgFrame = preview.get()
        track = tracklets.get()

        counter+=1
        current_time = time.monotonic()
        if (current_time - startTime) > 1 :
            fps = counter / (current_time - startTime)
            counter = 0
            startTime = current_time

        frame = imgFrame.getCvFrame()
        trackletsData = track.tracklets
        for t in trackletsData:
            roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
            x1 = int(roi.topLeft().x)
            y1 = int(roi.topLeft().y)
            x2 = int(roi.bottomRight().x)
            y2 = int(roi.bottomRight().y)
            #try:
            label = labelMap[t.label]
            #except:
                #label = t.label
            z=int(t.spatialCoordinates.z)
            #cv2.putText(frame, str(label), (x1 + 10, 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            #cv2.putText(frame, f"ID: {[t.id]}", (x1 + 10, y1 + 35), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            #cv2.putText(frame, t.status.name, (x1 + 10, 60), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            #cv2.rectangle(frame, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)

            #cv2.putText(frame, f"X: {int(t.spatialCoordinates.x)} mm", (x1 + 10, y1 + 65), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            #cv2.putText(frame, f"Y: {int(t.spatialCoordinates.y)} mm", (x1 + 10, y1 + 80), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            cv2.putText(frame, f"Z: {z} mm", (x1 + 10, 195), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)

        cv2.putText(frame, "NN fps: {:.2f}".format(fps), (2, 265), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color)
        #frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
        cv2.imshow("tracker", frame)

        if cv2.waitKey(1) == ord('q'):
            break


# === ./src/old/animation_manager_old.py ===
import time

import pygame
global resx, resy, yoffset
resx=1280
resy=800
yoffset=70

def load_images(path):
    images = []
    for file_name in os.listdir(path):
        image = pygame.image.load(path + os.sep + file_name).convert()
        images.append(image)
    return images

class AnimationManager:
    def __init__(self, config) -> None:
        pygame.init()

        self.display = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
        self.background = pygame.Surface(self.display.get_size())
        self.clock = pygame.time.Clock()
        self.animate_delay = config.animate_delay
    
    def animate(self, ticks: int):
        for _ in range(ticks):
            clock = pygame.time.Clock()
            value = 0
            run = True
            for loops in range(0,len(images)):      
                clock.tick(11) 
                if value >= len(images):
                    value = 0
                    break
                image = images[value]
                y = 20
                # Displaying the image in our game window
                window.blit(image, (x, y))
                # Updating the display surface
                pygame.display.update()
                # Filling the window with black color
                window.fill((0, 0, 0))
                # Increasing the value of value variable by 1
                # after every iteration
                value += 4

    def draw_robot_big_mouth(self):
        color=(200,50,50)
        black=(0,0,0)
        
        size1=(int(resx*0.33), int(resy*0.5)+yoffset, int(resx*0.35), int(resy*0.16)+yoffset)
        size2=(int(resx*0.345), int(resy*0.52)+yoffset, int(resx*0.32), int(resy*0.12)+yoffset)
        pygame.draw.ellipse(self.display, color, size1)
        pygame.draw.ellipse(self.display, black, size2)

        pygame.draw.rect(self.display, (50,50,150), pygame.Rect(resx*0.1, resy*0.04+yoffset, resx*0.8, resy*0.70+yoffset),  8) #head outline

        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.3), int(resy*0.22)+yoffset], 80, 4) #outer eye
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.7), int(resy*0.22)+yoffset], 80, 4)
        pygame.draw.circle(self.display,(5,5,200),[int(resx*0.3), int(resy*0.22)+yoffset], 70, 0) #blue eye
        pygame.draw.circle(self.display,(5,5,200),[int(resx*0.7), int(resy*0.22)+yoffset], 70, 0)
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.49), int(resy*0.37)+yoffset], 9, 5) #nothrals
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.51), int(resy*0.37)+yoffset], 9, 5)
        pygame.draw.circle(self.display,(0,0,0),[int(resx*0.3), int(resy*0.22)+yoffset], 29, 0) #iner eye
        pygame.draw.circle(self.display,(0,0,0),[int(resx*0.7), int(resy*0.22)+yoffset], 29, 0)
    
    def draw_robot_small_mouth(self):
        color=(200,50,50)
        black=(0,0,0)
        
        size1 = (int(resx*0.3), int(resy*0.5)+yoffset, int(resx*0.4), int(resy*0.067)+yoffset)
        size2 = (int(resx*0.35), int(resy*0.51+yoffset), int(resx*0.3), int(resy*0.05)+yoffset)

        pygame.draw.ellipse(self.display, color, size1)
        pygame.draw.ellipse(self.display, black, size2)
        pygame.draw.rect(self.display, (50,50,150), pygame.Rect(resx*0.1, resy*0.04+yoffset, resx*0.8, resy*0.70+yoffset),  8) # head outline

        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.3), int(resy*0.22)+yoffset], 80, 4) #outer eye
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.7), int(resy*0.22)+yoffset], 80, 4)
        pygame.draw.circle(self.display,(5,5,200),[int(resx*0.3), int(resy*0.22)+yoffset], 50, 0) #blue eye
        pygame.draw.circle(self.display,(5,5,200),[int(resx*0.7), int(resy*0.22)+yoffset], 50, 0)
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.49), int(resy*0.37)+yoffset], 9, 5) #nothrals
        pygame.draw.circle(self.display,(255,0,0),[int(resx*0.51), int(resy*0.37)+yoffset], 9, 5)
        pygame.draw.circle(self.display,(0,0,0),[int(resx*0.3), int(resy*0.22)+yoffset], 22, 0) #inner eye
        pygame.draw.circle(self.display,(0,0,0),[int(resx*0.7), int(resy*0.22)+yoffset], 22, 0)

# === ./src/old/operational_modes_workingfeb24-2024.py ===
import time
import operator
import depthai as dai
import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images 
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
from .operational_mode import OperationalMode
import UltraBorg_old

UB = UltraBorg_old.UltraBorg()      # Create a new UltraBorg object
UB.Init()
global stop_distance, colour
colour=(255,255,255)
stop_distance=220

class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(robot.config.animations[4])
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+20, y-96), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 1)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disagegender/')
        self.sound_manager.play_sound("disagegender")
        self.animate(robot.config.animations[5])
        #robot.say("Age and Gender Detection disabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(robot.config.animations[7])
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            responses = ['neutral', 'happy', 'sad', 'surprise', 'anger']
            neutral,happy,sad,surprise,anger=0,0,0,0,0
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]
                    
                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 1)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, emotion_name, (bbox[0]+10, y-110), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 7)
                        cv2.putText(frame, emotion_name, (bbox[0], y-90), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 1)
                        if emotion_name=="neutral":
                            neutral=neutral +1
                        if emotion_name=="happy":
                            happy=happy +1
                        if emotion_name=="sad":
                            sad=sad +1
                        if emotion_name=="surprise":
                            surprise=surprise +1
                        if emotion_name=="anger":
                            anger=anger +1
                        #print(emotion_name)    
                        
                            #self.animate(animations[7])
                            #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    if neutral>20 or happy>20 or sad>20 or surprise>20 or anger>20:
                            max_response = max(zip(responses, (map(eval, responses))), key=lambda tuple: tuple[1])[0]
                            self.sound_manager.play_sound(max_response)
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disemotional/')
        self.sound_manager.play_sound("disemotional")
        self.animate(robot.config.animations[6])
        #robot.say("Emotion Detection state disabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS
    
class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = [
            'person',
            'cup',
        ]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
            
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)  

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)
    
    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            #images=load_images('/home/raspberrypi/MiniMax/Results/searchterminated/')
            #self.sound_manager.play_sound("searchterminated")
            #self.animate(images)
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label} who like jellybeans")
        else:
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label}")
        #robot.animate(1)
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)
    
    def stop(self, robot):
        #print('Old pin '+old_pin +' new pin '+pin)
        print('First stop check')
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str=str(us1)
        us2_str=str(us2)
        us3_str=str(us3)
        #r_person_str=str(r_person)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str)
        #print('r_person = '+r_person_str)
        pin="2z"
        robot.write_serial(pin)
        #print('Old pin '+old_pin +' new pin '+pin)
        print('........................... reached objective')
        print('waiting at objective reached')
        robot.say("First stop stop")
        cv2.destroyAllWindows()
        r_person=0    

    def _main(self, robot):
        global old_pin, pin, z, r_person, current_z, current_z_str,us1,us2,us3,us1_str,us2_str,us3_str,r_person_str
        pipeline = self._create_pipeline2(robot)
        pin = '2z'
        r_person = 0
        z=1000
        current_z=1000
        stop_distance=500
        #found_people[0] = {'tid': 99999, 'objxcenter': 0, 'objycenter': 0, 'status': 'TRACKED', 'z_depth': 1000}
        with dai.Device(pipeline) as device:
            preview = device.getOutputQueue("preview", 4, False)
            tracklets = device.getOutputQueue("tracklets", 4, False)
            startTime = time.monotonic()
            counter = 0
            fps = 0
            frame = None
            found_people=[{}]
            sorted_x={}
            while True:
                us1 = int(UB.GetDistance1())
                us2 = int(UB.GetDistance2())
                us3 = int(UB.GetDistance3())
                if us1 == 0:
                    us1=99999
                if us2 == 0:
                    us2=99999
                if us3 == 0:
                    us3=99999
                #print(us1,us2,us3)
                imgFrame = preview.get()
                track = tracklets.get()
                counter+=1
                current_time = time.monotonic()
                if (current_time - startTime) > 1 :
                    fps = counter / (current_time - startTime)
                    counter = 0
                    startTime = current_time
                frame = imgFrame.getCvFrame()
                #frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST) # this line slows down everything!
                trackletsData = track.tracklets
                old_pin=pin
                # us = Ultrasonic sensor , z = distance to object measured by depth camera.
                if us1 < 400 or us2 < 400 or us2 < 400 or z < stop_distance:# object is very close.  
                    r_person=r_person+1
                    #print('Old pin '+old_pin +' new pin '+pin)
                    if r_person==2:
                        self.stop(robot) # do everything needed when robot finds person or obstacle
                        return True
                else:
                    r_person=0
                n_of_track_peeps=0
                for t in trackletsData:
                    roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
                    x1,y1 = int(roi.topLeft().x), int(roi.topLeft().y)
                    x2,y2 = int(roi.bottomRight().x), int(roi.bottomRight().y)
                    z=int(t.spatialCoordinates.z)
                    if t.status.name=='TRACKED':
                        n_of_track_peeps+=1
                    xmin, ymin = x1, y1
                    xmax, ymax = x2, y2
                    x_diff, y_diff = (xmax-xmin), (ymax-ymin)
                    obj_x_center = int(xmin+(x_diff/2))
                    obj_y_center = int(ymin+(y_diff/2))
                    center_coordinates = (obj_x_center, obj_y_center)
                    #dictionary to be updated
                    dict = {'tid': t.id, 'objxcenter': obj_x_center, 'objycenter': obj_y_center, 'status': t.status.name, 'z_depth': z}
                    found_people[n_of_track_peeps-1].update(dict)
                    #cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.7, colour)
                    #cv2.putText(frame, f"ID: {[t.id]}", (x1 + 10, y1 + 45), cv2.FONT_HERSHEY_TRIPLEX, 0.7, colour)
                    #cv2.putText(frame, t.status.name, (x1 + 10, y1 + 70), cv2.FONT_HERSHEY_TRIPLEX, 0.7, colour)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), colour, cv2.FONT_HERSHEY_SIMPLEX)
                    cv2.putText(frame, f"Z: {z} mm", (x1 + 10, 195), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
                if n_of_track_peeps==0:
                    robot.write_serial("5z") # no tracked people in current frame
                    #print('no peeps')
                else:
                    sorted_x=min((x for x in found_people if x['status'] == "TRACKED"), key=lambda x:x['tid'])
                    obj_x_center= sorted_x['objxcenter'] # get x co-ordinate of lowest ID
                    obj_y_center= sorted_x['objycenter'] # get y co-ordinate of lowest ID
                    current_z = sorted_x['z_depth'] # get z-depth co-ordinate of lowest ID
                    x_deviation = (int(robot.config.ps_xres/2)-obj_x_center)
                    # calculate the deviation from the center of the screen
                    if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                        if us1 < 400 or us2 < 400 or us2 < 400 or current_z < stop_distance:# object is very close 
                            print('Second stop check')
                            self.stop(robot)
                            return True
                        else:
                            pin="1z"
                            robot.write_serial(pin)
                            #print("........................... moving robot FORWARD")
                            r_person=0
                    else:
                        if (x_deviation>robot.config.ps_tolerance):
                            if x_deviation< robot.config.ps_far_boundry:
                                pin="3z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'........................... turning left' )
                                r_person=0
                            if x_deviation>=robot.config.ps_far_boundry:
                                pin="7z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'....... turning left on the spot' )
                                r_person=0
                        elif ((x_deviation*-1)>robot.config.ps_tolerance):
                            if abs(x_deviation)<robot.config.ps_far_boundry:
                                pin="4z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'........................... turning right' )
                                r_person=0
                            if abs(x_deviation)>=robot.config.ps_far_boundry:
                                pin="8z"
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'....... turning right on the spot' )
                                r_person=0
                cv2.putText(frame, "fps: {:.2f}".format(fps), (2, frame.shape[0] - 7), cv2.FONT_HERSHEY_TRIPLEX, 0.6, colour)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    pin='5z'
                    robot.write_serial(pin)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    return False
                frame=cv2.resize(frame,(600,400),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                cv2.imshow("tracker", frame)

    def _create_pipeline2(self, robot):
        # Create pipeline
        pipeline = dai.Pipeline()
        fullFrameTracking = False
       
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        #spatialDetectionNetwork.setNumInferenceThreads(1) # By default 2 threads are used
        #spatialDetectionNetwork.setNumNCEPerInferenceThread(2) # By default, 1 NCE is used per thread

        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        objectTracker = pipeline.create(dai.node.ObjectTracker)

        xoutRgb = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)

        xoutRgb.setStreamName("preview")
        trackerOut.setStreamName("tracklets")

        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        camRgb.setInterleaved(False)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        camRgb.setFps(11)
        
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoRight.setCamera("right")

        # setting node configs
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        # Align depth map to the perspective of RGB camera, on which inference is done
        stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
        #stereo.setSubpixel(True) #testing
        stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())

        spatialDetectionNetwork.setBlobPath('/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')
        spatialDetectionNetwork.setConfidenceThreshold(0.7)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)

        objectTracker.setDetectionLabelsToTrack([15])  # track only person
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        camRgb.preview.link(spatialDetectionNetwork.input)
        
        objectTracker.passthroughTrackerFrame.link(xoutRgb.input)
        objectTracker.out.link(trackerOut.input)
        
        #if fullFrameTracking:
           # camRgb.setPreviewKeepAspectRatio(False)
            #camRgb.video.link(objectTracker.inputTrackerFrame)
           # objectTracker.inputTrackerFrame.setBlocking(False)
            # do not block the pipeline if it's too slow on full frame
            #objectTracker.inputTrackerFrame.setQueueSize(2)
        #else:
        spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)
        objectTracker.inputTrackerFrame.setBlocking(False)# testing
        spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        spatialDetectionNetwork.out.link(objectTracker.inputDetections)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        return pipeline
   
    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animate(robot.config.animations[8])
                #robot.say(f'I think I found a {self.label}')
            else:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say("Search mode terminated")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(robot.config.animations[9])
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)

                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)
        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))
            time.sleep(1)
        robot.say('The jelly beans are leaving now bye bye')
        #robot.animate(1)
        robot.write_serial('9z')  # make robot do a 180 degree turn
        robot.write_serial('2z')  # stop robot
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT
        



# === ./src/old/sound_manager.py ===
from typing import List, Optional
import pygame
import pathlib

# I don't think sounds in pygame block, so should be fine to run in the main process
# If the sounds do block, then will probably want to split this all into a seperate process
_default_filepaths = [
    "DefaultSayings/greetings.wav",
    "DefaultSayings/powerdown.wav",
    "DefaultSayings/advised.wav",
    "DefaultSayings/agegender.wav",
    "DefaultSayings/disagegender.wav",
    "DefaultSayings/disemotional.wav",
    "DefaultSayings/emotional.wav",
    "DefaultSayings/found-person.wav",
    "DefaultSayings/objective.wav",
    "DefaultSayings/searchmode-off.wav",
    "DefaultSayings/searchterminated.wav",
    "DefaultSayings/better.wav",
    "DefaultSayings/cautionroguerobots.wav",
    "DefaultSayings/chess.wav",
    "DefaultSayings/compute.wav",
    "DefaultSayings/cross.wav",
    "DefaultSayings/danger.wav",
    "DefaultSayings/dangerwillrobinson.wav",
    "DefaultSayings/directive.wav",
    "DefaultSayings/humans.wav",
    "DefaultSayings/malfunction2.wav",
    "DefaultSayings/nicesoftware2.wav",
    "DefaultSayings/no5alive.wav",
    "DefaultSayings/program.wav",
    "DefaultSayings/selfdestruct.wav",
    "DefaultSayings/shallweplayagame.wav",
    "DefaultSayings/comewithme.wav",
    "DefaultSayings/gosomewhere2.wav",
    "DefaultSayings/hairybaby.wav",
    "DefaultSayings/lowbattery.wav",
    "DefaultSayings/robotnotoffended.wav",
    "DefaultSayings/satisfiedwithmycare.wav",
    "DefaultSayings/waitbeforeswim.wav",
    "DefaultSayings/silly.wav",
    "DefaultSayings/stare.wav",
    "DefaultSayings/world.wav",
    "DefaultSayings/anger.wav",
    "DefaultSayings/backwards.wav",
    "DefaultSayings/cautionmovingbackwards.wav",
    "DefaultSayings/cautionmovingforward.wav",
    "DefaultSayings/dizzy.wav",
    "DefaultSayings/forwards.wav",
    "DefaultSayings/found-you.wav",
    "DefaultSayings/happy.wav",
    "DefaultSayings/helpme.wav",
    "DefaultSayings/inmyway.wav",
    "DefaultSayings/left.wav",
    "DefaultSayings/movingback.wav",
    "DefaultSayings/movingforward.wav",
    "DefaultSayings/movingleft.wav",
    "DefaultSayings/movingright.wav",
    "DefaultSayings/neutral.wav",
    "DefaultSayings/right.wav",
    "DefaultSayings/sad.wav",
    "DefaultSayings/search-on.wav",
    "DefaultSayings/search-person.wav",
    "DefaultSayings/seeyou.wav",
    "DefaultSayings/surprise.wav",
    "Sound/2.mp3",
    "Sound/Powerup/Powerup_chirp2.mp3",
    "Sound/Randombeeps/Questioning_computer_chirp.mp3",
    "Sound/Randombeeps/Double_beep2.mp3",
    "Sound/Powerdown/Long_power_down.mp3",
    "Sound/Radarscanning/Radar_bleep_chirp.mp3",
    "Sound/Radarscanning/Radar_scanning_chirp.mp3",
    "Sound/celebrate1.mp3",
    "Sound/Randombeeps/Da_de_la.mp3",
]


class SoundManger:
    def __init__(self, config, file_paths: Optional[List[str]] = None) -> None:
        pygame.mixer.pre_init(48000, -16, 8, 8192)  # initialise music,sound mixer
        pygame.mixer.init()
        if file_paths is not None:
            file_paths += _default_filepaths
        else:
            file_paths = _default_filepaths
        self.sounds = {pathlib.Path(f).stem: pygame.mixer.Sound(f) for f in file_paths}
        self.channel = pygame.mixer.Channel(1)

    def play_sound(self, sound):
        self.channel.play(self.sounds[sound])
        # self.sounds[sound].play()


# === ./src/old/operational_modes_new_code-30-july-24.py ===
import time
import operator
import depthai as dai
#import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images 
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
from .operational_mode import OperationalMode
import UltraBorg_old
import board
import adafruit_bno055
import threading

# Board #1, address 10
UB1 = UltraBorg_old.UltraBorg()
UB1.i2cAddress = 10
UB1.Init()
# Board #2, address 11
UB2 = UltraBorg_old.UltraBorg()
UB2.i2cAddress = 11
UB2.Init()
# Board #3, address 12
UB3 = UltraBorg_old.UltraBorg()
UB3.i2cAddress = 12
UB3.Init()
# Board #4, address 13
#UB4 = UltraBorg.UltraBorg()
#UB4.i2cAddress = 13
#UB4.Init()
UB1.SetWithRetry(UB1.SetServoMaximum1, UB1.GetServoMaximum1, 4520, 5)
UB1.SetWithRetry(UB1.SetServoMinimum1, UB1.GetServoMinimum1, 1229, 5)
UB1.SetWithRetry(UB1.SetServoStartup1, UB1.GetServoStartup1, 3100, 5)
UB1.SetWithRetry(UB1.SetServoMaximum2, UB1.GetServoMaximum2, 3026, 5)
UB1.SetWithRetry(UB1.SetServoMinimum2, UB1.GetServoMinimum2, 1592, 5)
UB1.SetWithRetry(UB1.SetServoStartup2, UB1.GetServoStartup2, 2408, 5)
UB2.SetWithRetry(UB2.SetServoMaximum1, UB2.GetServoMaximum1, 4951, 5)
UB2.SetWithRetry(UB2.SetServoMinimum1, UB2.GetServoMinimum1, 1076, 5)
UB2.SetWithRetry(UB2.SetServoStartup1, UB2.GetServoStartup1, 2949, 5)
UB2.SetWithRetry(UB2.SetServoMaximum2, UB2.GetServoMaximum2, 5247, 5)
UB2.SetWithRetry(UB2.SetServoMinimum2, UB2.GetServoMinimum2, 1136, 5)
UB2.SetWithRetry(UB2.SetServoStartup2, UB2.GetServoStartup2, 3130, 5)
UB2.SetWithRetry(UB2.SetServoMaximum3, UB2.GetServoMaximum3, 4561, 5)
UB2.SetWithRetry(UB2.SetServoMinimum3, UB2.GetServoMinimum3, 989, 5)
UB2.SetWithRetry(UB2.SetServoStartup3, UB2.GetServoStartup3, 2737, 5)

i2c = board.I2C()  # uses board.SCL and board.SDA
sensor = adafruit_bno055.BNO055_I2C(i2c)
last_val = 0xFFFF

def temperature():
    global last_val  # pylint: disable=global-statement
    result = sensor.temperature
    if abs(result - last_val) == 128:
        result = sensor.temperature
        if abs(result - last_val) == 128:
            return 0b00111111 & result
    last_val = result
    return result

calibrated = False
print("Cal status (S,G,A,M):{}".format(sensor.calibration_status))
print("Temperature: {} degrees C".format(temperature()))  # Uncomment if using a Raspberry Pi
print("Euler angle: {}".format(sensor.euler))

global stop_distance,obdis, colour, stop_flag
colour=(255,255,255)
stop_distance,obdis,stop_threads,distance=400,400,False,9999
        
def sensor_scan(stop, distance):
    while True:
        distance[0]=int(UB2.GetDistance1());distance[1]=int(UB2.GetDistance2());distance[2]=int(UB2.GetDistance3());distance[3]=int(UB2.GetDistance4())
        if stop():
            break
        
class RobotChaseMode(OperationalMode):
    # ALIGNMENT  attaching servo horns the screw is positioned over the sticker on the servo
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def get_key(self) -> str:
        return 'y'
    
    def get_state(self) -> RobotState:
        return RobotState.ROBOT_CHASE
    
    def _start(self, robot):
        time.sleep(0.1)
        robot.say("Robot Chase Mode engaged")
        print("Robot Chase Mode engaged printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        robot.play_sound("Radar_scanning_chirp")

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        robot.say("Robot Chase Mode disengaged")
        print("Robot Chase Mode disengaged printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        time.sleep(1.6)
        robot.play_sound("Questioning_computer_chirp")
        
    def obstacleAvoid(self, avoided):
            mainloop=True
            rx=0
            ry=0
            forward_count=0
            turn=''
            us1,us2,us3,us4,usr1,usr2,usr3,usr4=9999,9999,9999,9999,9999,9999,9999,9999
            while mainloop:
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        mainloop = False
                        break
                    if event.type == pygame.KEYDOWN:
                        if event.key == pygame.K_z:
                            UB2.SetServoPosition2(0)
                            UB2.SetServoPosition1(0)
                            mainloop = False
                            break
                us1,us2,us3,us4=int(UB2.GetDistance1()),int(UB2.GetDistance2()),int(UB2.GetDistance3()),int(UB2.GetDistance4())
                usr1,usr2,usr3,usr4=int(UB3.GetDistance1()),int(UB3.GetDistance2()),int(UB3.GetDistance3()),int(UB3.GetDistance4())
                print (us1, "  ",us2, " ", us3, " ", us4," Front sensors")
                print (usr1, "  ",usr2, " ", usr3, " ", usr4," Rear sensors") 
                if us1<obdis or us2<obdis or us3<obdis or us4<obdis:  
                    if us1<obdis and us2>obdis and us3>obdis and us4>obdis:#turn left,right sensor close to object 
                        rx = 0.1
                        ry = 0.98
                        turn='left'#ant-clockwise
                    if us1<obdis and us2<obdis and us3>obdis and us4>obdis:#turn left,right 2 sensors close to object 
                        rx = 0.1
                        ry = 0.98
                        turn='left'#ant-clockwise
                    if us1<obdis and us2<obdis and us3<obdis and us4>obdis:#turn left,right 3 sensors close to object 
                        rx = 0.1
                        ry = 0.98
                        turn='left'#ant-clockwise
                    if us1>obdis and us2<obdis and us3>obdis and us4>obdis:#turn left,right 3 sensors close to object 
                        rx = 0.1
                        ry = 0.98
                        turn='left'#ant-clockwise
                    if us4<obdis and us3>obdis and us2>obdis and us1>obdis:#turn right,left sensor close to object
                        rx = -0.26
                        ry = -1
                        turn='right'#clockwise
                    if us4<obdis and us3<obdis and us2>obdis and us1>obdis:#turn right,left 2 sensors close to object
                        rx = -0.26
                        ry = -1
                        turn='right'#clockwise
                    if us4<obdis and us3<obdis and us2<obdis and us1>obdis:#turn right,left 3 sensors close to object
                        rx = -0.26
                        ry = -1
                        turn='right'#clockwise
                    if us4>obdis and us3<obdis and us2>obdis and us1>obdis:#turn right,left 3 sensors close to object
                        rx = -0.26
                        ry = -1
                        turn='right'#clockwise
                    if us1<obdis and us2<obdis and us3<obdis and us4<obdis:#back out of corner
                        rx = 0.98
                        ry = 0.24
                        turn='back'
                    if us2<obdis and us3<obdis and us1>obdis and us4>obdis:#front sensors triggered without other 2,turn right
                        rx = 0.1
                        ry = 0.98
                        turn='left'#ant-clockwise
                else:
                    if turn=='left':# turn 1 last time before going forward
                        rx=0.1
                        ry=0.98
                        turn=''
                    if turn=='right':# turn 1 last time before going forward
                        rx = -0.26
                        ry = -0.1
                        turn=''
                    if turn=='back':# go back 1 last time 
                        rx = 0.98
                        ry = 0.24
                        turn=''
                    UB2.SetServoPosition2(rx)
                    UB2.SetServoPosition1(ry)
                    time.sleep(0.2)
                    rx = -0.95
                    ry = -0.12# drive forward as NO objects have been detected
                    UB2.SetServoPosition2(rx)
                    UB2.SetServoPosition1(ry)
                    print("moving forward")
                    time.sleep(0.2)
                    avoided = True
                    forward_count += 1
                    if forward_count>6:
                        mainloop = False
                        forward_count=0
                UB2.SetServoPosition2(rx)
                UB2.SetServoPosition1(ry)
                time.sleep(0.15)
 
    def avoid(self, avoided):
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str=str(us1)
        us2_str=str(us2)
        us3_str=str(us3)
        us4_str=str(us4)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        print('........................... obstacle detected')
        print('attempting to avoid obstacle')
        #robot.say("Obstacle detected")
        images=load_images('/home/raspberrypi/MiniMax/Animations/inmyway/')
        self.sound_manager.play_sound("inmyway")
        preObsHeading, roll, pitch = sensor.euler #left euler goes down. Righr euler goes up.
        print("Before obstacle avoided heading",preObsHeading)
        self.obstacleAvoid(avoided)
        afterObsHeading, roll, pitch = sensor.euler
        print("after obstacle avoided heading",afterObsHeading)
        diff1 = int(preObsHeading - afterObsHeading)
        diff2 = abs(diff1)
        while diff2>1:
            afterObsHeading, roll, pitch = sensor.euler#aproximate same heading 
            diff1 = int(preObsHeading - afterObsHeading) #
            diff2 = abs(diff1) #
            if preObsHeading<=180:
                if afterObsHeading > preObsHeading and (preObsHeading+180) > afterObsHeading:
                    #turn Left or anti-clockwise to approach destination in shortest way
                    x=0.1
                    y=0.98
                else: # right clockwise 
                    x=-0.26
                    y=-1.0
            else:
                if preObsHeading > afterObsHeading and (afterObsHeading+180) > preObsHeading:
                    #turn Right
                    x=-0.26
                    y=-1.0
                else: # shortest way around a 360 degree circle to the pre obstacle angle
                    x=0.1
                    y=0.98
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        break
            UB2.SetServoPosition2(x)
            UB2.SetServoPosition1(y)
        print("back to original angle +- 1 degrees")
        UB2.SetServoPosition2(0)
        UB2.SetServoPosition1(0)
        #cv2.destroyAllWindows()
        r_person=0  
    
    def _exit():
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)

    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #THIS WAS USED, TO SAY "TO GIVE A BISCUIT TO"
            self.animate(images) #robot.config.animations[19])
        else:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #This is used if normal search person mode is activated.
            self.animate(images) #robot.config.animations[19])
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        
    def stop(self, robot):
        print('First stop check')
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str,us2_str,us3_str,us4_str=str(us1),str(us2),str(us3),str(us4)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        print('........................... Objective Reached')
        robot.say("Objective reached")
        cv2.destroyAllWindows()
        r_person=0
    
    def _main(self, robot):
        global z, current_z, current_z_str,us1_str,us2_str,us3_str,us4_str,rx,ry,us1,us2,us3,us4
        z, current_z, stop_distance,rx,ry = 9999,9999,700,0,0
        nnPath = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob'
        # MobilenetSSD label texts
        labelMap = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
                    "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
        syncNN = True
        pipeline = self._create_pipeline3(robot)
        cv2.namedWindow('rectified right', cv2.WINDOW_GUI_NORMAL)
        # Connect to device and start pipeline
        with dai.Device(pipeline) as device:
            # Output queues will be used to get the rgb frames and nn data from the outputs defined above
            previewQueue = device.getOutputQueue(name="right", maxSize=8, blocking=False)
            detectionNNQueue = device.getOutputQueue(name="detections", maxSize=8, blocking=False)
            depthQueue = device.getOutputQueue(name="depth", maxSize=8, blocking=False)
            rectifiedRight = None
            found_people=[{'objxcenter': 60,'z_depth':9999}]
            sorted_x={}
            detections = []
            startTime = time.monotonic()
            counter = 0
            fps = 0
            color = (255, 255, 255)
            us1,us2,us3,us4=9999,9999,9999,9999
            my_distances = [7777,8888,9999,5555]
            current_z=9999
            stop_threads = False
            t1 = threading.Thread(target = sensor_scan, args =(lambda : stop_threads, my_distances, ))
            t1.start()
            while True:
                inRectified = previewQueue.get()
                inDet = detectionNNQueue.get()
                counter += 1
                currentTime = time.monotonic()
                if (currentTime - startTime) > 1:
                    fps = counter / (currentTime - startTime)
                    counter = 0
                    startTime = currentTime
                rectifiedRight = inRectified.getCvFrame()
                detections = inDet.detections
                height = rectifiedRight.shape[0]
                width = rectifiedRight.shape[1]
                if current_z < stop_distance:
                    self.stop(robot)
                    stop_threads=True
                    t1.join()
                    return True
                if us1 < 450 or us2 < 450 or us3 < 450 or us4 <450: # forign object is close  
                    avoided=False
                    self.avoid(avoided)
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    # do everything needed when robot finds obstacle
                found_peeps=0    
                n_of_track_peeps=0
                for detection in detections:
                    label = labelMap[detection.label]
                    if label!='person':
                        continue
                    current_z=int(detection.spatialCoordinates.z)
                    roiData = detection.boundingBoxMapping
                    roi = roiData.roi
                    x1,x2,y1 = int(detection.xmin * width),int(detection.xmax * width),int(detection.ymin * height)
                    x_diff = (x2-x1)
                    obj_x_center = int(x1+(x_diff/2))
                    dict = {'objxcenter': obj_x_center, 'z_depth': current_z}
                    found_people.append(dict)
                    #cv2.putText(rectifiedRight, f"Z: {current_z} ", (x1 + 10, y1 + 80), cv2.FONT_HERSHEY_TRIPLEX, 0.75, color)
                    cv2.putText(rectifiedRight, f"c {obj_x_center} ", (x1 + 10, y1 + 120), cv2.FONT_HERSHEY_TRIPLEX, 0.75, color)
                sorted_x=min((x for x in found_people if x['z_depth'] != "9999"), key=lambda x:x['z_depth'], default=None)
                found_people=[{'objxcenter': 60,'z_depth':9999}]
                obj_x_center= sorted_x['objxcenter'] # get x co-ordinate of closest object
                current_z = sorted_x['z_depth']# get z-depth co-ordinate of closest object
                x_deviation = int(robot.config.ps_xres/2)-obj_x_center
                if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                    if current_z < stop_distance:
                        self.stop(robot)
                        stop_threads=True
                        t1.join()
                        return True 
                    if us1<450 or us2<450 or us3<450 or us4<450:#object is very close 
                        print('Second stop check')
                        avoided=False
                        self.avoid(avoided)
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                    else:
                        rx = -0.95
                        ry = -0.12 # move forward
                        UB2.SetServoPosition2(rx)
                        UB2.SetServoPosition1(ry)
                        #print("........................... moving robot FORWARD")
                else:
                    if (x_deviation>robot.config.ps_tolerance):
                        if x_deviation< robot.config.ps_far_boundry:
                            rx=-0.9
                            ry=0.1
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('........... turning left while moving forward' )
                        if x_deviation>=robot.config.ps_far_boundry:
                            rx=0.1
                            ry=0.98
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('..... turning left on the spot' )
                    elif ((x_deviation*-1)>robot.config.ps_tolerance):
                        if abs(x_deviation)<robot.config.ps_far_boundry:
                            rx=-0.9
                            ry=-0.3
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('............ turning right while moving forward' )
                        if abs(x_deviation)>=robot.config.ps_far_boundry:
                            rx=-0.26
                            ry=-1.0
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('..... turning right on the spot' )
                #cv2.rectangle(rectifiedRight, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)
                #cv2.putText(rectifiedRight, f"US: {(my_distances)} ", (1, 180), cv2.FONT_HERSHEY_TRIPLEX, 0.6, color)
                cv2.putText(rectifiedRight, "fps: {:.2f}".format(fps), (2, rectifiedRight.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(images) #robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    stop_threads=True
                    t1.join()
                    return False
                rectifiedRight=cv2.resize(rectifiedRight,(600,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                cv2.imshow("rectified right", rectifiedRight)
       
    def _create_pipeline3(self, robot):
        nnPath = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob'
        # MobilenetSSD label texts
        labelMap = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
                    "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
        syncNN = True
        # Create pipeline
        pipeline = dai.Pipeline()
        # Define sources and outputs
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        imageManip = pipeline.create(dai.node.ImageManip)
        xoutManip = pipeline.create(dai.node.XLinkOut)
        nnOut = pipeline.create(dai.node.XLinkOut)
        xoutDepth = pipeline.create(dai.node.XLinkOut)
        xoutManip.setStreamName("right")
        nnOut.setStreamName("detections")
        xoutDepth.setStreamName("depth")
        # Properties
        imageManip.initialConfig.setResize(300, 300)
        # The NN model expects BGR input. By default ImageManip output type would be same as input (gray in this case)
        imageManip.initialConfig.setFrameType(dai.ImgFrame.Type.BGR888p)
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
        monoLeft.setFps(23)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
        monoRight.setFps(23)
        monoRight.setCamera("right")
        # StereoDepth
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        stereo.setSubpixel(False)
        # Define a neural network that will make predictions based on the source frames
        spatialDetectionNetwork.setConfidenceThreshold(0.6)
        spatialDetectionNetwork.setBlobPath(nnPath)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)
        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        imageManip.setKeepAspectRatio(False)
        imageManip.out.link(spatialDetectionNetwork.input)
        
        if syncNN:
            spatialDetectionNetwork.passthrough.link(xoutManip.input)
        else:
            imageManip.out.link(xoutManip.input)
        spatialDetectionNetwork.out.link(nnOut.input)
        stereo.rectifiedRight.link(imageManip.inputImage)
        #stereo.rectifiedRight.setPreviewKeepAspectRatio(False)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        spatialDetectionNetwork.passthroughDepth.link(xoutDepth.input)
        return pipeline
   
#*****************************************************************************************************************
class RemoteControlMode(OperationalMode):
    # ALIGNMENT  attaching servo horns the screw is positioned over the sticker on the servo
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def get_key(self) -> str:
        return 'x'
    
    def get_state(self) -> RobotState:
        return RobotState.REMOTE_CONTROL
    
    def _start(self, robot):
        time.sleep(0.1)
        robot.say("Remote control mode enabled")
        print("Remote control enabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        robot.play_sound("Double_beep2")

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        robot.say("Remote Control Mode has been disabled")
        print("Remote control disabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        time.sleep(1.6)
        robot.play_sound("Long_power_down")
        
    def _main(self, robot):
        Direction='neutral'
        keyup,x,y,running=0,0,0,True
        UB2.SetServoPosition3(0)
        hp=0
        pantilt=0
        k_w,k_a,k_s,k_d,k_q,k_e,k_z,k_x,k_l,k_r,k_o,k_p = 0,0,0,0,0,0,0,0,0,0,0,0
        while running:
            events = pygame.event.get()
            for event in events:
                #print('this is keyup at start of loop ',keyup)
                if event.type == pygame.KEYUP:
                    keyup=1
                    if event.key == pygame.K_w:
                        k_w = 0
                    if event.key == pygame.K_a:
                        k_a = 0
                    if event.key == pygame.K_s:
                        k_s = 0
                    if event.key == pygame.K_d:
                        k_d = 0
                    if event.key == pygame.K_q:
                        k_q = 0
                    if event.key == pygame.K_e:
                        k_e = 0
                    if event.key == pygame.K_z:
                        k_z = 0
                    if event.key == pygame.K_x:
                        k_x = 0
                    if event.key == pygame.K_l:
                        k_l = 0
                    if event.key == pygame.K_r:
                        k_r = 0
                if event.type == pygame.QUIT:
                    _exit()
                    pygame.quit()
                    running = False
                    UB2.SetServoPosition1(0);UB2.SetServoPosition2(0);UB2.SetServoPosition3(0);UB1.SetServoPosition1(0)    #set servos to default positions on exit     
                if event.type == pygame.KEYDOWN:
                    keyup=0
                    if event.key == pygame.K_ESCAPE:
                        UB2.SetServoPosition1(0);UB2.SetServoPosition2(0);UB2.SetServoPosition3(0);UB1.SetServoPosition1(0) #set servos to default positions on exit  
                        running = False
                    if event.key == pygame.K_q:
                        k_q = 1
                    if event.key == pygame.K_e:
                        k_e = 1
                    if event.key == pygame.K_w:
                        k_w = 1
                    if event.key == pygame.K_a:
                        k_a = 1
                    if event.key == pygame.K_s:
                        k_s = 1
                    if event.key == pygame.K_d:
                        k_d = 1
                    if event.key == pygame.K_z:
                        k_z = 1
                    if event.key == pygame.K_x:
                        k_x = 1
                    if event.key == pygame.K_b:
                        k_b = 1
                    if event.key == pygame.K_f:
                        k_f = 1
                    if event.key == pygame.K_l:
                        k_l = 1
                    if event.key == pygame.K_r:
                        k_r = 1
                if k_w:
                    #print("w")
                    Direction ='forward'
                    x=-0.95
                    y=-0.12
                if k_a:
                    #print("a")
                    Direction ='left'# anti-clockwise euler goes down
                    x=0.1
                    y=0.8
                if k_d:
                    #print("d")
                    Direction='right'# clockwise euler goes up
                    x=-0.26
                    y=-1.0
                if k_q:
                    #print("q")
                    Direction ='left-forward'
                    x=-0.9
                    y=0.1
                if k_e:
                    #print("e")
                    Direction ='right-forward'
                    x=-0.9
                    y=-0.3
                if k_s:
                    #print("s")
                    Direction='back'
                    x=0.98
                    y=0.24
                if k_z:
                    #print("z")
                    Direction ='left-reverse'
                    x=0.98
                    y=-0.02
                if k_x:
                    #print("x")
                    Direction ='right-reverse'
                    x=0.98
                    y=0.4
                if k_l:
                    #print("Head Left")
                    Direction='Head left'
                    UB2.SetServoPosition4(0)
                    if hp <-0.95:
                        hp=-0.95
                    else:
                        hp=hp-0.02
                #print(hp)
                if k_r:
                    #print("Head Right")
                    Direction='Head right'
                    if hp > 0.95:
                        hp=0.95
                    else:
                        hp=hp+0.02
                if keyup==1:
                    Direction='neutral'
                    x=0
                    y=0
                UB2.SetServoPosition2(x);UB2.SetServoPosition1(y);UB2.SetServoPosition3(hp)
                
class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = ['person', 'cup',]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
    
    def obstacleAvoid(self, avoided):
        mainloop=True
        rx=0
        ry=0
        forward_count=0
        turn=''
        us1,us2,us3,us4,usr1,usr2,usr3,usr4=9999,9999,9999,9999,9999,9999,9999,9999
        while mainloop:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    mainloop = False
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        mainloop = False
                        break
            us1,us2,us3,us4=int(UB2.GetDistance1()),int(UB2.GetDistance2()),int(UB2.GetDistance3()),int(UB2.GetDistance4())
            usr1,usr2,usr3,usr4=int(UB3.GetDistance1()),int(UB3.GetDistance2()),int(UB3.GetDistance3()),int(UB3.GetDistance4())
            print (us1, "  ",us2, " ", us3, " ", us4," Front sensors")
            print (usr1, "  ",usr2, " ", usr3, " ", usr4," Rear sensors") 
            if us1<obdis or us2<obdis or us3<obdis or us4<obdis:  
                if us1<obdis and us2>obdis and us3>obdis and us4>obdis:#turn left,right sensor close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us1<obdis and us2<obdis and us3>obdis and us4>obdis:#turn left,right 2 sensors close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us1<obdis and us2<obdis and us3<obdis and us4>obdis:#turn left,right 3 sensors close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us1>obdis and us2<obdis and us3>obdis and us4>obdis:#turn left,right 3 sensors close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us4<obdis and us3>obdis and us2>obdis and us1>obdis:#turn right,left sensor close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us4<obdis and us3<obdis and us2>obdis and us1>obdis:#turn right,left 2 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us4<obdis and us3<obdis and us2<obdis and us1>obdis:#turn right,left 3 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us4>obdis and us3<obdis and us2>obdis and us1>obdis:#turn right,left 3 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us1<obdis and us2<obdis and us3<obdis and us4<obdis:#back out of corner
                    rx = 0.98
                    ry = 0.24
                    turn='back'
                if us2<obdis and us3<obdis and us1>obdis and us4>obdis:#front sensors triggered without other 2,turn right
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
            else:
                if turn=='left':# turn 1 last time before going forward
                    rx=0.1
                    ry=0.98
                    turn=''
                if turn=='right':# turn 1 last time before going forward
                    rx = -0.26
                    ry = -0.1
                    turn=''
                if turn=='back':# go back 1 last time 
                    rx = 0.98
                    ry = 0.24
                    turn=''
                UB2.SetServoPosition2(rx)
                UB2.SetServoPosition1(ry)
                time.sleep(0.2)
                rx = -0.95
                ry = -0.12# drive forward as NO objects have been detected
                UB2.SetServoPosition2(rx)
                UB2.SetServoPosition1(ry)
                print("moving forward")
                time.sleep(0.2)
                avoided = True
                forward_count += 1
                if forward_count>6:
                    mainloop = False
                    forward_count=0
            UB2.SetServoPosition2(rx)
            UB2.SetServoPosition1(ry)
            time.sleep(0.15)
 
    def avoid(self, avoided):
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str=str(us1)
        us2_str=str(us2)
        us3_str=str(us3)
        us4_str=str(us4)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        print('........................... obstacle detected')
        print('attempting to avoid obstacle')
        #robot.say("Obstacle detected")
        images=load_images('/home/raspberrypi/MiniMax/Animations/inmyway/')
        self.sound_manager.play_sound("inmyway")
        preObsHeading, roll, pitch = sensor.euler #left euler goes down. Righr euler goes up.
        print("Before obstacle avoided heading",preObsHeading)
        self.obstacleAvoid(avoided)
        afterObsHeading, roll, pitch = sensor.euler
        print("after obstacle avoided heading",afterObsHeading)
        diff1 = int(preObsHeading - afterObsHeading)
        diff2 = abs(diff1)
        while diff2>1:
            afterObsHeading, roll, pitch = sensor.euler#aproximate same heading 
            diff1 = int(preObsHeading - afterObsHeading) #
            diff2 = abs(diff1) #
            if preObsHeading<=180:
                if afterObsHeading > preObsHeading and (preObsHeading+180) > afterObsHeading:
                    #turn Left or anti-clockwise to approach destination in shortest way
                    x=0.1
                    y=0.98
                else: # right clockwise 
                    x=-0.26
                    y=-1.0
            else:
                if preObsHeading > afterObsHeading and (afterObsHeading+180) > preObsHeading:
                    #turn Right
                    x=-0.26
                    y=-1.0
                else: # shortest way around a 360 degree circle to the pre obstacle angle
                    x=0.1
                    y=0.98
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        break
            UB2.SetServoPosition2(x)
            UB2.SetServoPosition1(y)
        print("back to original angle +- 1 degrees")
        UB2.SetServoPosition2(0)
        UB2.SetServoPosition1(0)
        #cv2.destroyAllWindows()
        r_person=0  
    
    def _exit():
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)

    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #THIS WAS USED, TO SAY "TO GIVE A BISCUIT TO"
            self.animate(images) #robot.config.animations[19])
        else:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #This is used if normal search person mode is activated.
            self.animate(images) #robot.config.animations[19])
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        
    def stop(self, robot):
        print('First stop check')
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str,us2_str,us3_str,us4_str=str(us1),str(us2),str(us3),str(us4)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        print('........................... Objective Reached')
        robot.say("Objective reached")
        cv2.destroyAllWindows()
        r_person=0
    
    def _main(self, robot):
        global z, current_z, current_z_str,us1_str,us2_str,us3_str,us4_str,rx,ry,us1,us2,us3,us4
        z, current_z, stop_distance,rx,ry = 9999,9999,700,0,0
        nnPath = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob'
        # MobilenetSSD label texts
        labelMap = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
                    "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
        syncNN = True
        pipeline = self._create_pipeline2(robot)
        cv2.namedWindow('rectified right', cv2.WINDOW_GUI_NORMAL)
        # Connect to device and start pipeline
        with dai.Device(pipeline) as device:
            # Output queues will be used to get the rgb frames and nn data from the outputs defined above
            previewQueue = device.getOutputQueue(name="right", maxSize=8, blocking=False)
            detectionNNQueue = device.getOutputQueue(name="detections", maxSize=8, blocking=False)
            depthQueue = device.getOutputQueue(name="depth", maxSize=8, blocking=False)
            rectifiedRight = None
            found_people=[{'objxcenter': 60,'z_depth':9999}]
            sorted_x={}
            detections = []
            startTime = time.monotonic()
            counter = 0
            fps = 0
            color = (255, 255, 255)
            us1,us2,us3,us4=9999,9999,9999,9999
            my_distances = [7777,8888,9999,5555]
            current_z=9999
            stop_threads = False
            t1 = threading.Thread(target = sensor_scan, args =(lambda : stop_threads, my_distances, ))
            t1.start()
            while True:
                inRectified = previewQueue.get()
                inDet = detectionNNQueue.get()
                counter += 1
                currentTime = time.monotonic()
                if (currentTime - startTime) > 1:
                    fps = counter / (currentTime - startTime)
                    counter = 0
                    startTime = currentTime
                rectifiedRight = inRectified.getCvFrame()
                detections = inDet.detections
                height = rectifiedRight.shape[0]
                width = rectifiedRight.shape[1]
                if current_z < stop_distance:
                    self.stop(robot)
                    stop_threads=True
                    t1.join()
                    return True
                if us1 < 450 or us2 < 450 or us3 < 450 or us4 <450: # forign object is close  
                    avoided=False
                    self.avoid(avoided)
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    # do everything needed when robot finds obstacle
                found_peeps=0    
                n_of_track_peeps=0
                for detection in detections:
                    label = labelMap[detection.label]
                    if label!='person':
                        continue
                    current_z=int(detection.spatialCoordinates.z)
                    roiData = detection.boundingBoxMapping
                    roi = roiData.roi
                    x1,x2,y1 = int(detection.xmin * width),int(detection.xmax * width),int(detection.ymin * height)
                    x_diff = (x2-x1)
                    obj_x_center = int(x1+(x_diff/2))
                    dict = {'objxcenter': obj_x_center, 'z_depth': current_z}
                    found_people.append(dict)
                    #cv2.putText(rectifiedRight, f"Z: {current_z} ", (x1 + 10, y1 + 80), cv2.FONT_HERSHEY_TRIPLEX, 0.75, color)
                    cv2.putText(rectifiedRight, f"c {obj_x_center} ", (x1 + 10, y1 + 120), cv2.FONT_HERSHEY_TRIPLEX, 0.75, color)
                sorted_x=min((x for x in found_people if x['z_depth'] != "9999"), key=lambda x:x['z_depth'], default=None)
                found_people=[{'objxcenter': 60,'z_depth':9999}]
                obj_x_center= sorted_x['objxcenter'] # get x co-ordinate of closest object
                current_z = sorted_x['z_depth']# get z-depth co-ordinate of closest object
                x_deviation = int(robot.config.ps_xres/2)-obj_x_center
                if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                    if current_z < stop_distance:
                        self.stop(robot)
                        stop_threads=True
                        t1.join()
                        return True 
                    if us1<450 or us2<450 or us3<450 or us4<450:#object is very close 
                        print('Second stop check')
                        avoided=False
                        self.avoid(avoided)
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                    else:
                        rx = -0.95
                        ry = -0.12 # move forward
                        UB2.SetServoPosition2(rx)
                        UB2.SetServoPosition1(ry)
                        #print("........................... moving robot FORWARD")
                else:
                    if (x_deviation>robot.config.ps_tolerance):
                        if x_deviation< robot.config.ps_far_boundry:
                            rx=-0.9
                            ry=0.1
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('........... turning left while moving forward' )
                        if x_deviation>=robot.config.ps_far_boundry:
                            rx=0.1
                            ry=0.98
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('..... turning left on the spot' )
                    elif ((x_deviation*-1)>robot.config.ps_tolerance):
                        if abs(x_deviation)<robot.config.ps_far_boundry:
                            rx=-0.9
                            ry=-0.3
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('............ turning right while moving forward' )
                        if abs(x_deviation)>=robot.config.ps_far_boundry:
                            rx=-0.26
                            ry=-1.0
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #print('..... turning right on the spot' )
                #cv2.rectangle(rectifiedRight, (x1, y1), (x2, y2), color, cv2.FONT_HERSHEY_SIMPLEX)
                cv2.putText(rectifiedRight, f"US: {(my_distances)} ", (1, 180), cv2.FONT_HERSHEY_TRIPLEX, 0.6, color)
                cv2.putText(rectifiedRight, "fps: {:.2f}".format(fps), (2, rectifiedRight.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(images) #robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    stop_threads=True
                    t1.join()
                    return False
                rectifiedRight=cv2.resize(rectifiedRight,(600,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                
                cv2.imshow("rectified right", rectifiedRight)
       
    def _create_pipeline2(self, robot):
        nnPath = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob'
        # MobilenetSSD label texts
        labelMap = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
                    "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
        syncNN = True
        # Create pipeline
        pipeline = dai.Pipeline()
        # Define sources and outputs
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        imageManip = pipeline.create(dai.node.ImageManip)
        xoutManip = pipeline.create(dai.node.XLinkOut)
        nnOut = pipeline.create(dai.node.XLinkOut)
        xoutDepth = pipeline.create(dai.node.XLinkOut)
        xoutManip.setStreamName("right")
        nnOut.setStreamName("detections")
        xoutDepth.setStreamName("depth")
        # Properties
        imageManip.initialConfig.setResize(300, 300)
        # The NN model expects BGR input. By default ImageManip output type would be same as input (gray in this case)
        imageManip.initialConfig.setFrameType(dai.ImgFrame.Type.BGR888p)
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
        monoLeft.setFps(23)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
        monoRight.setFps(23)
        monoRight.setCamera("right")
        # StereoDepth
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        stereo.setSubpixel(False)
        # Define a neural network that will make predictions based on the source frames
        spatialDetectionNetwork.setConfidenceThreshold(0.6)
        spatialDetectionNetwork.setBlobPath(nnPath)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)
        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        imageManip.setKeepAspectRatio(False)
        imageManip.out.link(spatialDetectionNetwork.input)
        
        if syncNN:
            spatialDetectionNetwork.passthrough.link(xoutManip.input)
        else:
            imageManip.out.link(xoutManip.input)
        spatialDetectionNetwork.out.link(nnOut.input)
        stereo.rectifiedRight.link(imageManip.inputImage)
        #stereo.rectifiedRight.setPreviewKeepAspectRatio(False)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        spatialDetectionNetwork.passthroughDepth.link(xoutDepth.input)
        return pipeline
   
    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                images=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animate(images) #robot.config.animations[8])
                #robot.say(f'I think I found a {self.label}')
            else:
                images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images) #robot.config.animations[11])
                robot.say("what is going on")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images) #robot.config.animations[11])
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                images=load_images('/home/raspberrypi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(images) #robot.config.animations[9])
                time.sleep(0.5)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)
                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)
        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))
            time.sleep(1)
        robot.say('The jelly beans are leaving now bye bye')
        #robot.animate(1)
        robot.write_serial('9z')  # make robot do a 180 degree turn
        robot.write_serial('2z')  # stop robot
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT

# === ./src/old/operational_modes_kinda_works.py ===
import time
import operator
import depthai as dai
import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images 
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
from .operational_mode import OperationalMode
import UltraBorg_old

UB = UltraBorg_old.UltraBorg()      # Create a new UltraBorg object
UB.Init()
global stop_distance, colour
colour=(255,255,255)
stop_distance=500

class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(robot.config.animations[4])
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+20, y-96), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 1)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disagegender/')
        self.sound_manager.play_sound("disagegender")
        self.animate(robot.config.animations[5])
        #robot.say("Age and Gender Detection disabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(robot.config.animations[7])
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            responses = ['neutral', 'happy', 'sad', 'surprise', 'anger']
            neutral,happy,sad,surprise,anger=0,0,0,0,0
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]
                    
                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 1)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, emotion_name, (bbox[0]+10, y-110), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 7)
                        cv2.putText(frame, emotion_name, (bbox[0], y-90), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 1)
                        if emotion_name=="neutral":
                            neutral=neutral +1
                        if emotion_name=="happy":
                            happy=happy +1
                        if emotion_name=="sad":
                            sad=sad +1
                        if emotion_name=="surprise":
                            surprise=surprise +1
                        if emotion_name=="anger":
                            anger=anger +1
                        #print(emotion_name)    
                        
                            #self.animate(animations[7])
                            #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    if neutral>20 or happy>20 or sad>20 or surprise>20 or anger>20:
                            max_response = max(zip(responses, (map(eval, responses))), key=lambda tuple: tuple[1])[0]
                            self.sound_manager.play_sound(max_response)
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/disemotional/')
        self.sound_manager.play_sound("disemotional")
        self.animate(robot.config.animations[6])
        #robot.say("Emotion Detection state disabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will display spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't display spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS
    
class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = ['person', 'cup',]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)  

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)
    
    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label} who like jellybeans")
        else:
            self.sound_manager.play_sound("search-on")
            self.animate(robot.config.animations[18])
            time.sleep(0.1)
            self.sound_manager.play_sound("search-person")
            self.animate(robot.config.animations[19])
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)
    
    def stop(self, robot):
        #print('Old pin '+old_pin +' new pin '+pin)
        print('First stop check')
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str=str(us1)
        us2_str=str(us2)
        us3_str=str(us3)
        #r_person_str=str(r_person)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str)
        #print('r_person = '+r_person_str)
        pin="2z"
        robot.write_serial(pin)
        #print('Old pin '+old_pin +' new pin '+pin)
        print('........................... reached objective')
        print('waiting at objective reached')
        robot.say("First stop stop")
        cv2.destroyAllWindows()
        r_person=0    

    def _main(self, robot):
        global old_pin, pin, z, r_person, current_z, current_z_str,us1,us2,us3,us1_str,us2_str,us3_str,r_person_str
        pipeline = self._create_pipeline2(robot)
        pin, r_person, z, current_z, stop_distance = '2z', 0, 1000,1000,500
        #found_people[0] = {'tid': 99999, 'objxcenter': 0, 'objycenter': 0, 'status': 'TRACKED', 'z_depth': 1000}
        with dai.Device(pipeline) as device:
            preview = device.getOutputQueue("preview", 4, False)
            tracklets = device.getOutputQueue("tracklets", 4, False)
            startTime = time.monotonic()
            counter, fps, frame = 0, 0, None
            found_people=[{'tid': 666, 'objxcenter': 60,'status':'LOST','z_depth':9000}]
            
            sorted_x={}
            while True:
                us1,us2,us3 = int(UB.GetDistance1()), int(UB.GetDistance2()), int(UB.GetDistance3())
                if us1 == 0:
                    us1=99999
                if us2 == 0:
                    us2=99999
                if us3 == 0:
                    us3=99999
                imgFrame = preview.get()
                track = tracklets.get()
                counter+=1
                current_time = time.monotonic()
                if (current_time - startTime) > 1 :
                    fps = counter / (current_time - startTime)
                    counter = 0
                    startTime = current_time
                frame = imgFrame.getCvFrame()
                #frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST) # this line slows down everything!
                trackletsData = track.tracklets
                old_pin=pin
                # us = Ultrasonic sensor , z = distance to object measured by depth camera.
                if us1 < 400 or us2 < 400 or us3 < 400 or z < stop_distance:# object is very close.  
                    r_person=r_person+1
                    #print('Old pin '+old_pin +' new pin '+pin)
                    if r_person==2:
                        self.stop(robot) # do everything needed when robot finds person or obstacle
                        return True
                else:
                    r_person=0
                found_peeps=0    
                n_of_track_peeps=0
                for t in trackletsData:
                    found_peeps+=1
                    roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
                    x1,y1 = int(roi.topLeft().x), int(roi.topLeft().y)
                    x2,y2 = int(roi.bottomRight().x), int(roi.bottomRight().y)
                    z=int(t.spatialCoordinates.z)
                    xmin, ymin = x1, y1
                    xmax, ymax = x2, y2
                    x_diff, y_diff = (xmax-xmin), (ymax-ymin)
                    obj_x_center = int(xmin+(x_diff/2))
                    #obj_y_center=int(ymin+(y_diff/2))
                    #dictionary to be updated
                    dict = {'tid': t.id, 'objxcenter': obj_x_center, 'status': t.status.name, 'z_depth': z}
                    found_people.append(dict)
                    #print(' Found peeps ',found_peeps)
                    if t.status.name=='TRACKED':
                        n_of_track_peeps+=1
                    else:
                        found_people.pop()
                    #print(found_people)
                    #print('number of tracked people ',n_of_track_peeps)
                    #cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.7, colour)
                    #cv2.putText(frame, f"ID: {[t.id]}", (x1 + 10, y1 + 45), cv2.FONT_HERSHEY_TRIPLEX, 0.7, colour)
                    #cv2.putText(frame, t.status.name, (x1 + 10, y1 + 70), cv2.FONT_HERSHEY_TRIPLEX, 0.7, colour)
                    #cv2.rectangle(frame, (x1, y1), (x2, y2), colour, cv2.FONT_HERSHEY_SIMPLEX)
                    #cv2.putText(frame, f"Z: {z} mm", (x1 + 10, 195), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
                if n_of_track_peeps==0:
                    robot.write_serial("5z") # no tracked people in current frame
                    #print('no peeps')
                else:
                    sorted_x=min((x for x in found_people if x['status'] == "TRACKED"), key=lambda x:x['tid'], default=None)
                    found_people=[{'tid': 666, 'objxcenter': 60,'status':'LOST','z_depth':9000}]
                    obj_x_center= sorted_x['objxcenter'] # get x co-ordinate of lowest ID
                    #obj_y_center= sorted_x['objycenter'] # get y co-ordinate of lowest ID
                    current_z = sorted_x['z_depth'] # get z-depth co-ordinate of lowest ID
                    x_deviation = int(robot.config.ps_xres/2)-obj_x_center
                    # calculate the deviation from the center of the screen
                    if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                        if us1 < 400 or us2 < 400 or us3 < 400 or current_z < stop_distance:# object is very close 
                            print('Second stop check')
                            self.stop(robot)
                            return True
                        else:
                            pin, r_person="1z", 0
                            robot.write_serial(pin)
                            #print("........................... moving robot FORWARD")
                    else:
                        if (x_deviation>robot.config.ps_tolerance):
                            if x_deviation< robot.config.ps_far_boundry:
                                pin, r_person="3z", 0
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'........... turning left' )
                            if x_deviation>=robot.config.ps_far_boundry:
                                pin, r_person="7z", 0
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'..... turning left on the spot' )
                        elif ((x_deviation*-1)>robot.config.ps_tolerance):
                            if abs(x_deviation)<robot.config.ps_far_boundry:
                                pin, r_person="4z", 0
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'............ turning right' )
                            if abs(x_deviation)>=robot.config.ps_far_boundry:
                                pin, r_person="8z", 0
                                robot.write_serial(pin)
                                #print('Old pin '+old_pin +'  '+pin+'..... turning right on the spot' )
                cv2.putText(frame, "fps: {:.2f}".format(fps), (2, frame.shape[0] - 7), cv2.FONT_HERSHEY_TRIPLEX, 0.6, colour)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    pin='5z'
                    robot.write_serial(pin)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    return False
                #frame=cv2.resize(frame,(600,400),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                cv2.imshow("tracker", frame)

    def _create_pipeline2(self, robot):
        # Create pipeline
        pipeline = dai.Pipeline()
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        objectTracker = pipeline.create(dai.node.ObjectTracker)

        xoutRgb = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)
        #xoutDepth = pipeline.create(dai.node.XLinkOut) # test may remove
        
        xoutRgb.setStreamName("preview")
        trackerOut.setStreamName("tracklets")
        #xoutDepth.setStreamName("depth") # test may remove
        
        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        camRgb.setInterleaved(False)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        camRgb.setFps(10)
        
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoRight.setCamera("right")

        # setting node configs
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        # Align depth map to the perspective of RGB camera, on which inference is done
        stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
        stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())
        
        # Better handling for occlusions:
        stereo.setLeftRightCheck(True)
        # Closer-in minimum depth, disparity range is doubled:
        stereo.setExtendedDisparity(False)
        # Better accuracy for longer distance, fractional disparity 32-levels:
        stereo.setSubpixel(False)

        spatialDetectionNetwork.setBlobPath('/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')
        spatialDetectionNetwork.setConfidenceThreshold(0.55)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)
        
        objectTracker.setDetectionLabelsToTrack([15])  # track only person
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        camRgb.preview.link(spatialDetectionNetwork.input)
        objectTracker.passthroughTrackerFrame.link(xoutRgb.input)
        objectTracker.out.link(trackerOut.input)
        
        spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)
        #objectTracker.inputTrackerFrame.setBlocking(False)# testing
        spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        spatialDetectionNetwork.out.link(objectTracker.inputDetections)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        return pipeline
   
    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animate(robot.config.animations[8])
                #robot.say(f'I think I found a {self.label}')
            else:
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say("Search mode terminated")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(robot.config.animations[11])
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                #images=load_images('/home/raspberrypi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(robot.config.animations[9])
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)

                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)
        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))
            time.sleep(1)
        robot.say('The jelly beans are leaving now bye bye')
        #robot.animate(1)
        robot.write_serial('9z')  # make robot do a 180 degree turn
        robot.write_serial('2z')  # stop robot
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT
        



# === ./src/old/tts_manager.py ===
import threading
import queue
import pyttsx3


class TTSThread(threading.Thread):
    def __init__(self, queue, config):
        threading.Thread.__init__(self)
        self.queue = queue
        self.daemon = True
        self.config = config

        self.start()

    def run(self):
        engine = pyttsx3.init()
        engine.setProperty("rate", self.config.tts_rate)
        engine.setProperty("voice", self.config.tts_voice)
        engine.setProperty("volume", self.config.tts_volume)
        engine.startLoop(False)
        t_running = True
        while t_running:
            if self.queue.empty():
                engine.iterate()
                continue
            else:
                data = self.queue.get()
                if data == "exit":
                    t_running = False
                else:
                    engine.say(data)
                    continue
        engine.endLoop()


class TTSManager:
    def __init__(self, config) -> None:
        self.queue = queue.Queue()
        self.thread = TTSThread(self.queue, config)

    def say(self, string):
        self.queue.put(string)


# === ./src/old/anim.py ===
import time
import os
import pygame
from typing import List, Optional
import pathlib
from natsort import natsorted
global images, folder_dir

pygame.init()
display = pygame.display.set_mode((1280, 720))
thesound = pygame.mixer.Sound("/home/pi/MiniMax/DefaultSayings/greetings.wav")
folder_dir = "/home/pi/MiniMax/Results/greetings"



#display = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
def load_images(path):
    store =[]
    for file_name in os.listdir(path):
        temp=file_name
        store.append(temp)
    store=natsorted(store)  
    images = []
    image_array=[]
    for xyz in store:
        pic=path+xyz
        image_array.append(pic)
    for names in image_array:
        imagine = pygame.image.load(names).convert()
        images.append(imagine)
    return images

def show_fps():
    "shows the frame rate on the screen"
    fr = str(int(clock.get_fps()))
    frt = font.render(fr, 1, pygame.Color("red"))
    return frt

def play_sequence(images):
    loops = 0
    run = True
    length=len(images)
    print('starting the loop',length)
    while loops < length:      
        #frt=show_fps()
        #print(loops)
        image = images[loops]
        display.blit(image, (0, 0))
        pygame.display.flip()
        loops = loops +2
      
clock = pygame.time.Clock()
font = pygame.font.SysFont("Arial", 30)
animation=load_images('/home/pi/MiniMax/Results/greetings/')
pygame.mixer.Sound.play(thesound)
play_sequence(animation)
display.blit(images[22], (x, y))
pygame.display.update()
pygame.quit()

# === ./src/old/multithread1.py ===
from threading import Thread, Event
from time import sleep
import UltraBorg_old
import board
import adafruit_bno055
# Board #1, address 10
UB1 = UltraBorg_old.UltraBorg()
UB1.i2cAddress = 10
UB1.Init()
# Board #2, address 11
UB2 = UltraBorg_old.UltraBorg()
UB2.i2cAddress = 11
UB2.Init()
# Board #3, address 12
UB3 = UltraBorg_old.UltraBorg()
UB3.i2cAddress = 12
UB3.Init()
event = Event()

def modify_variable(var):
    while True:
        var=int(UB2.GetDistance1())
        
        if event.is_set():
            break
        sleep(.5)
    print('Stop printing')


my_var = [7778]
t = Thread(target=modify_variable, args=(my_var, ))
t.start()
while True:
    try:
        print(my_var)
        sleep(1)
    except KeyboardInterrupt:
        event.set()
        break
t.join()
print(my_var)

# === ./src/old/sound_manager_videoplay.py ===
from typing import List, Optional
import pygame
#import moviepy.editor
#from moviepy.editor import VideoFileClip
import numpy as np
import cv2
import pygame  #pip install pygame
from pygame import mixer
import pathlib
mixer.init()
# I don't think sounds in pygame block, so should be fine to run in the main process
# If the sounds do block, then will probably want to split this all into a seperate
# process

#TODO: want to change all the filenames to be more descriptive, given that we will be
# playing sounds by filename

_default_filepaths = [
   "Sound/Default/greetings.wav",
    "Sound/Default/powerdown.wav",
    "Sound/Default/advised.wav",
    "Sound/Default/agegender.wav",
    "Sound/Default/dis-agegender.wav",
    "Sound/Default/dis-emotional.wav",
    "Sound/Default/emotional.wav",
    "Sound/Default/found-person.wav",
    "Sound/Default/objective.wav",
    "Sound/Default/searchmode-off.wav",
    "Sound/Default/searchterminated.wav",
    
    "Sound/Sayings/better.wav",
    "Sound/Sayings/compute.wav",
    "Sound/Sayings/cross.wav",
    "Sound/Sayings/danger.wav",
    "Sound/Sayings/directive.wav",
    "Sound/Sayings/humans.wav",

    "Sound/Sayings/cautionroguerobots.mp3",
    "Sound/Sayings/chess.mp3",
    
    "Sound/Sayings/dangerwillrobinson.mp3",
    "Sound/Sayings/malfunction.mp3",
    "Sound/Sayings/nicesoftware.mp3",
    "Sound/Sayings/no5alive.mp3",
    "Sound/Sayings/program.wav",
    "Sound/Sayings/selfdestruct.wav",
    "Sound/Sayings/shallweplayagame.mp3",
    
    "Sound/Sayings/comewithme.mp3",
    "Sound/Sayings/gosomewhere.mp3",
    "Sound/Sayings/hairybaby.mp3",
    "Sound/Sayings/lowbattery.mp3",
    "Sound/Sayings/robotnotoffended.mp3",
    "Sound/Sayings/satisfiedwithmycare.wav",
    "Sound/Sayings/waitbeforeswim.mp3",
    
    "Sound/Sayings/silly.wav",
    "Sound/Sayings/stare.wav",
    "Sound/Sayings/world.wav",
    
    
       
    "Sound/2.mp3",
    "Sound/Powerup/Powerup_chirp2.mp3",
    "Sound/Randombeeps/Questioning_computer_chirp.mp3",
    "Sound/Randombeeps/Double_beep2.mp3",
    "Sound/Powerdown/Long_power_down.mp3",
    "Sound/Radarscanning/Radar_bleep_chirp.mp3",
    "Sound/Radarscanning/Radar_scanning_chirp.mp3",
    "Sound/celebrate1.mp3",
    "Sound/Randombeeps/Da_de_la.mp3",
]

class SoundManger:
    def __init__(self, config, file_paths: Optional[List[str]] = None) -> None:
        pygame.mixer.pre_init(48000, -16, 8, 8192)# initialise music,sound mixer
        pygame.mixer.init()
        if file_paths is not None:
            file_paths += _default_filepaths
        else:
            file_paths = _default_filepaths
        #self.sounds = {
           # pathlib.Path(f).stem: pygame.mixer.Sound(f) for f in file_paths
       # }
        self.channel = pygame.mixer.Channel(1)

    def play_sound(self, sound):
        video_path='DefaultSayings/'+sound+'.avi'
        sound_path='DefaultSayings/'+sound+'.wav'
        file_name = video_path
        window_name = "RobotFace"
        interframe_wait_ms = 1

        cap = cv2.VideoCapture(file_name)
        if not cap.isOpened():
            print("Error: Could not open video.")
            exit()
            
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
        #cv2.resizeWindow(window_name, 800, 600)
        mixer.music.load(sound_path)
        mixer.music.play()

        while (True):
            ret, frame = cap.read()
            key = cv2.waitKey(1)
            if not ret:
                print("Reached end of video, exiting.")
                break

            cv2.imshow(window_name, frame)
            if key & 0x7F == ord('q'):
                print("Exit requested.")
                break

        cap.release()
        cv2.destroyAllWindows()
    

#clip = VideoFileClip('DefaultSayings/'+sound+'.avi')
        #clipresized = clip.resize (height=100)
        #clipresized.preview(fullscreen=True)
        #video = moviepy.editor.VideoFileClip("")
        #clip.ipython_display()
        #self.channel.play(self.sounds[sound])
        #self.sounds[sound].play()

# === ./src/old/operational_modes_moded_by_daniel.py ===
import time
import operator
import depthai as dai
import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images 
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
from .operational_mode import OperationalMode
import UltraBorg_old
import board
import adafruit_bno055

# Board #1, address 10
UB1 = UltraBorg_old.UltraBorg()
UB1.i2cAddress = 10
UB1.Init()
# Board #2, address 11
UB2 = UltraBorg_old.UltraBorg()
UB2.i2cAddress = 11
UB2.Init()
# Board #3, address 12
UB3 = UltraBorg_old.UltraBorg()
UB3.i2cAddress = 12
UB3.Init()
# Board #4, address 13
#UB4 = UltraBorg.UltraBorg()
#UB4.i2cAddress = 13
#UB4.Init()
UB1.SetWithRetry(UB1.SetServoMaximum1, UB1.GetServoMaximum1, 4520, 5)
UB1.SetWithRetry(UB1.SetServoMinimum1, UB1.GetServoMinimum1, 1229, 5)
UB1.SetWithRetry(UB1.SetServoStartup1, UB1.GetServoStartup1, 3100, 5)
UB1.SetWithRetry(UB1.SetServoMaximum2, UB1.GetServoMaximum2, 3026, 5)
UB1.SetWithRetry(UB1.SetServoMinimum2, UB1.GetServoMinimum2, 1592, 5)
UB1.SetWithRetry(UB1.SetServoStartup2, UB1.GetServoStartup2, 2408, 5)
UB2.SetWithRetry(UB2.SetServoMaximum1, UB2.GetServoMaximum1, 4951, 5)
UB2.SetWithRetry(UB2.SetServoMinimum1, UB2.GetServoMinimum1, 1076, 5)
UB2.SetWithRetry(UB2.SetServoStartup1, UB2.GetServoStartup1, 2949, 5)
UB2.SetWithRetry(UB2.SetServoMaximum2, UB2.GetServoMaximum2, 5247, 5)
UB2.SetWithRetry(UB2.SetServoMinimum2, UB2.GetServoMinimum2, 1136, 5)
UB2.SetWithRetry(UB2.SetServoStartup2, UB2.GetServoStartup2, 3130, 5)
UB2.SetWithRetry(UB2.SetServoMaximum3, UB2.GetServoMaximum3, 4561, 5)
UB2.SetWithRetry(UB2.SetServoMinimum3, UB2.GetServoMinimum3, 989, 5)
UB2.SetWithRetry(UB2.SetServoStartup3, UB2.GetServoStartup3, 2737, 5)

# SPDX-FileCopyrightText: 2021 ladyada for Adafruit Industries
# SPDX-License-Identifier: MIT
i2c = board.I2C()  # uses board.SCL and board.SDA
# i2c = board.STEMMA_I2C()  # For using the built-in STEMMA QT connector on a microcontroller
sensor = adafruit_bno055.BNO055_I2C(i2c)
last_val = 0xFFFF
def temperature():
    global last_val  # pylint: disable=global-statement
    result = sensor.temperature
    if abs(result - last_val) == 128:
        result = sensor.temperature
        if abs(result - last_val) == 128:
            return 0b00111111 & result
    last_val = result
    return result
calibrated = False
#calibrated = sensor.calibrated()
print("Cal status (S,G,A,M):{}".format(sensor.calibration_status))
# Read the calibration status, 0=uncalibrated and 3=fully calibrated.
print("Temperature: {} degrees C".format(temperature()))  # Uncomment if using a Raspberry Pi
#print("Accelerometer (m/s^2): {}".format(sensor.acceleration))
#print("Magnetometer (microteslas): {}".format(sensor.magnetic))
#print("Gyroscope (rad/sec): {}".format(sensor.gyro))
print("Euler angle: {}".format(sensor.euler))
#print("Quaternion: {}".format(sensor.quaternion))
#print("Linear acceleration (m/s^2): {}".format(sensor.linear_acceleration))
#print("Gravity (m/s^2): {}".format(sensor.gravity))
#print(sensor.calibrated)

global stop_distance,obdis, colour
colour=(255,255,255)
stop_distance=400
obdis=400
# INITIALIZE MOTOR CONTROLER
# ************************************************************************************************
#mc = motoron.MotoronI2C()
# Reset the controller to its default settings, then obdisable CRC.  The bytes for
# each of these commands are shown here in case you want to implement them on
# your own without using the library.
#mc.reinitialize()  # Bytes: 0x96 0x74
#mc.obdisable_crc()   # Bytes: 0x8B 0x04 0x7B 0x43
# Clear the reset flag, which is set after the controller reinitializes and
# counts as an error.
#mc.clear_reset_flag()  # Bytes: 0xA9 0x00 0x04
# By default, the Motoron is configured to stop the motors if it does not get
# a motor control command for 1500 ms.  You can uncomment a line below to
# adjust this time or obdisable the timeout feature.
# mc.set_command_timeout_milliseconds(1000)
# mc.obdisable_command_timeout()
# Configure motor 1
#mc.set_max_acceleration(1, 40)
#mc.set_max_deceleration(1, 40)


class RemoteControlMode(OperationalMode):
    # ALIGNMENT  attaching servo horns the screw is positioned over the sticker on the servo
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def get_key(self) -> str:
        return 'x'
    
    def get_state(self) -> RobotState:
        return RobotState.REMOTE_CONTROL
    
    def _start(self, robot):
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        #self.sound_manager.play_sound("agegender")
        #self.animate(robot.config.animations[4])
        robot.say("Remote control mode enabled")
        print("Remote control enabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Double_beep2")

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/obdisagegender/')
        #self.sound_manager.play_sound("obdisagegender")
        #self.animate(robot.config.animations[5])
        robot.say("Remote Control Mode has been disabled")
        print("Remote control disabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        #robot.animate(1)
        #engine.runAndWait()
        time.sleep(1.6)
        robot.play_sound("Long_power_down")
        
    def _main(self, robot):
        Direction='neutral'
        keyup=0
        x=0
        y=0
        running = True
        UB2.SetServoPosition3(0)
        hp=0
        pantilt=0
        k_w,k_a,k_s,k_d,k_q,k_e,k_z,k_x,k_l,k_r,k_o,k_p = 0,0,0,0,0,0,0,0,0,0,0,0
        while running:
            events = pygame.event.get()
            for event in events:
                #print('this is keyup at start of loop ',keyup)
                if event.type == pygame.KEYUP:
                    keyup=1
                    if event.key == pygame.K_w:
                        k_w = 0
                    if event.key == pygame.K_a:
                        k_a = 0
                    if event.key == pygame.K_s:
                        k_s = 0
                    if event.key == pygame.K_d:
                        k_d = 0
                    if event.key == pygame.K_q:
                        k_q = 0
                    if event.key == pygame.K_e:
                        k_e = 0
                    if event.key == pygame.K_z:
                        k_z = 0
                    if event.key == pygame.K_x:
                        k_x = 0
                    if event.key == pygame.K_l:
                        k_l = 0
                    if event.key == pygame.K_r:
                        k_r = 0
                if event.type == pygame.QUIT:
                    _exit()
                    pygame.quit()
                    running = False
                    UB2.SetServoPosition1(0)    #set servos to default positions on exit     
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition3(0)
                    UB1.SetServoPosition1(0)
                if event.type == pygame.KEYDOWN:
                    keyup=0
                    if event.key == pygame.K_ESCAPE:
                        UB2.SetServoPosition1(0)   #set servos to default positions on exit  
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition3(0)
                        UB1.SetServoPosition1(0)
                        running = False
                    if event.key == pygame.K_q:
                        k_q = 1
                    if event.key == pygame.K_e:
                        k_e = 1
                    if event.key == pygame.K_w:
                        k_w = 1
                    if event.key == pygame.K_a:
                        k_a = 1
                    if event.key == pygame.K_s:
                        k_s = 1
                    if event.key == pygame.K_d:
                        k_d = 1
                    if event.key == pygame.K_z:
                        k_z = 1
                    if event.key == pygame.K_x:
                        k_x = 1
                    if event.key == pygame.K_b:
                        k_b = 1
                    if event.key == pygame.K_f:
                        k_f = 1
                    if event.key == pygame.K_l:
                        k_l = 1
                    if event.key == pygame.K_r:
                        k_r = 1
                if k_w:
                    #print("w")
                    Direction ='forward'
                    x=-0.95
                    y=-0.12
                if k_a:
                    #print("a")
                    Direction ='left'# anti-clockwise euler goes down
                    x=0.1
                    y=0.98
                if k_d:
                    #print("d")
                    Direction='right'# clockwise euler goes up
                    x=-0.26
                    y=-1.0
                if k_q:
                    #print("q")
                    Direction ='left-forward'
                    x=-0.9
                    y=0.1
                if k_e:
                    #print("e")
                    Direction ='right-forward'
                    x=-0.9
                    y=-0.3
                if k_s:
                    #print("s")
                    Direction='back'
                    x=0.98
                    y=0.24
                if k_z:
                    #print("z")
                    Direction ='left-reverse'
                    x=0.98
                    y=-0.02
                if k_x:
                    #print("x")
                    Direction ='right-reverse'
                    x=0.98
                    y=0.4
                if k_l:
                    #print("Head Left")
                    Direction='Head left'
                    UB2.SetServoPosition4(0)
                    if hp <-0.95:
                        hp=-0.95
                    else:
                        hp=hp-0.02
                #print(hp)
                if k_r:
                    #print("Head Right")
                    Direction='Head right'
                    if hp > 0.95:
                        hp=0.95
                    else:
                        hp=hp+0.02
                if keyup==1:
                    Direction='neutral'
                    x=0
                    y=0
                UB2.SetServoPosition2(x)
                UB2.SetServoPosition1(y)
                UB2.SetServoPosition3(hp)
                #UB1.SetServoPosition1(pantilt)
                
class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(images)
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+20, y-96), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 1)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/obdisagegender/')
        self.sound_manager.play_sound("obdisagegender")
        self.animate(images)
        #robot.say("Age and Gender Detection obdisabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will obdisplay spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't obdisplay spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(images)
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            responses = ['neutral', 'happy', 'sad', 'surprise', 'anger']
            neutral,happy,sad,surprise,anger=0,0,0,0,0
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]
                    
                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 1)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, emotion_name, (bbox[0]+10, y-110), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 7)
                        cv2.putText(frame, emotion_name, (bbox[0], y-90), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 1)
                        if emotion_name=="neutral":
                            neutral=neutral +1
                        if emotion_name=="happy":
                            happy=happy +1
                        if emotion_name=="sad":
                            sad=sad +1
                        if emotion_name=="surprise":
                            surprise=surprise +1
                        if emotion_name=="anger":
                            anger=anger +1
                        #print(emotion_name)    
                        
                            #self.animate(animations[7])
                            #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    if neutral>20 or happy>20 or sad>20 or surprise>20 or anger>20:
                            max_response = max(zip(responses, (map(eval, responses))), key=lambda tuple: tuple[1])[0]
                            self.sound_manager.play_sound(max_response)
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/obdisemotional/')
        self.sound_manager.play_sound("obdisemotional")
        self.animate(images)
        #robot.say("Emotion Detection state obdisabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will obdisplay spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't obdisplay spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS

class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = ['person', 'cup',]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
    
    def obstacleAvoid(self, avoided, robot):
        mainloop=True
        rx=0
        ry=0
        forward_count=0
        turn_count=0
        turn=0
        move_stage = 0
        while mainloop:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    mainloop = False
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        mainloop = False
                        break
            us1,us2,us3,us4=int(UB2.GetDistance1()),int(UB2.GetDistance2()),int(UB2.GetDistance3()),int(UB2.GetDistance4())
            usr1,usr2,usr3,usr4=int(UB3.GetDistance1()),int(UB3.GetDistance2()),int(UB3.GetDistance3()),int(UB3.GetDistance4())
            if us1 == 0:
                us1=9999
            if us2 == 0:
                us2=9999
            if us3 == 0:
                us3=9999
            if us4 ==0:
                us4=9999
            if usr1 == 0:
                usr1=9999
            if usr2 == 0:
                usr2=9999
            if usr3 == 0:
                usr3=9999
            if usr4 ==0:
                usr4=9999
            print (us1, "  ",us2, " ", us3, " ", us4," Front sensors")
            print (usr1, "  ",usr2, " ", usr3, " ", usr4," Rear sensors")
            
            
            if (us1<obdis or us2<obdis or us3<obdis or us4<obdis):
                turn_count = 0
                forward_count = 0
                
                maxdis = robot.config.us_max_dis
                mindis = robot.config.us_min_dis
                
                usmaxs = [us1 < maxdis, us2 < maxdis, us3 < maxdis, us4 < maxdis]
                usmins = [us1 < mindis, us2 < mindis, us3 < mindis, us4 < mindis]                
                
                
                if (not(min(usmins))):
                    total = usmaxs[0] * -1 + usmaxs[1] * -2 + usmaxs[2] * 2 + usmaxs[3] * 1
                    if (total < 0):
                        turn = -0.5
                    else:
                        turn = 0.5
                else:
                    total = usmins[0] * -1 + usmins[1] * -2 + usmins[2] * 2 + usmins[3] * 1
                    if (sum(usmins[:4]) > 3):
                        turn = 2
                    elif (total < 0):
                        turn = -1
                    else:
                        turn = 1
                
                
                
                

            elif (turn_count < 2):
                turn_count += 1
            
            elif (forward_count<6):
                
                # drive forward as NO objects have been detected
                #robot.say("forward")
                print("moving forward")
                turn=0
                
                forward_count += 1
            else:
                turn_count=0
                forward_count=0
                mainloop=False
            
            
            
            
            match turn:
                case -0.5:
                    rx = -0.9
                    ry = 0.1
                case 0.5:
                    rx = -0.9
                    ry = -0.3
                case -1:
                    rx = 0.1
                    ry = 0.98
                case 0:
                    rx = -0.95
                    ry = -0.12
                case 1:
                    rx = 0.1
                    ry = 0.98
                case 2:
                    rx = 0.98
                    ry = 0.24
                case _:
                    rx = 0
                    ry = 0
            
            UB2.SetServoPosition2(rx)
            UB2.SetServoPosition1(ry)
            time.sleep(0.15)
            
 
    def avoid(self, avoided, robot):
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str=str(us1)
        us2_str=str(us2)
        us3_str=str(us3)
        us4_str=str(us4)
        #r_person_str=str(r_person)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        print('........................... obstacle detected')
        print('attempting to avoid obstacle')
        #robot.say("Obstacle detected")
        images=load_images('/home/raspberrypi/MiniMax/Animations/inmyway/')
        self.sound_manager.play_sound("inmyway")
        preObsHeading, roll, pitch = sensor.euler #left euler goes down. Righr euler goes up.
        print("Before obstacle avoided heading",preObsHeading)
        self.obstacleAvoid(avoided, robot)
        afterObsHeading, roll, pitch = sensor.euler
        print("after obstacle avoided heading",afterObsHeading)
        diff1 = int(preObsHeading - afterObsHeading)
        diff2 = abs(diff1)
        while diff2>1:
            afterObsHeading, roll, pitch = sensor.euler#aproximate same heading
            
            dif = preObsHeading - afterObsHeading
            if (dif < -180 or (dif > 0 and dif < 180)):
                x = -0.26
                y = 1
            else:
                x = 0.1
                y = 0.98
            
            
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        break
            UB2.SetServoPosition2(x)
            UB2.SetServoPosition1(y)
        print("back to original angle +- 1 degrees")
        UB2.SetServoPosition2(0)
        UB2.SetServoPosition1(0)
        #cv2.destroyAllWindows()
        r_person=0  
    
    def _exit():
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)

    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #THIS WAS USED, TO SAY "TO GIVE A BISCUIT TO"
            self.animate(images) #robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label} who like jellybeans")
        else:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #This is used if normal search person mode is activated.
            self.animate(images) #robot.config.animations[19])
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        
    def stop(self, robot):
        print('First stop check')
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str=str(us1)
        us2_str=str(us2)
        us3_str=str(us3)
        us4_str=str(us4)
        #r_person_str=str(r_person)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        #print('r_person = '+r_person_str)
        #pin="2z"
        #robot.write_serial(pin)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        #print('Old pin '+old_pin +' new pin '+pin)
        print('........................... Objective Reached')
        robot.say("Objective reached")
        cv2.destroyAllWindows()
        r_person=0
    
    def _main(self, robot):
        global old_pin, pin, z, r_person, current_z, current_z_str,us1,us2,us3,us4,us1_str,us2_str,us3_str,us4_str,r_person_str,rx,ry
        pipeline = self._create_pipeline2(robot)
        pin, r_person, z, current_z, stop_distance,rx,ry = '2z', 0, 9999,9999,700,0,0
        #found_people[0] = {'tid': 99999, 'objxcenter': 0, 'objycenter': 0, 'status': 'TRACKED', 'z_depth': 1000}
        with dai.Device(pipeline) as device:
            preview = device.getOutputQueue("preview", 4, False)
            tracklets = device.getOutputQueue("tracklets", 4, False)
            startTime = time.monotonic()
            counter, fps, frame = 0, 0, None
            found_people=[{'tid': 666, 'objxcenter': 60,'status':'LOST','z_depth':9000}]
            sorted_x={}
            while True:
                us1,us2,us3,us4=int(UB2.GetDistance1()),int(UB2.GetDistance2()),int(UB2.GetDistance3()),int(UB2.GetDistance4())
                if us1 == 0:
                    us1=99999
                if us2 == 0:
                    us2=99999
                if us3 == 0:
                    us3=99999
                if us4 ==0:
                    us4=99999
                imgFrame = preview.get()
                track = tracklets.get()
                counter+=1
                current_time = time.monotonic()
                if (current_time - startTime) > 1 :
                    fps = counter / (current_time - startTime)
                    counter = 0
                    startTime = current_time
                frame = imgFrame.getCvFrame()
                #frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST) # this line slows down everything!
                trackletsData = track.tracklets
                if current_z==0:
                    current_z=9999
                # us = Ultrasonic sensor , z = distance to object measured by depth camera.
                if current_z < stop_distance:
                            self.stop(robot)
                            r_person=0
                            return True
                if us1 < 450 or us2 < 450 or us3 < 450 or us4 <450: #or current_z < stop_distance:object is close  
                    r_person=r_person+1
                    if r_person>=2:
                        avoided=False
                        self.avoid(avoided, robot)
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        r_person=0
                        # do everything needed when robot finds obstacle
                else:
                    r_person=0
                found_peeps=0    
                n_of_track_peeps=0
                for t in trackletsData:
                    found_peeps+=1
                    roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
                    x1,y1 = int(roi.topLeft().x), int(roi.topLeft().y)
                    x2,y2 = int(roi.bottomRight().x), int(roi.bottomRight().y)
                    z=int(t.spatialCoordinates.z)
                    xmin, ymin = x1, y1
                    xmax, ymax = x2, y2
                    x_diff, y_diff = (xmax-xmin), (ymax-ymin)
                    obj_x_center = int(xmin+(x_diff/2))
                    #obj_y_center=int(ymin+(y_diff/2))
                    #dictionary to be updated
                    dict = {'tid': t.id, 'objxcenter': obj_x_center, 'status': t.status.name, 'z_depth': z}
                    #print(' Found peeps ',found_peeps)
                    if t.status.name=='TRACKED':
                        n_of_track_peeps+=1
                        found_people.append(dict)
                if n_of_track_peeps <1:
                    pass
                    # no tracked people in current frame
                    #robot.say("No people detected") says this WAY too many times as detection may not always work.
                    #print('no peeps')
                else:
                    sorted_x=min((x for x in found_people if x['status'] == "TRACKED"), key=lambda x:x['tid'], default=None)
                    #print('sorted x is ',sorted_x)
                    found_people=[{'tid': 666, 'objxcenter': 60,'status':'LOST','z_depth':9000}]
                    obj_x_center= sorted_x['objxcenter'] # get x co-ordinate of lowest ID
                    #obj_y_center= sorted_x['objycenter'] # get y co-ordinate of lowest ID
                    current_z = sorted_x['z_depth'] # get z-depth co-ordinate of lowest ID
                    if current_z==0:
                        current_z=9999
                    x_deviation = int(robot.config.ps_xres/2)-obj_x_center
                    # calculate the deviation from the center of the screen
                    if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                        if current_z < stop_distance:
                            self.stop(robot)
                            r_person=0
                            return True
                        if min(us1, us2, us3, us4) < robot.config.us_max_dis:#object is very close 
                            print('Second stop check')
                            avoided=False
                            self.avoid(avoided, robot)
                            UB2.SetServoPosition2(0)
                            UB2.SetServoPosition1(0)
                            r_person=0
                        else:
                            r_person=0
                            rx = -0.95
                            ry = -0.12 # move forward
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #break
                            #print("........................... moving robot FORWARD")
                    else:
                        if (x_deviation>robot.config.ps_tolerance):
                            if x_deviation< robot.config.ps_far_boundry:
                                r_person=0
                                robot.write_serial(pin)
                                rx=-0.9
                                ry=0.1
                                UB2.SetServoPosition2(rx)
                                UB2.SetServoPosition1(ry)
                                #break
                                #print('........... turning left while moving forward' )
                            if x_deviation>=robot.config.ps_far_boundry:
                                r_person=0
                                rx=0.1
                                ry=0.98
                                UB2.SetServoPosition2(rx)
                                UB2.SetServoPosition1(ry)
                                #break
                                #print('..... turning left on the spot' )
                        elif ((x_deviation*-1)>robot.config.ps_tolerance):
                            if abs(x_deviation)<robot.config.ps_far_boundry:
                                r_person=0
                                rx=-0.9
                                ry=-0.3
                                UB2.SetServoPosition2(rx)
                                UB2.SetServoPosition1(ry)
                                #break
                                #print('............ turning right while moving forward' )
                            if abs(x_deviation)>=robot.config.ps_far_boundry:
                                r_person=0
                                rx=-0.26
                                ry=-1.0
                                UB2.SetServoPosition2(rx)
                                UB2.SetServoPosition1(ry)
                                #break
                                #print('..... turning right on the spot' )
                cv2.putText(frame, "fps: {:.2f}".format(fps), (2, frame.shape[0] - 7), cv2.FONT_HERSHEY_TRIPLEX, 0.6, colour)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    pin='5z'
                    #robot.write_serial(pin)
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(images) #robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    return False
                #frame=cv2.resize(frame,(600,400),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                cv2.imshow("tracker", frame)

    def _create_pipeline2(self, robot):
        # Create pipeline
        pipeline = dai.Pipeline()
        
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        
        camRgb.setPreviewKeepAspectRatio(False)#
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        objectTracker = pipeline.create(dai.node.ObjectTracker)

        xoutRgb = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)
        #xoutDepth = pipeline.create(dai.node.XLinkOut) # test may remove
        
        xoutRgb.setStreamName("preview")
        trackerOut.setStreamName("tracklets")
        #xoutDepth.setStreamName("depth") # test may remove
        
        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_2024X1520)#
        camRgb.setInterleaved(False)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        #camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        #camRgb.setFps(15)
        
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)#
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)#
        monoRight.setCamera("right")

        # setting node configs
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        # Align depth map to the perspective of RGB camera, on which inference is done
        stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
        stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())
        
        # Better handling for occlusions:
        stereo.setLeftRightCheck(True)
        # Closer-in minimum depth, disparity range is doubled:
        stereo.setExtendedDisparity(False)
        # Better accuracy for longer distance, fractional disparity 32-levels:
        stereo.setSubpixel(False)

        spatialDetectionNetwork.setBlobPath('/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')
        spatialDetectionNetwork.setConfidenceThreshold(0.70)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)
        
        objectTracker.setDetectionLabelsToTrack([15])  # track only person
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        camRgb.preview.link(spatialDetectionNetwork.input)
        objectTracker.passthroughTrackerFrame.link(xoutRgb.input)
        objectTracker.out.link(trackerOut.input)
        
        spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)
        #objectTracker.inputTrackerFrame.setBlocking(False)# testing
        spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        spatialDetectionNetwork.out.link(objectTracker.inputDetections)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        return pipeline
   
    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                images=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animate(images) #robot.config.animations[8])
                #robot.say(f'I think I found a {self.label}')
            else:
                images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images) #robot.config.animations[11])
                robot.say("what is going on")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images) #robot.config.animations[11])
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                images=load_images('/home/raspberrypi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(images) #robot.config.animations[9])
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)
                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)
        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))
            time.sleep(1)
        robot.say('The jelly beans are leaving now bye bye')
        #robot.animate(1)
        robot.write_serial('9z')  # make robot do a 180 degree turn
        robot.write_serial('2z')  # stop robot
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT

# === ./src/old/operational_mode.py ===
from abc import ABC, abstractmethod
from .utils import RobotState

class OperationalMode(ABC):

    @abstractmethod
    def run(self, robot):
        pass

    @abstractmethod
    def get_key(self) -> str:
        pass

    @abstractmethod
    def get_state(self) -> RobotState:
        pass



# === ./src/old/robot.py ===
# built-in imports
import time
import logging
import random
import subprocess
import serial
import subprocess
import os
import numpy as np
global animations, images, imp, display, luckystrike
import pygame
# local imports
from .operational_modes import ObjectSearchOperationMode, RemoteControlMode
from .ageAndEmotion import  AgeGenderOperationalMode, EmotionOperationalMode
from .sound_manager import SoundManger
from .animation_manager import AnimationManager, load_images
from .tts_manager import TTSManager
from .utils import RobotConfig, RobotState, InputObject
luckystrike=0

sayings=[]
sayings.append("better")
sayings.append("compute")
sayings.append("cross")
sayings.append("danger")
sayings.append("directive")
sayings.append("humans")
sayings.append("cautionroguerobots")
sayings.append("chess")
sayings.append("dangerwillrobinson")
sayings.append("malfunction2")
sayings.append("nicesoftware2")
sayings.append("no5alive")
sayings.append("program")
sayings.append("selfdestruct")
sayings.append("shallweplayagame")
sayings.append("silly")
sayings.append("stare")
sayings.append("world")
sayings.append("comewithme")
sayings.append("gosomewhere2")
#sayings.append("hairybaby")
sayings.append("lowbattery")
sayings.append("robotnotoffended")
sayings.append("satisfiedwithmycare")
sayings.append("waitbeforeswim")
sayings.append("helpme")

#closedmouth = pygame.image.load('/home/raspberrypi/MiniMax/Animations/advised/outputFile_001.jpg').convert()
# The Robot class is the main driver of the entire application. It stores the main run loop, all operational
# states (which control the behaviour of the robot), and all application managers which control sounds, TTS,
# animation, and engine control via the serial port. 
#
# The aim of a main driver class like this is to keep things as flexible as possible, with most application
# code abstracted away. For example, say we wanted to add a new behavior to the robot - we have a new depth AI
# feature we want implemented. In that case, we would want to create a new OperationalMode and add it to the list
# of OperationMode's stores in the self.operation_modes list created in the Robot initialiser, as opposed to 
# e.g. create a new function called new_depthai_feature() in the robot class. This way we can create as many
# new features as we want to the application without pollution this main driver class. The end product should
# be one where the different features stay out of each other's way, and any issues are much easier to test and 
# debug.
def playvideo(animatedvideo):
    subprocess.call(['cvlc', '--fullscreen', animatedvideo, '--play-and-exit'])

class Robot:
    def __init__(self, config: RobotConfig = None) -> None:
        self._setup_logging()
        #logging.info("Starting Robot")
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        #logging.debug("Starting TTSManager")
        self.tts_manager = TTSManager(config=self.config)
        #logging.debug("Starting Serial Port")
        #self.port = serial.Serial("/dev/ttyAMA0", baudrate=self.config.port_baudrate, timeout=None)
        # load up all the operational states here. 
        #logging.debug("Creating Operational Modes")
        self.operational_modes = [
            AgeGenderOperationalMode(),
            EmotionOperationalMode(),
            ObjectSearchOperationMode(biscuit_mode=False, label='person'),
            ObjectSearchOperationMode(biscuit_mode=True, label='person'),
            ObjectSearchOperationMode(biscuit_mode=False, label='cup'),
            ObjectSearchOperationMode(biscuit_mode=True, label='cup'),
            RemoteControlMode(),
        ]
    def animate(self, images):
        self.animator.animate(images)
        
    def load(self):
        self.animator.load_animations()
        
    def run(self):
        luckystrike=0
        self.start()
        # logging.info("Starting run")
        # start the robot in IDLE
        state = RobotState.IDLE
        # state = RobotState.CUP_SEARCH_NO_GIVE_BISCUIT

        keep_running = True
        
        while keep_running:
            # logging.debug(f"Step on current state: {state}")
            state = self.step(state)
            if state is None:
                # if for some reason our step does not set a new state, put the robot
                # into idle
                state = RobotState.IDLE
            elif state == RobotState.EXIT:
                # if state is switched to EXIT, then exit out of the loop
                keep_running = False
        self.exit()
    
    def start(self):
        # take care of any startup activities here
        #logging.info("Starting")
        images=load_images('/home/raspberrypi/MiniMax/Animations/greetings/')
        #self.say("Greetings Humans")
        #playvideo('/home/raspberrypi/MiniMax/DefaultSayings/greetings.mp4')
        self.sound_manager.play_sound("greetings")
        self.animate(images) #self.config.animations[0])
        
    def exit(self):
        #logging.info("Exiting")
        # take care of any close down activities here.
        images=load_images('/home/raspberrypi/MiniMax/Animations/powerdown/')
        self.sound_manager.play_sound("powerdown")
        self.animate(images)
        
        #self.config.animations[2])
        #playvideo('/home/raspberrypi/MiniMax/DefaultSayings/powerdown.mp4')
        #self.sound_manager.play_sound("powerdown")
        #pygame.display.flip()
        #self.animate(1)
        #images=[]
        time.sleep(1.5)

    def step(self, state):
        global luckystrike
        
        # logging.debug("Getting inputs")
        inputs = self._get_inputs()

        # logging.debug(inputs.pressed_keys)

        # Special Case States
        if state == RobotState.IDLE:
            luckynumb=random.randint(0,1100000)
            if luckynumb==7000:
                luckystrike+=1
            if luckystrike>2:
                # put random saying in here
                randsay=random.randint(0,24)
                images=load_images('/home/raspberrypi/MiniMax/Animations/'+sayings[randsay]+'/')
                self.sound_manager.play_sound(sayings[randsay])
                self.animate(images)
                luckystrike=0
            # logging.debug("Processing idle state")
            # If we're in IDLE, we want to check whether we should be switching across to 
            # any other operational modes, or quitting. If so, return the relevant state.
            # If there is no input, then return IDLE again so that we keep idling.
            if 'q' in inputs.pressed_keys:
                # exit the entire program if q is pressed after playing powerdown voice
                return RobotState.EXIT
            # Operational Mode States
            # We want to loop through all the operational states currently loaded into the robot.
            # For each loaded state, we check whether the relevant trigger key has been set. If it
            # has, then we return the relevant RobotState so that we know to trigger that operational
            # state in the next call to step()
            for om in self.operational_modes:
                if om.get_key() in inputs.pressed_keys:
                    return om.get_state()
            
            return RobotState.IDLE
        
        elif state == RobotState.EXIT:
            # shouldn't actually be able to get here, but take care of it just in case
            return RobotState.EXIT

        # logging.debug("Processing operational mode states")
        # Operational Mode States
        for om in self.operational_modes:
            if om.get_state() == state:
                return om.run(self)

    def _get_inputs(self) -> InputObject:
        inputs = InputObject()

        return inputs
    
    def say(self, string):
        self.tts_manager.say(string)

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)

    def animate(self, images):
        self.animator.animate(images)

    def write_serial(self, pin):
        self.port.write(str.encode(pin))

    def _setup_logging(self):
        logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler( "logs/debug.log", mode='w'),
            logging.StreamHandler()
            ]
        )

# === ./src/old/depth_spatial_mobilenet.py ===
import cv2
import depthai as dai
import numpy as np
import time

pin, r_person, z, current_z, stop_distance,rx,ry = '2z', 0, 9999,9999,700,0,0
nnPath = '/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob'

labelMap = ["background", "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
    "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]
syncNN = True
# Create pipeline
pipeline = dai.Pipeline()
# Define sources and outputs
camRgb = pipeline.create(dai.node.ColorCamera)
spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
monoLeft = pipeline.create(dai.node.MonoCamera)
monoRight = pipeline.create(dai.node.MonoCamera)
stereo = pipeline.create(dai.node.StereoDepth)

xoutRgb = pipeline.create(dai.node.XLinkOut)
xoutNN = pipeline.create(dai.node.XLinkOut)
#xoutDepth = pipeline.create(dai.node.XLinkOut)

xoutRgb.setStreamName("rgb")
xoutNN.setStreamName("detections")
#xoutDepth.setStreamName("depth")

# Properties
camRgb.setPreviewSize(300, 300)
camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_2024X1520)#THE_2024X1520
camRgb.setInterleaved(False)

camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
camRgb.setPreviewKeepAspectRatio(False)
monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
monoLeft.setCamera("left")
monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)
monoRight.setCamera("right")

# Setting node configs
stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
# Align depth map to the perspective of RGB camera, on which inference is done
stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
stereo.setSubpixel(False)
stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())

spatialDetectionNetwork.setBlobPath(nnPath)
spatialDetectionNetwork.setConfidenceThreshold(0.7)
spatialDetectionNetwork.input.setBlocking(False)
spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
spatialDetectionNetwork.setDepthLowerThreshold(100)
spatialDetectionNetwork.setDepthUpperThreshold(5000)

# Linking
monoLeft.out.link(stereo.left)
monoRight.out.link(stereo.right)

camRgb.preview.link(spatialDetectionNetwork.input)
if syncNN:
    spatialDetectionNetwork.passthrough.link(xoutRgb.input)
else:
    camRgb.preview.link(xoutRgb.input)

spatialDetectionNetwork.out.link(xoutNN.input)

stereo.depth.link(spatialDetectionNetwork.inputDepth)
#spatialDetectionNetwork.passthroughDepth.link(xoutDepth.input)

# Connect to device and start pipeline
with dai.Device(pipeline) as device:

    # Output queues will be used to get the rgb frames and nn data from the outputs defined above
    previewQueue = device.getOutputQueue(name="rgb", maxSize=4, blocking=False)
    detectionNNQueue = device.getOutputQueue(name="detections", maxSize=4, blocking=False)
    #depthQueue = device.getOutputQueue(name="depth", maxSize=4, blocking=False)

    startTime = time.monotonic()
    counter = 0
    fps = 0
    color = (255, 255, 255)

    while True:
        inPreview = previewQueue.get()
        inDet = detectionNNQueue.get()
        #depth = depthQueue.get()

        counter+=1
        current_time = time.monotonic()
        if (current_time - startTime) > 1 :
            fps = counter / (current_time - startTime)
            counter = 0
            startTime = current_time

        frame = inPreview.getCvFrame()

        #depthFrame = depth.getFrame() # depthFrame values are in millimeters

        #depth_downscaled = depthFrame[::4]
        #if np.all(depth_downscaled == 0):
            #min_depth = 0  # Set a default minimum depth value when all elements are zero
        #else:
            #min_depth = np.percentile(depth_downscaled[depth_downscaled != 0], 1)
        #max_depth = np.percentile(depth_downscaled, 99)
        #depthFrameColor = np.interp(depthFrame, (min_depth, max_depth), (0, 255)).astype(np.uint8)
        #depthFrameColor = cv2.applyColorMap(depthFrameColor, cv2.COLORMAP_HOT)

        detections = inDet.detections

        # If the frame is available, draw bounding boxes on it and show the frame
        height = frame.shape[0]
        width  = frame.shape[1]
        for detection in detections:
            roiData = detection.boundingBoxMapping
            roi = roiData.roi
            #roi = roi.denormalize(depthFrameColor.shape[1], depthFrameColor.shape[0])
            topLeft = roi.topLeft()
            bottomRight = roi.bottomRight()
            xmin = int(topLeft.x)
            ymin = int(topLeft.y)
            xmax = int(bottomRight.x)
            ymax = int(bottomRight.y)
            #cv2.rectangle(depthFrameColor, (xmin, ymin), (xmax, ymax), color, 1)

            # Denormalize bounding box
            x1 = int(detection.xmin * width)
            x2 = int(detection.xmax * width)
            y1 = int(detection.ymin * height)
            y2 = int(detection.ymax * height)
            try:
                label = labelMap[detection.label]
            except:
                label = detection.label
            #cv2.putText(frame, str(label), (x1 + 10, y1 + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            #cv2.putText(frame, "{:.2f}".format(detection.confidence*100), (x1 + 10, y1 + 35), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            #cv2.putText(frame, f"X: {int(detection.spatialCoordinates.x)} mm", (x1 + 10, y1 + 50), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            #cv2.putText(frame, f"Y: {int(detection.spatialCoordinates.y)} mm", (x1 + 10, y1 + 65), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            cv2.putText(frame, f"Z: {int(detection.spatialCoordinates.z)} mm", (x1 + 10, y1 + 80), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            #cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), cv2.FONT_HERSHEY_SIMPLEX)
        cv2.putText(frame, "NN fps: {:.2f}".format(fps), (2, frame.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, (255,255,255))
        #cv2.imshow("depth", depthFrameColor)
        cv2.imshow("preview", frame)
        if cv2.waitKey(1) == ord('q'):
            #print(fps)
            break


# === ./src/old/ageAndEmotion.py ===
import depthai as dai
from .operational_mode import OperationalMode
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .animation_manager import AnimationManager, load_images
from .sound_manager import *
import blobconverter
from .MultiMsgSync import TwoStageHostSeqSync
import cv2
import numpy as np
import time

class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(images)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+20, y-96), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 1)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/disagegender/')
        self.sound_manager.play_sound("disagegender")
        self.animate(images)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will obdisplay spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't obdisplay spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath('/home/raspberrypi/depthai-python/examples/models/face-detection-retail-0004.blob')
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(images)
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            responses = ['neutral', 'happy', 'sad', 'surprise', 'anger']
            neutral,happy,sad,surprise,anger=0,0,0,0,0
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]
                    
                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 1)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, emotion_name, (bbox[0]+10, y-110), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 7)
                        cv2.putText(frame, emotion_name, (bbox[0], y-90), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 1)
                        if emotion_name=="neutral":
                            neutral=neutral +1
                        if emotion_name=="happy":
                            happy=happy +1
                        if emotion_name=="sad":
                            sad=sad +1
                        if emotion_name=="surprise":
                            surprise=surprise +1
                        if emotion_name=="anger":
                            anger=anger +1
                        #print(emotion_name)    
                            #self.animate(animations[7])
                            #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    if neutral>20 or happy>20 or sad>20 or surprise>20 or anger>20:
                            max_response = max(zip(responses, (map(eval, responses))), key=lambda tuple: tuple[1])[0]
                            self.sound_manager.play_sound(max_response)
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/disemotional/')
        self.sound_manager.play_sound("disemotional")
        self.animate(images)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will obdisplay spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't obdisplay spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath('/home/raspberrypi/depthai-python/examples/models/face-detection-retail-0004.blob')
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath('/home/raspberrypi/depthai-python/examples/models/emotions-recognition-retail-0003.blob')
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS

# === ./src/old/pipeline.py ===
        # Create pipeline
        pipeline = dai.Pipeline()
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        objectTracker = pipeline.create(dai.node.ObjectTracker)

        xoutRgb = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)
        #xoutDepth = pipeline.create(dai.node.XLinkOut) # test may remove
        
        xoutRgb.setStreamName("preview")
        trackerOut.setStreamName("tracklets")
        #xoutDepth.setStreamName("depth") # test may remove
        
        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        camRgb.setInterleaved(False)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        camRgb.setFps(10)
        
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        monoRight.setCamera("right")

        # setting node configs
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        # Align depth map to the perspective of RGB camera, on which inference is done
        stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
        stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())
        
        # Better handling for occlusions:
        stereo.setLeftRightCheck(True)
        # Closer-in minimum depth, disparity range is doubled:
        stereo.setExtendedDisparity(False)
        # Better accuracy for longer distance, fractional disparity 32-levels:
        stereo.setSubpixel(False)

        spatialDetectionNetwork.setBlobPath('/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')
        spatialDetectionNetwork.setConfidenceThreshold(0.7)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)
        
        objectTracker.setDetectionLabelsToTrack([15])  # track only person
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        camRgb.preview.link(spatialDetectionNetwork.input)
        objectTracker.passthroughTrackerFrame.link(xoutRgb.input)
        objectTracker.out.link(trackerOut.input)
        
        spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)
        #objectTracker.inputTrackerFrame.setBlocking(False)# testing
        spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        spatialDetectionNetwork.out.link(objectTracker.inputDetections)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
    

# === ./src/old/Wide_angle.py ===


# === ./src/old/operational_modes_backup_17_7_2024.py ===
import time
import operator
import depthai as dai
import blobconverter
import cv2
import numpy as np
from .MultiMsgSync import TwoStageHostSeqSync
from .animation_manager import AnimationManager, load_images 
from .sound_manager import *
from .utils import RobotState, frame_norm
from .utils import RobotConfig
from .robot import *
from .operational_mode import OperationalMode
import UltraBorg_old
import board
import adafruit_bno055

# Board #1, address 10
UB1 = UltraBorg_old.UltraBorg()
UB1.i2cAddress = 10
UB1.Init()
# Board #2, address 11
UB2 = UltraBorg_old.UltraBorg()
UB2.i2cAddress = 11
UB2.Init()
# Board #3, address 12
UB3 = UltraBorg_old.UltraBorg()
UB3.i2cAddress = 12
UB3.Init()
# Board #4, address 13
#UB4 = UltraBorg.UltraBorg()
#UB4.i2cAddress = 13
#UB4.Init()
UB1.SetWithRetry(UB1.SetServoMaximum1, UB1.GetServoMaximum1, 4520, 5)
UB1.SetWithRetry(UB1.SetServoMinimum1, UB1.GetServoMinimum1, 1229, 5)
UB1.SetWithRetry(UB1.SetServoStartup1, UB1.GetServoStartup1, 3100, 5)
UB1.SetWithRetry(UB1.SetServoMaximum2, UB1.GetServoMaximum2, 3026, 5)
UB1.SetWithRetry(UB1.SetServoMinimum2, UB1.GetServoMinimum2, 1592, 5)
UB1.SetWithRetry(UB1.SetServoStartup2, UB1.GetServoStartup2, 2408, 5)
UB2.SetWithRetry(UB2.SetServoMaximum1, UB2.GetServoMaximum1, 4951, 5)
UB2.SetWithRetry(UB2.SetServoMinimum1, UB2.GetServoMinimum1, 1076, 5)
UB2.SetWithRetry(UB2.SetServoStartup1, UB2.GetServoStartup1, 2949, 5)
UB2.SetWithRetry(UB2.SetServoMaximum2, UB2.GetServoMaximum2, 5247, 5)
UB2.SetWithRetry(UB2.SetServoMinimum2, UB2.GetServoMinimum2, 1136, 5)
UB2.SetWithRetry(UB2.SetServoStartup2, UB2.GetServoStartup2, 3130, 5)
UB2.SetWithRetry(UB2.SetServoMaximum3, UB2.GetServoMaximum3, 4561, 5)
UB2.SetWithRetry(UB2.SetServoMinimum3, UB2.GetServoMinimum3, 989, 5)
UB2.SetWithRetry(UB2.SetServoStartup3, UB2.GetServoStartup3, 2737, 5)

# SPDX-FileCopyrightText: 2021 ladyada for Adafruit Industries
# SPDX-License-Identifier: MIT
i2c = board.I2C()  # uses board.SCL and board.SDA
# i2c = board.STEMMA_I2C()  # For using the built-in STEMMA QT connector on a microcontroller
sensor = adafruit_bno055.BNO055_I2C(i2c)
last_val = 0xFFFF
def temperature():
    global last_val  # pylint: disable=global-statement
    result = sensor.temperature
    if abs(result - last_val) == 128:
        result = sensor.temperature
        if abs(result - last_val) == 128:
            return 0b00111111 & result
    last_val = result
    return result
calibrated = False
#calibrated = sensor.calibrated()
print("Cal status (S,G,A,M):{}".format(sensor.calibration_status))
# Read the calibration status, 0=uncalibrated and 3=fully calibrated.
print("Temperature: {} degrees C".format(temperature()))  # Uncomment if using a Raspberry Pi
#print("Accelerometer (m/s^2): {}".format(sensor.acceleration))
#print("Magnetometer (microteslas): {}".format(sensor.magnetic))
#print("Gyroscope (rad/sec): {}".format(sensor.gyro))
print("Euler angle: {}".format(sensor.euler))
#print("Quaternion: {}".format(sensor.quaternion))
#print("Linear acceleration (m/s^2): {}".format(sensor.linear_acceleration))
#print("Gravity (m/s^2): {}".format(sensor.gravity))
#print(sensor.calibrated)

global stop_distance,obdis, colour
colour=(255,255,255)
stop_distance=400
obdis=400
# INITIALIZE MOTOR CONTROLER
# ************************************************************************************************
#mc = motoron.MotoronI2C()
# Reset the controller to its default settings, then obdisable CRC.  The bytes for
# each of these commands are shown here in case you want to implement them on
# your own without using the library.
#mc.reinitialize()  # Bytes: 0x96 0x74
#mc.obdisable_crc()   # Bytes: 0x8B 0x04 0x7B 0x43
# Clear the reset flag, which is set after the controller reinitializes and
# counts as an error.
#mc.clear_reset_flag()  # Bytes: 0xA9 0x00 0x04
# By default, the Motoron is configured to stop the motors if it does not get
# a motor control command for 1500 ms.  You can uncomment a line below to
# adjust this time or obdisable the timeout feature.
# mc.set_command_timeout_milliseconds(1000)
# mc.obdisable_command_timeout()
# Configure motor 1
#mc.set_max_acceleration(1, 40)
#mc.set_max_deceleration(1, 40)


class RemoteControlMode(OperationalMode):
    # ALIGNMENT  attaching servo horns the screw is positioned over the sticker on the servo
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def get_key(self) -> str:
        return 'x'
    
    def get_state(self) -> RobotState:
        return RobotState.REMOTE_CONTROL
    
    def _start(self, robot):
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        #self.sound_manager.play_sound("agegender")
        #self.animate(robot.config.animations[4])
        robot.say("Remote control mode enabled")
        print("Remote control enabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Double_beep2")

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        #images=load_images('/home/raspberrypi/MiniMax/Animations/obdisagegender/')
        #self.sound_manager.play_sound("obdisagegender")
        #self.animate(robot.config.animations[5])
        robot.say("Remote Control Mode has been disabled")
        print("Remote control disabled printing euler angle")
        heading, roll, pitch = sensor.euler
        print(heading)
        #robot.animate(1)
        #engine.runAndWait()
        time.sleep(1.6)
        robot.play_sound("Long_power_down")
        
    def _main(self, robot):
        Direction='neutral'
        keyup=0
        x=0
        y=0
        running = True
        UB2.SetServoPosition3(0)
        hp=0
        pantilt=0
        k_w,k_a,k_s,k_d,k_q,k_e,k_z,k_x,k_l,k_r,k_o,k_p = 0,0,0,0,0,0,0,0,0,0,0,0
        while running:
            events = pygame.event.get()
            for event in events:
                #print('this is keyup at start of loop ',keyup)
                if event.type == pygame.KEYUP:
                    keyup=1
                    if event.key == pygame.K_w:
                        k_w = 0
                    if event.key == pygame.K_a:
                        k_a = 0
                    if event.key == pygame.K_s:
                        k_s = 0
                    if event.key == pygame.K_d:
                        k_d = 0
                    if event.key == pygame.K_q:
                        k_q = 0
                    if event.key == pygame.K_e:
                        k_e = 0
                    if event.key == pygame.K_z:
                        k_z = 0
                    if event.key == pygame.K_x:
                        k_x = 0
                    if event.key == pygame.K_l:
                        k_l = 0
                    if event.key == pygame.K_r:
                        k_r = 0
                if event.type == pygame.QUIT:
                    _exit()
                    pygame.quit()
                    running = False
                    UB2.SetServoPosition1(0)    #set servos to default positions on exit     
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition3(0)
                    UB1.SetServoPosition1(0)
                if event.type == pygame.KEYDOWN:
                    keyup=0
                    if event.key == pygame.K_ESCAPE:
                        UB2.SetServoPosition1(0)   #set servos to default positions on exit  
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition3(0)
                        UB1.SetServoPosition1(0)
                        running = False
                    if event.key == pygame.K_q:
                        k_q = 1
                    if event.key == pygame.K_e:
                        k_e = 1
                    if event.key == pygame.K_w:
                        k_w = 1
                    if event.key == pygame.K_a:
                        k_a = 1
                    if event.key == pygame.K_s:
                        k_s = 1
                    if event.key == pygame.K_d:
                        k_d = 1
                    if event.key == pygame.K_z:
                        k_z = 1
                    if event.key == pygame.K_x:
                        k_x = 1
                    if event.key == pygame.K_b:
                        k_b = 1
                    if event.key == pygame.K_f:
                        k_f = 1
                    if event.key == pygame.K_l:
                        k_l = 1
                    if event.key == pygame.K_r:
                        k_r = 1
                if k_w:
                    #print("w")
                    Direction ='forward'
                    x=-0.95
                    y=-0.12
                if k_a:
                    #print("a")
                    Direction ='left'# anti-clockwise euler goes down
                    x=0.1
                    y=0.8
                if k_d:
                    #print("d")
                    Direction='right'# clockwise euler goes up
                    x=-0.26
                    y=-1.0
                if k_q:
                    #print("q")
                    Direction ='left-forward'
                    x=-0.9
                    y=0.1
                if k_e:
                    #print("e")
                    Direction ='right-forward'
                    x=-0.9
                    y=-0.3
                if k_s:
                    #print("s")
                    Direction='back'
                    x=0.98
                    y=0.24
                if k_z:
                    #print("z")
                    Direction ='left-reverse'
                    x=0.98
                    y=-0.02
                if k_x:
                    #print("x")
                    Direction ='right-reverse'
                    x=0.98
                    y=0.4
                if k_l:
                    #print("Head Left")
                    Direction='Head left'
                    UB2.SetServoPosition4(0)
                    if hp <-0.95:
                        hp=-0.95
                    else:
                        hp=hp-0.02
                #print(hp)
                if k_r:
                    #print("Head Right")
                    Direction='Head right'
                    if hp > 0.95:
                        hp=0.95
                    else:
                        hp=hp+0.02
                if keyup==1:
                    Direction='neutral'
                    x=0
                    y=0
                UB2.SetServoPosition2(x)
                UB2.SetServoPosition1(y)
                UB2.SetServoPosition3(hp)
                #UB1.SetServoPosition1(pantilt)
                
class AgeGenderOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
            #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)

        return RobotState.IDLE
    
    def _start(self, robot):
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')
        self.sound_manager.play_sound("agegender")
        self.animate(images)
        #robot.say("Detecting human age and gender")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))

            sync = TwoStageHostSeqSync()
            queues = {}
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)

            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)

                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]

                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))

                        # Decoding of recognition results
                        rec = recognitions[i]
                        age = int(float(np.squeeze(np.array(rec.getLayerFp16('age_conv3')))) * 100)
                        gender = np.squeeze(np.array(rec.getLayerFp16('prob')))
                        gender_str = "female" if gender[0] > gender[1] else "male"

                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 2)
                        y = (bbox[1] + bbox[3]) // 2
                        
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, gender_str, (bbox[0], y - 102), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                        #cv2.putText(frame, str(age), (bbox[0]+120, y-102), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 0), 8)
                        cv2.putText(frame, str(age), (bbox[0]+20, y-96), cv2.FONT_HERSHEY_TRIPLEX, 0.8, (255, 255, 255), 1)
                        #if stereo:
                        # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                        #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 8)
                        #cv2.putText(frame, coords, (bbox[0], y + 60), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 2)
                    #flippy=cv2.flip(frame,0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)

                if key_pressed == ord('z'):
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/obdisagegender/')
        self.sound_manager.play_sound("obdisagegender")
        self.animate(images)
        #robot.say("Age and Gender Detection obdisabled.")

        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self, stereo):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300, 300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        copy_manip.out.link(cam_xout.input)
        # ImageManip will resize the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)
        '''#if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will obdisplay spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't obdisplay spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_nn.input.setQueueSize(1)
        face_det_manip.out.link(face_det_nn.input)

        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)

        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'recognition_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Remove in 2.18 and use `imgFrame.getSequenceNum()` in Script node
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()
        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")
            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg
            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element
        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None
        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb
        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)
            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(62, 62)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        recognition_manip = pipeline.create(dai.node.ImageManip)
        recognition_manip.initialConfig.setResize(62, 62)
        recognition_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(recognition_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(recognition_manip.inputImage)

        # Second stange recognition NN
        print("Creating recognition Neural Network...")
        recognition_nn = pipeline.create(dai.node.NeuralNetwork)
        recognition_nn.setBlobPath(blobconverter.from_zoo(name="age-gender-recognition-retail-0013", shaves=6))
        recognition_manip.out.link(recognition_nn.input)

        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        recognition_nn.out.link(recognition_xout.input)

        return pipeline

    def get_key(self) -> str:
        return 'a'
    
    def get_state(self) -> RobotState:
        return RobotState.AGEGENDER


class EmotionOperationalMode(OperationalMode):
    def __init__(self, config: RobotConfig = None) -> None:
        super().__init__()
        self.emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']
        
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)

        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
        
    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
    
    def animate(self, images):
        self.animator.animate(images)

    def run(self, robot):
        self._start(robot)
        self._main(robot)
        self._end(robot)
        return RobotState.IDLE
    
    def _start(self, robot):
        #time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')
        self.sound_manager.play_sound("emotional")
        self.animate(images)
        #robot.say("Detection of human emotional state enabled.")
        #robot.animate(1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _main(self, robot):
        with dai.Device() as device:
            device.setLogLevel(dai.LogLevel.CRITICAL)
            device.setLogOutputLevel(dai.LogLevel.CRITICAL)
            stereo = 1 < len(device.getConnectedCameras())
            device.startPipeline(self._create_pipeline(stereo))
            sync = TwoStageHostSeqSync()
            queues = {}
            responses = ['neutral', 'happy', 'sad', 'surprise', 'anger']
            neutral,happy,sad,surprise,anger=0,0,0,0,0
            # Create output queues
            for name in ["color", "detection", "recognition"]:
                queues[name] = device.getOutputQueue(name)
            while True:
                for name, q in queues.items():
                    # Add all msgs (color frames, object detections and age/gender recognitions) to the Sync class.
                    if q.has():
                        sync.add_msg(q.get(), name)
                msgs = sync.get_msgs()
                if msgs is not None:
                    frame = msgs["color"].getCvFrame()
                    detections = msgs["detection"].detections
                    recognitions = msgs["recognition"]
                    
                    for i, detection in enumerate(detections):
                        bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
                        rec = recognitions[i]
                        emotion_results = np.array(rec.getFirstLayerFp16())
                        emotion_name = self.emotions[np.argmax(emotion_results)]
                        #cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (10, 245, 10), 1)
                        y = (bbox[1] + bbox[3]) // 2
                        #cv2.putText(frame, emotion_name, (bbox[0]+10, y-110), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 0), 7)
                        cv2.putText(frame, emotion_name, (bbox[0], y-90), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 255), 1)
                        if emotion_name=="neutral":
                            neutral=neutral +1
                        if emotion_name=="happy":
                            happy=happy +1
                        if emotion_name=="sad":
                            sad=sad +1
                        if emotion_name=="surprise":
                            surprise=surprise +1
                        if emotion_name=="anger":
                            anger=anger +1
                        #print(emotion_name)    
                        
                            #self.animate(animations[7])
                            #if stereo:
                            # You could also get detection.spatialCoordinates.x and detection.spatialCoordinates.y coordinates
                            #coords = "Z: {:.2f} m".format(detection.spatialCoordinates.z/1000)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (0, 0, 0), 8)
                            #cv2.putText(frame, coords, (bbox[0], y + 80), cv2.FONT_HERSHEY_TRIPLEX, .7, (255, 255, 255), 2)
                    
                    #flipped = cv2.flip(frame, 0)
                    cv2.imshow("Camera", frame)
                key_pressed=cv2.waitKey(1)
                if key_pressed == ord('z'):
                    if neutral>20 or happy>20 or sad>20 or surprise>20 or anger>20:
                            max_response = max(zip(responses, (map(eval, responses))), key=lambda tuple: tuple[1])[0]
                            self.sound_manager.play_sound(max_response)
                    break

    def _end(self, robot):
        cv2.destroyAllWindows()
        time.sleep(0.1)
        images=load_images('/home/raspberrypi/MiniMax/Animations/obdisemotional/')
        self.sound_manager.play_sound("obdisemotional")
        self.animate(images)
        #robot.say("Emotion Detection state obdisabled.")
        #robot.animate(1)
        #engine.runAndWait()
        #time.sleep(0.1)
        robot.play_sound("Radar_scanning_chirp")
    
    def _create_pipeline(self,robot):
        pipeline = dai.Pipeline()
        print("Creating Color Camera...")
        cam = pipeline.create(dai.node.ColorCamera)
        cam.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        cam.setPreviewSize(300,300)
        cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        #SensorResolution.THE_1080_P
        cam.setInterleaved(False)
        cam.setBoardSocket(dai.CameraBoardSocket.RGB)
        cam_xout = pipeline.create(dai.node.XLinkOut)
        cam_xout.setStreamName("color")
        cam.preview.link(cam_xout.input)
        # Workaround: remove in 2.18, use `cam.setPreviewNumFramesPool(10)`
        # This manip uses 15*3.5 MB => 52 MB of RAM.
        copy_manip = pipeline.create(dai.node.ImageManip)
        copy_manip.setNumFramesPool(15)
        copy_manip.setMaxOutputFrameSize(3499200)
        cam.preview.link(copy_manip.inputImage)

        # ImageManip that will crop the frame before sending it to the Face detection NN node
        face_det_manip = pipeline.create(dai.node.ImageManip)
        face_det_manip.initialConfig.setResize(300, 300)
        face_det_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.RGB888p)
        copy_manip.out.link(face_det_manip.inputImage)

        '''if stereo:
            monoLeft = pipeline.create(dai.node.MonoCamera)
            monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)
            monoRight = pipeline.create(dai.node.MonoCamera)
            monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
            monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)
            stereo = pipeline.create(dai.node.StereoDepth)
            stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
            stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
            monoLeft.out.link(stereo.left)
            monoRight.out.link(stereo.right)
            # Spatial Detection network if OAK-D
            print("OAK-D detected, app will obdisplay spatial coordiantes")
            face_det_nn = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
            face_det_nn.setBoundingBoxScaleFactor(0.8)
            face_det_nn.setDepthLowerThreshold(100)
            face_det_nn.setDepthUpperThreshold(5000)
            stereo.depth.link(face_det_nn.inputDepth)
        else: # Detection network if OAK-1'''
        #print("OAK-1 detected, app won't obdisplay spatial coordiantes")
        face_det_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
        face_det_nn.setConfidenceThreshold(0.5)
        face_det_nn.setBlobPath(blobconverter.from_zoo(name="face-detection-retail-0004", shaves=6))
        face_det_manip.out.link(face_det_nn.input)
        # Send face detections to the host (for bounding boxes)
        face_det_xout = pipeline.create(dai.node.XLinkOut)
        face_det_xout.setStreamName("detection")
        face_det_nn.out.link(face_det_xout.input)
        # Script node will take the output from the face detection NN as an input and set ImageManipConfig
        # to the 'age_gender_manip' to crop the initial frame
        image_manip_script = pipeline.create(dai.node.Script)
        face_det_nn.out.link(image_manip_script.inputs['face_det_in'])
        # Only send metadata, we are only interested in timestamp, so we can sync
        # depth frames with NN output
        face_det_nn.passthrough.link(image_manip_script.inputs['passthrough'])
        copy_manip.out.link(image_manip_script.inputs['preview'])
        image_manip_script.setScript("""
        import time
        msgs = dict()

        def add_msg(msg, name, seq = None):
            global msgs
            if seq is None:
                seq = msg.getSequenceNum()
            seq = str(seq)
            # node.warn(f"New msg {name}, seq {seq}")

            # Each seq number has it's own dict of msgs
            if seq not in msgs:
                msgs[seq] = dict()
            msgs[seq][name] = msg

            # To avoid freezing (not necessary for this ObjDet model)
            if 15 < len(msgs):
                #node.warn(f"Removing first element! len {len(msgs)}")
                msgs.popitem() # Remove first element

        def get_msgs():
            global msgs
            seq_remove = [] # Arr of sequence numbers to get deleted
            for seq, syncMsgs in msgs.items():
                seq_remove.append(seq) # Will get removed from dict if we find synced msgs pair
                # node.warn(f"Checking sync {seq}")

                # Check if we have both detections and color frame with this sequence number
                if len(syncMsgs) == 2: # 1 frame, 1 detection
                    for rm in seq_remove:
                        del msgs[rm]
                    # node.warn(f"synced {seq}. Removed older sync values. len {len(msgs)}")
                    return syncMsgs # Returned synced msgs
            return None

        def correct_bb(bb):
            if bb.xmin < 0: bb.xmin = 0.001
            if bb.ymin < 0: bb.ymin = 0.001
            if bb.xmax > 1: bb.xmax = 0.999
            if bb.ymax > 1: bb.ymax = 0.999
            return bb

        while True:
            time.sleep(0.001) # Avoid lazy looping

            preview = node.io['preview'].tryGet()
            if preview is not None:
                add_msg(preview, 'preview')

            face_dets = node.io['face_det_in'].tryGet()
            if face_dets is not None:
                # TODO: in 2.18.0.0 use face_dets.getSequenceNum()
                passthrough = node.io['passthrough'].get()
                seq = passthrough.getSequenceNum()
                add_msg(face_dets, 'dets', seq)

            sync_msgs = get_msgs()
            if sync_msgs is not None:
                img = sync_msgs['preview']
                dets = sync_msgs['dets']
                for i, det in enumerate(dets.detections):
                    cfg = ImageManipConfig()
                    correct_bb(det)
                    cfg.setCropRect(det.xmin, det.ymin, det.xmax, det.ymax)
                    # node.warn(f"Sending {i + 1}. age/gender det. Seq {seq}. Det {det.xmin}, {det.ymin}, {det.xmax}, {det.ymax}")
                    cfg.setResize(64, 64)
                    cfg.setKeepAspectRatio(False)
                    node.io['manip_cfg'].send(cfg)
                    node.io['manip_img'].send(img)
        """)
        manip_manip = pipeline.create(dai.node.ImageManip)
        manip_manip.initialConfig.setResize(64, 64)
        manip_manip.setWaitForConfigInput(True)
        image_manip_script.outputs['manip_cfg'].link(manip_manip.inputConfig)
        image_manip_script.outputs['manip_img'].link(manip_manip.inputImage)
        # This ImageManip will crop the mono frame based on the NN detections. Resulting image will be the cropped
        # face that was detected by the face-detection NN.
        emotions_nn = pipeline.create(dai.node.NeuralNetwork)
        emotions_nn.setBlobPath(blobconverter.from_zoo(name="emotions-recognition-retail-0003", shaves=6))
        manip_manip.out.link(emotions_nn.input)
        recognition_xout = pipeline.create(dai.node.XLinkOut)
        recognition_xout.setStreamName("recognition")
        emotions_nn.out.link(recognition_xout.input)
        return pipeline

    def get_key(self) -> str:
        return 'e'
    
    def get_state(self) -> RobotState:
        return RobotState.EMOTIONS

class ObjectSearchOperationMode(OperationalMode):
    def __init__(self, label: str, biscuit_mode=True, config: RobotConfig = None) -> None:
        super().__init__()
        _supported_labels = ['person', 'cup',]
        assert label in _supported_labels, f"label '{label}' is not in supported labels {_supported_labels}"
        self.label = label
        self.biscuit_mode = biscuit_mode
        if self.label == 'person':
            self.model_id = 15
            self.model_threshold = 0.5
        elif self.label == 'cup':
            self.model_id = 3
            self.model_threshold = 0.3
        if config is None:
            # use default config
            self.config = RobotConfig()
        else:
            self.config = config    
        #logging.debug("Starting AnimationManager")
        self.animator = AnimationManager(config=self.config)
        #logging.debug("Starting SoundManager")
        self.sound_manager = SoundManger(config=self.config)
    
    def obstacleAvoid(self, avoided):
        mainloop=True
        rx=0
        ry=0
        forward_count=0
        turn=''
        while mainloop:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    mainloop = False
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        mainloop = False
                        break
            us1,us2,us3,us4=int(UB2.GetDistance1()),int(UB2.GetDistance2()),int(UB2.GetDistance3()),int(UB2.GetDistance4())
            usr1,usr2,usr3,usr4=int(UB3.GetDistance1()),int(UB3.GetDistance2()),int(UB3.GetDistance3()),int(UB3.GetDistance4())
            if us1 == 0:
                us1=9999
            if us2 == 0:
                us2=9999
            if us3 == 0:
                us3=9999
            if us4 ==0:
                us4=9999
            if usr1 == 0:
                usr1=9999
            if usr2 == 0:
                usr2=9999
            if usr3 == 0:
                usr3=9999
            if usr4 ==0:
                usr4=9999
            print (us1, "  ",us2, " ", us3, " ", us4," Front sensors")
            print (usr1, "  ",usr2, " ", usr3, " ", usr4," Rear sensors") 
            if us1<obdis or us2<obdis or us3<obdis or us4<obdis:  
                if us1<obdis and us2>obdis and us3>obdis and us4>obdis:#turn left,right sensor close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us1<obdis and us2<obdis and us3>obdis and us4>obdis:#turn left,right 2 sensors close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us1<obdis and us2<obdis and us3<obdis and us4>obdis:#turn left,right 3 sensors close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us1>obdis and us2<obdis and us3>obdis and us4>obdis:#turn left,right 3 sensors close to object 
                    rx = 0.1
                    ry = 0.98
                    turn='left'#ant-clockwise
                if us4<obdis and us3>obdis and us2>obdis and us1>obdis:#turn right,left sensor close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us4<obdis and us3<obdis and us2>obdis and us1>obdis:#turn right,left 2 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us4<obdis and us3<obdis and us2<obdis and us1>obdis:#turn right,left 3 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us4>obdis and us3<obdis and us2>obdis and us1>obdis:#turn right,left 3 sensors close to object
                    rx = -0.26
                    ry = -1
                    turn='right'#clockwise
                if us1<obdis and us2<obdis and us3<obdis and us4<obdis:#back out of corner
                    rx = 0.98
                    ry = 0.24
                    turn='back'
                if us2<obdis and us3<obdis and us1>obdis and us4>obdis:#front sensors triggered without other 2,turn right
                    rx = 0.1
                    ry = 0.98
                    turn='left'
            else:
                if turn=='left':# turn 1 last time before going forward
                    rx=0.1
                    ry=0.98
                    turn=''
                if turn=='right':# turn 1 last time before going forward
                    rx = -0.26
                    ry = -0.1
                    turn=''
                if turn=='back':# go back 1 last time 
                    rx = 0.98
                    ry = 0.24
                    turn=''
                UB2.SetServoPosition2(rx)
                UB2.SetServoPosition1(ry)
                time.sleep(0.2)
                rx = -0.95
                ry = -0.12# drive forward as NO objects have been detected
                #robot.say("forward")
                #x=-0.95
                #y=-0.08
                UB2.SetServoPosition2(rx)
                UB2.SetServoPosition1(ry)
                print("moving forward")
                time.sleep(0.2)
                avoided = True
                forward_count += 1
                if forward_count>6:
                    mainloop = False
                    forward_count=0

            UB2.SetServoPosition2(rx)
            UB2.SetServoPosition1(ry)
            time.sleep(0.15)
 
    def avoid(self, avoided):
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str=str(us1)
        us2_str=str(us2)
        us3_str=str(us3)
        us4_str=str(us4)
        #r_person_str=str(r_person)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        print('........................... obstacle detected')
        print('attempting to avoid obstacle')
        #robot.say("Obstacle detected")
        images=load_images('/home/raspberrypi/MiniMax/Animations/inmyway/')
        self.sound_manager.play_sound("inmyway")
        preObsHeading, roll, pitch = sensor.euler #left euler goes down. Righr euler goes up.
        print("Before obstacle avoided heading",preObsHeading)
        self.obstacleAvoid(avoided)
        afterObsHeading, roll, pitch = sensor.euler
        print("after obstacle avoided heading",afterObsHeading)
        diff1 = int(preObsHeading - afterObsHeading)
        diff2 = abs(diff1)
        while diff2>1:
            afterObsHeading, roll, pitch = sensor.euler#aproximate same heading 
            diff1 = int(preObsHeading - afterObsHeading) #
            diff2 = abs(diff1) #
            if preObsHeading<=180:
                if afterObsHeading > preObsHeading and (preObsHeading+180) > afterObsHeading:
                    #turn Left or anti-clockwise to approach destination in shortest way
                    x=0.1
                    y=0.98
                else: # right clockwise 
                    x=-0.26
                    y=-1.0
            else:
                if preObsHeading > afterObsHeading and (afterObsHeading+180) > preObsHeading:
                    #turn Right
                    x=-0.26
                    y=-1.0
                else: # shortest way around a 360 degree circle to the pre obstacle angle
                    x=0.1
                    y=0.98
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    break
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_z:
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        break
            UB2.SetServoPosition2(x)
            UB2.SetServoPosition1(y)
        print("back to original angle +- 1 degrees")
        UB2.SetServoPosition2(0)
        UB2.SetServoPosition1(0)
        #cv2.destroyAllWindows()
        r_person=0  
    
    def _exit():
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)

    def play_sound(self, filename):
        self.sound_manager.play_sound(filename)
        
    def animate(self, images):
        self.animator.animate(images)
    
    def run(self, robot):
        self._start(robot)
        success = self._main(robot)
        return self._end(robot, success = success, give_biscuit_on_success=robot.config.ps_give_biscuit_on_success)

    def _start(self, robot):
        time.sleep(0.1)
        if self.biscuit_mode:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #THIS WAS USED, TO SAY "TO GIVE A BISCUIT TO"
            self.animate(images) #robot.config.animations[19])
            #robot.say(f"Search mode enabled. Searching for {self.label} who like jellybeans")
        else:
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')
            self.sound_manager.play_sound("search-on")
            self.animate(images) #(robot.config.animations[18])
            time.sleep(0.1)
            images=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')
            self.sound_manager.play_sound("search-person") #This is used if normal search person mode is activated.
            self.animate(images) #robot.config.animations[19])
        time.sleep(0.1)
        robot.play_sound('Radar_bleep_chirp')
        time.sleep(0.1)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        
    def stop(self, robot):
        print('First stop check')
        z_str=str(z)
        current_z_str=str(current_z)
        us1_str=str(us1)
        us2_str=str(us2)
        us3_str=str(us3)
        us4_str=str(us4)
        #r_person_str=str(r_person)
        print('z = '+z_str+' this is current_z = '+ current_z_str)
        print('Ultrasonic us1= '+us1_str+' us2 = '+us2_str+' us3 = '+us3_str+' us4 = '+us4_str)
        #print('r_person = '+r_person_str)
        #pin="2z"
        #robot.write_serial(pin)
        UB2.SetServoPosition1(0)         
        UB2.SetServoPosition2(0)
        #print('Old pin '+old_pin +' new pin '+pin)
        print('........................... Objective Reached')
        robot.say("Objective reached")
        cv2.destroyAllWindows()
        r_person=0
    
    def _main(self, robot):
        global old_pin, pin, z, r_person, current_z, current_z_str,us1,us2,us3,us4,us1_str,us2_str,us3_str,us4_str,r_person_str,rx,ry
        pipeline = self._create_pipeline2(robot)
        pin, r_person, z, current_z, stop_distance,rx,ry = '2z', 0, 9999,9999,700,0,0
        #found_people[0] = {'tid': 99999, 'objxcenter': 0, 'objycenter': 0, 'status': 'TRACKED', 'z_depth': 1000}
        with dai.Device(pipeline) as device:
            preview = device.getOutputQueue("preview", 4, False)
            tracklets = device.getOutputQueue("tracklets", 4, False)
            startTime = time.monotonic()
            counter, fps, frame = 0, 0, None
            found_people=[{'tid': 666, 'objxcenter': 60,'status':'LOST','z_depth':9000}]
            sorted_x={}
            while True:
                us1,us2,us3,us4=int(UB2.GetDistance1()),int(UB2.GetDistance2()),int(UB2.GetDistance3()),int(UB2.GetDistance4())
                if us1 == 0:
                    us1=99999
                if us2 == 0:
                    us2=99999
                if us3 == 0:
                    us3=99999
                if us4 ==0:
                    us4=99999
                imgFrame = preview.get()
                track = tracklets.get()
                counter+=1
                current_time = time.monotonic()
                if (current_time - startTime) > 1 :
                    fps = counter / (current_time - startTime)
                    counter = 0
                    startTime = current_time
                frame = imgFrame.getCvFrame()
                #frame=cv2.resize(frame,(800,600),fx=0,fy=0, interpolation = cv2.INTER_NEAREST) # this line slows down everything!
                trackletsData = track.tracklets
                if current_z==0:
                    current_z=9999
                # us = Ultrasonic sensor , z = distance to object measured by depth camera.
                if current_z < stop_distance:
                            self.stop(robot)
                            r_person=0
                            return True
                if us1 < 450 or us2 < 450 or us3 < 450 or us4 <450: #object is close  
                    r_person=r_person+1
                    if r_person>=2:
                        avoided=False
                        self.avoid(avoided)
                        UB2.SetServoPosition2(0)
                        UB2.SetServoPosition1(0)
                        r_person=0
                        # do everything needed when robot finds obstacle
                else:
                    r_person=0
                found_peeps=0    
                n_of_track_peeps=0
                for t in trackletsData:
                    found_peeps+=1
                    roi = t.roi.denormalize(frame.shape[1], frame.shape[0])
                    x1,y1 = int(roi.topLeft().x), int(roi.topLeft().y)
                    x2,y2 = int(roi.bottomRight().x), int(roi.bottomRight().y)
                    z=int(t.spatialCoordinates.z)
                    xmin, ymin = x1, y1
                    xmax, ymax = x2, y2
                    x_diff, y_diff = (xmax-xmin), (ymax-ymin)
                    obj_x_center = int(xmin+(x_diff/2))
                    #obj_y_center=int(ymin+(y_diff/2))
                    #dictionary to be updated
                    dict = {'tid': t.id, 'objxcenter': obj_x_center, 'status': t.status.name, 'z_depth': z}
                    #print(' Found peeps ',found_peeps)
                    if t.status.name=='TRACKED':
                        n_of_track_peeps+=1
                        found_people.append(dict)
                if n_of_track_peeps <1:
                    pass
                    # no tracked people in current frame
                    #robot.say("No people detected") says this WAY too many times as detection may not always work.
                    #print('no peeps')
                else:
                    sorted_x=min((x for x in found_people if x['status'] == "TRACKED"), key=lambda x:x['tid'], default=None)
                    #print('sorted x is ',sorted_x)
                    found_people=[{'tid': 666, 'objxcenter': 60,'status':'LOST','z_depth':9000}]
                    obj_x_center= sorted_x['objxcenter'] # get x co-ordinate of lowest ID
                    #obj_y_center= sorted_x['objycenter'] # get y co-ordinate of lowest ID
                    current_z = sorted_x['z_depth'] # get z-depth co-ordinate of lowest ID
                    if current_z==0:
                        current_z=9999
                    x_deviation = int(robot.config.ps_xres/2)-obj_x_center
                    # calculate the deviation from the center of the screen
                    if (abs(x_deviation)<robot.config.ps_tolerance): # is object in the middle of screen?
                        if current_z < stop_distance:
                            self.stop(robot)
                            r_person=0
                            return True 
                        if us1<450 or us2<450 or us3<450 or us4<450:#object is very close 
                            print('Second stop check')
                            avoided=False
                            self.avoid(avoided)
                            UB2.SetServoPosition2(0)
                            UB2.SetServoPosition1(0)
                            r_person=0
                        else:
                            r_person=0
                            rx = -0.95
                            ry = -0.12 # move forward
                            UB2.SetServoPosition2(rx)
                            UB2.SetServoPosition1(ry)
                            #break
                            #print("........................... moving robot FORWARD")
                    else:
                        if (x_deviation>robot.config.ps_tolerance):
                            if x_deviation< robot.config.ps_far_boundry:
                                r_person=0
                                robot.write_serial(pin)
                                rx=-0.9
                                ry=0.1
                                UB2.SetServoPosition2(rx)
                                UB2.SetServoPosition1(ry)
                                #break
                                #print('........... turning left while moving forward' )
                            if x_deviation>=robot.config.ps_far_boundry:
                                r_person=0
                                rx=0.1
                                ry=0.98
                                UB2.SetServoPosition2(rx)
                                UB2.SetServoPosition1(ry)
                                #break
                                #print('..... turning left on the spot' )
                        elif ((x_deviation*-1)>robot.config.ps_tolerance):
                            if abs(x_deviation)<robot.config.ps_far_boundry:
                                r_person=0
                                rx=-0.9
                                ry=-0.3
                                UB2.SetServoPosition2(rx)
                                UB2.SetServoPosition1(ry)
                                #break
                                #print('............ turning right while moving forward' )
                            if abs(x_deviation)>=robot.config.ps_far_boundry:
                                r_person=0
                                rx=-0.26
                                ry=-1.0
                                UB2.SetServoPosition2(rx)
                                UB2.SetServoPosition1(ry)
                                #break
                                #print('..... turning right on the spot' )
                cv2.putText(frame, "fps: {:.2f}".format(fps), (2, frame.shape[0] - 7), cv2.FONT_HERSHEY_TRIPLEX, 0.6, colour)
                key_pressed=cv2.waitKey(1)
                if key_pressed==ord('z'):
                    pin='5z'
                    #robot.write_serial(pin)
                    UB2.SetServoPosition2(0)
                    UB2.SetServoPosition1(0)
                    print('Aborting search z ')
                    cv2.destroyAllWindows()
                    images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                    self.sound_manager.play_sound("searchterminated")
                    self.animate(images) #robot.config.animations[11])
                    print('menu waiting for keyboard input')
                    return False
                #frame=cv2.resize(frame,(600,400),fx=0,fy=0, interpolation = cv2.INTER_NEAREST)
                cv2.imshow("tracker", frame)
        

    def _create_pipeline2(self, robot):
        # Create pipeline
        pipeline = dai.Pipeline()
        
        # Define sources and outputs
        camRgb = pipeline.create(dai.node.ColorCamera)
        
        camRgb.setPreviewKeepAspectRatio(False)#
        spatialDetectionNetwork = pipeline.create(dai.node.MobileNetSpatialDetectionNetwork)
        monoLeft = pipeline.create(dai.node.MonoCamera)
        monoRight = pipeline.create(dai.node.MonoCamera)
        stereo = pipeline.create(dai.node.StereoDepth)
        objectTracker = pipeline.create(dai.node.ObjectTracker)

        xoutRgb = pipeline.create(dai.node.XLinkOut)
        trackerOut = pipeline.create(dai.node.XLinkOut)
        #xoutDepth = pipeline.create(dai.node.XLinkOut) # test may remove
        
        xoutRgb.setStreamName("preview")
        trackerOut.setStreamName("tracklets")
        #xoutDepth.setStreamName("depth") # test may remove
        
        # Properties
        camRgb.setPreviewSize(300, 300)
        camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_2024X1520)#
        camRgb.setInterleaved(False)
        camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        #camRgb.setImageOrientation(dai.CameraImageOrientation.ROTATE_180_DEG)
        #camRgb.setFps(15)
        
        monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)#
        monoLeft.setCamera("left")
        monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_800_P)#
        monoRight.setCamera("right")

        # setting node configs
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        # Align depth map to the perspective of RGB camera, on which inference is done
        stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)
        stereo.setOutputSize(monoLeft.getResolutionWidth(), monoLeft.getResolutionHeight())
        
        # Better handling for occlusions:
        stereo.setLeftRightCheck(True)
        # Closer-in minimum depth, disparity range is doubled:
        stereo.setExtendedDisparity(False)
        # Better accuracy for longer distance, fractional disparity 32-levels:
        stereo.setSubpixel(False)

        spatialDetectionNetwork.setBlobPath('/home/raspberrypi/depthai-python/examples/models/mobilenet-ssd_openvino_2021.4_5shave.blob')
        spatialDetectionNetwork.setConfidenceThreshold(0.70)
        spatialDetectionNetwork.input.setBlocking(False)
        spatialDetectionNetwork.setBoundingBoxScaleFactor(0.5)
        spatialDetectionNetwork.setDepthLowerThreshold(100)
        spatialDetectionNetwork.setDepthUpperThreshold(5000)
        
        objectTracker.setDetectionLabelsToTrack([15])  # track only person
        # possible tracking types: ZERO_TERM_COLOR_HISTOGRAM, ZERO_TERM_IMAGELESS, SHORT_TERM_IMAGELESS, SHORT_TERM_KCF
        objectTracker.setTrackerType(dai.TrackerType.ZERO_TERM_IMAGELESS)
        # take the smallest ID when new object is tracked, possible options: SMALLEST_ID, UNIQUE_ID
        objectTracker.setTrackerIdAssignmentPolicy(dai.TrackerIdAssignmentPolicy.SMALLEST_ID)

        # Linking
        monoLeft.out.link(stereo.left)
        monoRight.out.link(stereo.right)
        camRgb.preview.link(spatialDetectionNetwork.input)
        objectTracker.passthroughTrackerFrame.link(xoutRgb.input)
        objectTracker.out.link(trackerOut.input)
        
        spatialDetectionNetwork.passthrough.link(objectTracker.inputTrackerFrame)
        #objectTracker.inputTrackerFrame.setBlocking(False)# testing
        spatialDetectionNetwork.passthrough.link(objectTracker.inputDetectionFrame)
        spatialDetectionNetwork.out.link(objectTracker.inputDetections)
        stereo.depth.link(spatialDetectionNetwork.inputDepth)
        return pipeline
   
    def _end(self, robot, success=False, give_biscuit_on_success=True):
        if not self.biscuit_mode:
            # Person Search Mode
            time.sleep(0.1)
            if success:
                images=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')
                self.sound_manager.play_sound("found-person")
                self.animate(images) #robot.config.animations[8])
                #robot.say(f'I think I found a {self.label}')
            else:
                images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images) #robot.config.animations[11])
                robot.say("what is going on")
            
            #robot.animate(1)
            print(f'well, hello there, I think I found a {self.label}')
            time.sleep(0.2)

            return RobotState.IDLE
        else:
            # Biscuit giving mode
            if not success:
                time.sleep(0.1)
                images=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')
                self.sound_manager.play_sound("searchterminated")
                self.animate(images) #robot.config.animations[11])
                #robot.say('Search mode Terminated.')
                #robot.animate(1)
                robot.play_sound('Radar_bleep_chirp')
                time.sleep(0.2)

                return RobotState.IDLE

            elif success and give_biscuit_on_success:
                self._give_biscuit(robot)

                return RobotState.BISCUIT
            else:
                time.sleep(0.1)
                images=load_images('/home/raspberrypi/MiniMax/Animations/objective/')
                self.sound_manager.play_sound("objective")
                self.animate(images) #robot.config.animations[9])
                #robot.say("Objective reached.")
                #robot.animate(1)
                time.sleep(0.5)
                #robot.say("Woo Hoo, Yay.")
                #robot.animate(1)
                #time.sleep(0.2)
                robot.play_sound('celebrate1')
                time.sleep(0.2)
                robot.play_sound('Da_de_la')
                time.sleep(0.2)
                robot.play_sound('celebrate1')
                print('focus on terminal')
                time.sleep(0.1)
                return RobotState.PERSON_SEARCH
    
    def _give_biscuit(self, robot):
        time.sleep(1)
        robot.say('If you want a jelly bean, take one from my tray')
        #robot.animate(1)
        time.sleep(0.1)
        start=time.time()
        elapsed = 0
        while elapsed < robot.config.biscuit_wait_time:
            end = time.time()
            elapsed = end - start
            robot.say(str(int(robot.config.biscuit_wait_time - elapsed)))
            time.sleep(1)
        robot.say('The jelly beans are leaving now bye bye')
        #robot.animate(1)
        robot.write_serial('9z')  # make robot do a 180 degree turn
        robot.write_serial('2z')  # stop robot
        time.sleep(1)

    def get_key(self) -> str:
        if self.biscuit_mode and self.label == 'person':
            return 'b'
        elif not self.biscuit_mode and self.label == 'person':
            return 'p'
        elif self.biscuit_mode and self.label == 'cup':
            return 'v'
        elif not self.biscuit_mode and self.label == 'cup':
            return 'c'
    
    def get_state(self) -> RobotState:
        if self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'person':
            return RobotState.PERSON_SEARCH_NO_GIVE_BISCUIT
        elif self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_GIVE_BISCUIT
        elif not self.biscuit_mode and self.label == 'cup':
            return RobotState.CUP_SEARCH_NO_GIVE_BISCUIT

# === ./src/old/animation_manager_load_27_animations_at_start.py ===
import time
import os
import pygame
from .sound_manager import SoundManger
from natsort import natsorted
import numpy as np
global resx, resy, yoffset, imp, initPygameOnce, animations
resx=1280
resy=1024
yoffset=70
initPygameOnce=0
animations=[]

pygame.init()
#display = pygame.display.set_mode((1024, 768))
#animations.append(pygame.image.load('/home/raspberrypi/MiniMax/Animations/powerdown/out_001.jpg'))
display = pygame.display.set_mode((0,0), pygame.FULLSCREEN)
#display = pygame.display.set_mode((1024, 768))
#imp = pygame.image.load('/home/raspberrypi/MiniMax/powerdown/001.jpg').convert()
def load_images(path):
    store =[]
    q=0
    for file_name in os.listdir(path):
        temp=file_name
        q=q+1
        if q>=2:
            store.append(temp)
            q=0
    store=natsorted(store)  
    images = []
    image_array=[]
    for xyz in store:
        pic=path+xyz
        image_array.append(pic)
    for names in image_array:
        imagine = pygame.image.load(names)
        images.append(imagine)
    store.clear()
    imagine=[]
    image_array.clear()
    return images

class AnimationManager:
    def __init__(self, config) -> None:
        self.display = display # Reuse display from the call to pygame.display.set_mode() on import
        #self.display = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
        #self.background = pygame.Surface(self.display.get_size())
        #self.clock = pygame.time.Clock()
        self.animate_delay = config.animate_delay
        #self.sound_manager = SoundManger
        self.channel = pygame.mixer.Channel(1)

    def animate(self, images):
        closedmouth = pygame.image.load('/home/raspberrypi/MiniMax/Animations/advised/outputFile_001.jpg')
        loops = 0
        run = True
        length=len(images)
        #print('starting the loop',length)
        while loops < length:      
            #frt=show_fps()
            #print(loops)
            intloops = int(loops)
            image = images[intloops]
            self.display.blit(image, (0, 0))
            pygame.display.flip()
            loops = loops + 0.4
        #time.sleep(0.05)
        self.display.blit(closedmouth, (0, 0)) # end animation with closed mouth .jpg 
        pygame.display.flip()
        
    def load_animations(self):
        print('Load Animations')
        '''temp=load_images('/home/raspberrypi/MiniMax/Animations/greetings/')#0
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/seeyou/')#1
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/powerdown/')#2
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/advised/')#3
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/agegender/')#4
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/disagegender/')#5
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/disemotional/')#6
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/emotional/')#7
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/found-person/')#8
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/objective/')#9
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/searchmode-off/')#10
        animations.append(temp)
        print('10 Loaded')
        temp=load_images('/home/raspberrypi/MiniMax/Animations/searchterminated/')#11
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/backwards/')#12
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/cautionmovingbackwards/')#13
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/cautionmovingforward/')#14
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/found-you/')#15
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/helpme/')#16
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/inmyway/')#17
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/search-on/')#18
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/search-person/')#19
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/left/')#20
        animations.append(temp)
        print('20 Loaded')
        temp=load_images('/home/raspberrypi/MiniMax/Animations/movingback/')#21
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/movingforward/')#22
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/movingleft/')#23
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/movingright/')#24
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/neutral/')#25
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/right/')#26
        animations.append(temp)
        temp=load_images('/home/raspberrypi/MiniMax/Animations/forwards/')#27
        animations.append(temp)'''
        print('27 Loaded')
        return animations
       


# === ./src/old/motoron.py ===
# Copyright (C) Pololu Corporation.  See LICENSE.txt for details.

import math
import os
import struct
import time

try:
  from enum import Enum
  def enum_value(x): return x.value
except ImportError:
  Enum = object
  def enum_value(x): return x

from motoron_protocol import *

## \file motoron.py
##
## This is the main file for the Motoron Motor Controller Python library for
## Raspberry Pi.
##
## For more information about the library, see the main repository at:
## https://github.com/pololu/motoron-python

class CurrentSenseType(Enum):
  MOTORON_18V18 = 0b0001
  MOTORON_24V14 = 0b0101
  MOTORON_18V20 = 0b1010
  MOTORON_24V16 = 0b1101

class VinSenseType(Enum):
  MOTORON_256 =  0b0000  # M*256 Motorons
  MOTORON_HP  =  0b0010  # High-power Motorons
  MOTORON_550 =  0b0011  # M*550 Motorons

class MotoronBase():
  """
  Represents a connection to a Pololu Motoron Motoron Controller.
  """

  __DEFAULT_PROTOCOL_OPTIONS = (
    (1 << PROTOCOL_OPTION_I2C_GENERAL_CALL) |
    (1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS) |
    (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  ## The default value of the Motoron's "Error mask" variable.
  DEFAULT_ERROR_MASK = (
    (1 << STATUS_FLAG_COMMAND_TIMEOUT) |
    (1 << STATUS_FLAG_RESET))

  def __init__(self):
    ## The bits in this variable are defined by the
    ## motoron.PROTOCOL_OPTION_* constants.  See set_protocol_options().
    self.protocol_options = MotoronBase.__DEFAULT_PROTOCOL_OPTIONS

  def get_firmware_version(self):
    """
    Sends the "Get firmware version" command to get the device's firmware
    product ID and firmware version numbers.

    For more information, see the "Get firmware version"
    command in the Motoron user's guide.

    \return A dictionary in this format:
    ```{.py}
    {'product_id': 204, 'firmware_version': {'major': 1, 'minor': 0}}
    ```
    """
    cmd = [CMD_GET_FIRMWARE_VERSION]
    response = self._send_command_and_read_response(cmd, 4)
    product_id, minor, major = struct.unpack('<HBB', response)
    return {
      'product_id': product_id,
      'firmware_version': {'major': major, 'minor': minor}
    }

  def set_protocol_options(self, options):
    """
    Sends the "Set protocol options" command to the device to specify options
    related to how the device processes commands and sends responses.
    The options are also saved in this object and are used later
    when sending other commands or reading responses.

    When CRC for commands is enabled, this library generates the CRC
    byte and appends it to the end of each command it sends.  The Motoron
    checks it to help ensure the command was received correctly.

    When CRC for responses is enabled, this library reads the CRC byte sent
    by the Motoron in its responses and makes sure it is correct.  If the
    response CRC byte is incorrect, get_last_error() will return a non-zero
    error code after the command has been run.

    When the I2C general call address is enabled, the Motoron receives
    commands sent to address 0 in addition to its usual I2C address.
    The general call address is write-only; reading bytes from it is not
    supported.

    By default (in this libary and the Motoron itself), CRC for commands and
    responses is enabled, and the I2C general call address is enabled.

    This method always sends its command with a CRC byte, so it will work
    even if CRC was previously disabled but has been re-enabled on the device
    (e.g. due to a reset).

    The \p options argument should be 0 or combination of the following
    expressions made using the bitwise or operator (|):
    - (1 << motoron.PROTOCOL_OPTION_CRC_FOR_COMMANDS)
    - (1 << motoron.PROTOCOL_OPTION_CRC_FOR_RESPONSES)
    - (1 << motoron.PROTOCOL_OPTION_I2C_GENERAL_CALL)

    For more information, see the "Set protocol optons"
    command in the Motoron user's guide.

    \sa enable_crc(), disable_crc(),
      enable_crc_for_commands(), disable_crc_for_commands(),
      enable_crc_for_responses(), disable_crc_for_responses(),
      enable_i2c_general_call(), disable_i2c_general_call()
    """
    self.protocol_options = options
    cmd = [
      CMD_SET_PROTOCOL_OPTIONS,
      options & 0x7F,
      ~options & 0x7F
    ]
    self._send_command_core(cmd, True)

  def set_protocol_options_locally(self, options):
    """
    Sets the protocol options for this object, without sending a command to
    the Motoron.

    If the options you specify here do not match the actual configuration of
    the Motoron, future communication could fail.

    Most users should use set_protocol_options() instead of this.
    """
    self.protocol_options = options

  def enable_crc(self):
    """
     Enables CRC for commands and responses.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      | (1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS)
      | (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  def disable_crc(self):
    """
     Disables CRC for commands and responses.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      & ~(1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS)
      & ~(1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  def enable_crc_for_commands(self):
    """
    Enables CRC for commands.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      | (1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS))

  def disable_crc_for_commands(self):
    """
    Disables CRC for commands.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      & ~(1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS))

  def enable_crc_for_responses(self):
    """
    Enables CRC for responses.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      | (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  def disable_crc_for_responses(self):
    """
    Disables CRC for responses.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      & ~(1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  def enable_i2c_general_call(self):
    """
    Enables the I2C general call address.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      | (1 << PROTOCOL_OPTION_I2C_GENERAL_CALL))

  def disable_i2c_general_call(self):
    """
    Disables the I2C general call address.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      & ~(1 << PROTOCOL_OPTION_I2C_GENERAL_CALL))

  def read_eeprom(self, offset, length):
    """
    Reads the specified bytes from the Motoron's EEPROM memory.

    For more information, see the "Read EEPROM" command in the
    Motoron user's guide.
    """
    cmd = [
      CMD_READ_EEPROM,
      offset & 0x7F,
      length & 0x7F,
    ]
    return self._send_command_and_read_response(cmd, length)

  def read_eeprom_device_number(self):
    """
    Reads the EEPROM device number from the device.
    This is the I2C address that the device uses if it detects that JMP1
    is shorted to GND when it starts up.  It is stored in non-volatile
    EEPROM memory.
    """
    return self.read_eeprom(SETTING_DEVICE_NUMBER, 1)[0]

  def write_eeprom(self, offset, value):
    """
    Writes a value to one byte in the Motoron's EEPROM memory.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**

    For more information, see the "Write EEPROM" command in the
    Motoron user's guide.
    """
    cmd = [
      CMD_WRITE_EEPROM,
      offset & 0x7F,
      value & 0x7F,
      (value >> 7) & 1,
    ]
    cmd += [
      cmd[1] ^ 0x7F,
      cmd[2] ^ 0x7F,
      cmd[3] ^ 0x7F,
    ]
    self._send_command(cmd)
    time.sleep(0.006)

  def write_eeprom16(self, offset, value):
    """
    Writes a 2-byte value in the Motoron's EEPROM memory.

    This command only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron’s microcontroller is only rated for
    100,000 erase/write cycles.
    """
    self.write_eeprom(offset, value & 0xFF)
    self.write_eeprom(offset + 1, value >> 8 & 0xFF)

  def write_eeprom_device_number(self, number):
    """
    Writes to the EEPROM device number, changing it to the specified value.

    This command only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**
    """
    self.write_eeprom(SETTING_DEVICE_NUMBER, number & 0x7F)
    self.write_eeprom(SETTING_DEVICE_NUMBER + 1, number >> 7 & 0x7F)

  def write_eeprom_alternative_device_number(self, number):
    """
    Writes to the alternative device number stored in EEPROM, changing it to
    the specified value.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**

    \sa write_eeprom_disable_alternative_device_number()
    """
    self.write_eeprom(SETTING_ALTERNATIVE_DEVICE_NUMBER, (number & 0x7F) | 0x80)
    self.write_eeprom(SETTING_ALTERNATIVE_DEVICE_NUMBER + 1, number >> 7 & 0x7F)

  def write_eeprom_disable_alternative_device_number(self):
    """
    Writes to EEPROM to disable the alternative device number.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**

    \sa write_eeprom_alternative_device_number()
    """
    self.write_eeprom(SETTING_ALTERNATIVE_DEVICE_NUMBER, 0)
    self.write_eeprom(SETTING_ALTERNATIVE_DEVICE_NUMBER + 1, 0)

  def write_eeprom_communication_options(self, options):
    """
    Writes to the serial options byte stored in EEPROM, changing it to
    the specified value.

    The bits in this byte are defined by the
    MOTORON_COMMUNICATION_OPTION_* constants.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**
    """
    self.write_eeprom(SETTING_COMMUNICATION_OPTIONS, options)

  def write_eeprom_baud_rate(self, baud):
    """
    Writes to the baud rate stored in EEPROM, changing it to the
    specified value.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**
    """
    if (baud < MIN_BAUD_RATE): baud = MIN_BAUD_RATE
    if (baud > MAX_BAUD_RATE): baud = MAX_BAUD_RATE
    self.write_eeprom16(SETTING_BAUD_DIVIDER, round(16000000 / baud))

  def write_eeprom_response_delay(self, delay):
    """
    Writes to the serial response delay setting stored in EEPROM, changing
    it to the specified value, in units of microseconds.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**
    """
    self.write_eeprom(SETTING_RESPONSE_DELAY, delay)

  def reinitialize(self):
    """
    Sends a "Reinitialize" command to the Motoron, which resets most of the
    Motoron's variables to their default state.

    For more information, see the "Reinitialize" command in the Motoron
    user's guide.

    \sa reset()
    """
    # Always send the reinitialize command with a CRC byte to make it more reliable.
    cmd = [CMD_REINITIALIZE]
    self._send_command_core(cmd, True)
    self.protocol_options = MotoronBase.__DEFAULT_PROTOCOL_OPTIONS

  def reset(self, ignore_nack=True):
    """
    Sends a "Reset" command to the Motoron, which does a full hardware reset.

    This command is equivalent to briefly driving the Motoron's RST pin low.
    The Motoron's RST is briefly driven low by the Motoron itself as a
    result of this command.

    After running this command, we recommend waiting for at least 5
    milliseconds before you try to communicate with the Motoron.

    \param ignore_nack Optional argument: if `True` (the default), this method
      ignores a NACK error if it occurs on sending the Reset command. This is
      useful in case the Motoron has CRC off and executes the reset before it
      can ACK the CRC byte (which this method always sends to make it more
      reliable).

    \sa reinitialize()
    """
    # Always send the reset command with a CRC byte to make it more reliable.
    cmd = [CMD_RESET]
    try:
      self._send_command_core(cmd, True)
    except OSError as e:
      # Errno 5 (Input/output error) or 121 (Remote I/O error) indicates a
      # NACK of a data byte.  Ignore it if the ignore_nack argument is True.
      # In all other cases, re-raise the exception.
      if not (ignore_nack and (e.args[0] == 5 or e.args[0] == 121)): raise
    self.protocol_options = MotoronBase.__DEFAULT_PROTOCOL_OPTIONS

  def get_variables(self, motor, offset, length):
    """
    Reads information from the Motoron using a "Get variables" command.

    This library has helper methods to read every variable, but this method
    is useful if you want to get the raw bytes, or if you want to read
    multiple consecutive variables at the same time for efficiency.

    \param motor 0 to read general variables, or a motor number to read
      motor-specific variables.
    \param offset The location of the first byte to read.
    \param length How many bytes to read.
    """
    cmd = [
      CMD_GET_VARIABLES,
      motor & 0x7F,
      offset & 0x7F,
      length & 0x7F,
    ]
    return self._send_command_and_read_response(cmd, length)

  def get_var_u8(self, motor, offset):
    """
    Reads one byte from the Motoron using a "Get variables" command
    and returns the result as an unsigned 8-bit integer.

    \param motor 0 to read a general variable, or a motor number to read
      a motor-specific variable.
    \param offset The location of the byte to read.
    """
    return self.get_variables(motor, offset, 1)[0]

  def get_var_u16(self, motor, offset):
    """
    Reads two bytes from the Motoron using a "Get variables" command
    and returns the result as an unsigned 16-bit integer.

    \param motor 0 to read general variables, or a motor number to read
      motor-specific variables.
    \param offset The location of the first byte to read.
    """
    buffer = self.get_variables(motor, offset, 2)
    return struct.unpack('<H', buffer)[0]

  def get_var_s16(self, motor, offset):
    """
    Reads two bytes from the Motoron using a "Get variables" command
    and returns the result as a signed 16-bit integer.

    \param motor 0 to read general variables, or a motor number to read
      motor-specific variables.
    \param offset The location of the first byte to read.
    """
    buffer = self.get_variables(motor, offset, 2)
    return struct.unpack('<h', buffer)[0]

  def get_status_flags(self):
    """
    Reads the "Status flags" variable from the Motoron.

    The bits in this variable are defined by the STATUS_FLAGS_*
    constants in the motoron package:

    - motoron.STATUS_FLAG_PROTOCOL_ERROR
    - motoron.STATUS_FLAG_CRC_ERROR
    - motoron.STATUS_FLAG_COMMAND_TIMEOUT_LATCHED
    - motoron.STATUS_FLAG_MOTOR_FAULT_LATCHED
    - motoron.STATUS_FLAG_NO_POWER_LATCHED
    - motoron.STATUS_FLAG_RESET
    - motoron.STATUS_FLAG_COMMAND_TIMEOUT
    - motoron.STATUS_FLAG_MOTOR_FAULTING
    - motoron.STATUS_FLAG_NO_POWER
    - motoron.STATUS_FLAG_ERROR_ACTIVE
    - motoron.STATUS_FLAG_MOTOR_OUTPUT_ENABLED
    - motoron.STATUS_FLAG_MOTOR_DRIVING

    Here is some example code that uses bitwise operators to check
    whether there is currently a motor fault or a lack of power:

    ```{.py}
    mask = ((1 << motoron.STATUS_FLAG_NO_POWER) |
      (1 << motoron.STATUS_FLAG_MOTOR_FAULTING))
    if mc.get_status_flags() & mask: # do something
    ```

    This library has helper methods that make it easier if you just want to
    read a single bit:

    - get_protocol_error_flag()
    - get_crc_error_flag()
    - get_command_timeout_latched_flag()
    - get_motor_fault_latched_flag()
    - get_no_power_latched_flag()
    - get_reset_flag()
    - get_motor_faulting_flag()
    - get_no_power_flag()
    - get_error_active_flag()
    - get_motor_output_enabled_flag()
    - get_motor_driving_flag()

    The clear_latched_status_flags() method sets the specified set of latched
    status flags to 0.  The reinitialize() and reset() commands reset the
    latched status flags to their default values.

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return self.get_var_u16(0, VAR_STATUS_FLAGS)

  def get_protocol_error_flag(self):
    """
    Returns the "Protocol error" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_PROTOCOL_ERROR))

  def get_crc_error_flag(self):
    """
    Returns the "CRC error" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_CRC_ERROR))

  def get_command_timeout_latched_flag(self):
    """
    Returns the "Command timeout latched" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_COMMAND_TIMEOUT_LATCHED))

  def get_motor_fault_latched_flag(self):
    """
    Returns the "Motor fault latched" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_MOTOR_FAULT_LATCHED))

  def get_no_power_latched_flag(self):
    """
    Returns the "No power latched" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_NO_POWER_LATCHED))

  def get_reset_flag(self):
    """
    Returns the "Reset" bit from get_status_flags().

    This bit is set to 1 when the Motoron powers on, its processor is
    reset (e.g. by reset()), or it receives a reinitialize() command.
    It can be cleared using clear_reset_flag() or clear_latched_status_flags().

    By default, the Motoron is configured to treat this bit as an error,
    so you will need to clear it before you can turn on the motors.

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_RESET))

  def get_motor_faulting_flag(self):
    """
    Returns the "Motor faulting" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_MOTOR_FAULTING))

  def get_no_power_flag(self):
    """
    Returns the "No power" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_NO_POWER))

  def get_error_active_flag(self):
    """
    Returns the "Error active" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_ERROR_ACTIVE))

  def get_motor_output_enabled_flag(self):
    """
    Returns the "Motor output enabled" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_MOTOR_OUTPUT_ENABLED))

  def get_motor_driving_flag(self):
    """
    Returns the "Motor driving" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_MOTOR_DRIVING))

  def get_vin_voltage(self):
    """
    Reads voltage on the Motoron's VIN pin, in raw device units.

    For more information, see the "VIN voltage" variable in the Motoron
    user's guide.

    \sa get_vin_voltage_mv()
    """
    return self.get_var_u16(0, VAR_VIN_VOLTAGE)

  def get_vin_voltage_mv(self, reference_mv=3300, type=VinSenseType.MOTORON_256):
    """
    Reads the voltage on the Motoron's VIN pin and converts it to millivolts.

    For more information, see the "VIN voltage" variable in the Motoron
    user's guide.

    \param reference_mv The logic voltage of the Motoron, in millivolts.
      This is assumed to be 3300 by default.
    \param type Specifies what type of Motoron you are using.  This should be one
      of the members of the motoron.VinSenseType enum.

    \sa get_vin_voltage()
    """
    scale = 459 if enum_value(type) & 1 else 1047
    return self.get_vin_voltage() * reference_mv / 1024 * scale / 47

  def get_command_timeout_milliseconds(self):
    """
    Reads the "Command timeout" variable and converts it to milliseconds.

    For more information, see the "Command timeout" variable in the Motoron
    user's guide.

    \sa set_command_timeout_milliseconds()
    """
    return self.get_var_u16(0, VAR_COMMAND_TIMEOUT) * 4

  def get_error_response(self):
    """
    Reads the "Error response" variable, which defines how the Motoron will
    stop its motors when an error is happening.

    For more information, see the "Error response" variable in the Motoron
    user's guide.

    \sa set_error_response()
    """
    return self.get_var_u8(0, VAR_ERROR_RESPONSE)

  def get_error_mask(self):
    """
    Reads the "Error mask" variable, which defines which status flags are
    considered to be errors.

    For more information, see the "Error mask" variable in the Motoron
    user's guide.

    \sa set_error_mask()
    """
    return self.get_var_u16(0, VAR_ERROR_MASK)

  def get_jumper_state(self):
    """
    Reads the "Jumper state" variable.

    For more information, see the "Jumper state" variable in the Motoron
    user's guide
    """
    return self.get_var_u8(0, VAR_JUMPER_STATE)

  def get_target_speed(self, motor):
    """
    Reads the target speed of the specified motor, which is the speed at
    which the motor has been commanded to move.

    For more information, see the "Target speed" variable in the Motoron
    user's guide.

    \sa set_speed(), set_all_speeds(), set_all_speeds_using_buffers()
    """
    return self.get_var_s16(motor, MVAR_TARGET_SPEED)

  def get_target_brake_amount(self, motor):
    """
    Reads the target brake amount for the specified motor.

    For more information, see the "Target speed" variable in the Motoron
    user's guide.

    \sa set_target_brake_amount()
    """
    return self.get_var_u16(motor, MVAR_TARGET_BRAKE_AMOUNT)

  def get_current_speed(self, motor):
    """
    Reads the current speed of the specified motor, which is the speed that
    the Motoron is currently trying to apply to the motor.

    For more information, see the "Target speed" variable in the Motoron
    user's guide.

    \sa set_speed_now(), set_all_speeds_now(), set_all_speeds_now_using_buffers()
    """
    return self.get_var_s16(motor, MVAR_CURRENT_SPEED)

  def get_buffered_speed(self, motor):
    """
    Reads the buffered speed of the specified motor.

    For more information, see the "Buffered speed" variable in the Motoron
    user's guide.

    \sa set_buffered_speed(), set_all_buffered_speeds()
    """
    return self.get_var_s16(motor, MVAR_BUFFERED_SPEED)

  def get_pwm_mode(self, motor):
    """
    Reads the PWM mode of the specified motor.

    For more information, see the "PWM mode" variable in the Motoron
    user's guide.

    \sa set_pwm_mode()
    """
    return self.get_var_u8(motor, MVAR_PWM_MODE)

  def get_max_acceleration_forward(self, motor):
    """
    Reads the maximum acceleration of the specified motor for the forward
    direction.

    For more information, see the "Max acceleration forward" variable in the
    Motoron user's guide.

    \sa set_max_acceleration(), set_max_acceleration_forward()
    """
    return self.get_var_u16(motor, MVAR_MAX_ACCEL_FORWARD)

  def get_max_acceleration_reverse(self, motor):
    """
    Reads the maximum acceleration of the specified motor for the reverse
    direction.

    For more information, see the "Max acceleration reverse" variable in the
    Motoron user's guide.

    \sa set_max_acceleration(), set_max_acceleration_reverse()
    """
    return self.get_var_u16(motor, MVAR_MAX_ACCEL_REVERSE)

  def get_max_deceleration_forward(self, motor):
    """
    Reads the maximum deceleration of the specified motor for the forward
    direction.

    For more information, see the "Max deceleration forward" variable in the
    Motoron user's guide.

    \sa set_max_deceleration(), set_max_deceleration_forward()
    """
    return self.get_var_u16(motor, MVAR_MAX_DECEL_FORWARD)

  def get_max_deceleration_reverse(self, motor):
    """
    Reads the maximum deceleration of the specified motor for the reverse
    direction.

    For more information, see the "Max deceleration reverse" variable in the
    Motoron user's guide.

    \sa set_max_deceleration(), set_max_deceleration_reverse()
    """
    return self.get_var_u16(motor, MVAR_MAX_DECEL_REVERSE)


# \cond

  # This function is used by Pololu for testing.
  def get_max_deceleration_temporary(self, motor):
    return self.get_var_u16(motor, MVAR_MAX_DECEL_TMP)

# \endcond

  def get_starting_speed_forward(self, motor):
    """
    Reads the starting speed for the specified motor in the forward direction.

    For more information, see the "Starting speed forward" variable in the
    Motoron user's guide.

    \sa set_starting_speed(), set_starting_speed_forward()
    """
    return self.get_var_u16(motor, MVAR_STARTING_SPEED_FORWARD)

  def get_starting_speed_reverse(self, motor):
    """
    Reads the starting speed for the specified motor in the reverse direction.

    For more information, see the "Starting speed reverse" variable in the
    Motoron user's guide.

    \sa set_starting_speed(), set_starting_speed_reverse()
    """
    return self.get_var_u16(motor, MVAR_STARTING_SPEED_REVERSE)

  def get_direction_change_delay_forward(self, motor):
    """
    Reads the direction change delay for the specified motor in the
    forward direction.

    For more information, see the "Direction change delay forward" variable
    in the Motoron user's guide.

    \sa set_direction_change_delay(), set_direction_change_delay_forward()
    """
    return self.get_var_u8(motor, MVAR_DIRECTION_CHANGE_DELAY_FORWARD)

  def get_direction_change_delay_reverse(self, motor):
    """
    Reads the direction change delay for the specified motor in the
    reverse direction.

    For more information, see the "Direction change delay reverse" variable
    in the Motoron user's guide.

    \sa set_direction_change_delay(), set_direction_change_delay_reverse()
    """
    return self.get_var_u8(motor, MVAR_DIRECTION_CHANGE_DELAY_REVERSE)

  def get_current_limit(self, motor):
    """
    Reads the current limit for the specified motor.

    This only works for the high-power Motorons.

    For more information, see the "Current limit" variable in the Motoron user's
    guide.

    \sa set_current_limit()
    """
    return self.get_var_u16(motor, MVAR_CURRENT_LIMIT)

  def get_current_sense_reading(self, motor):
    """
    Reads all the results from the last current sense measurement for the
    specified motor.

    This function reads the "Current sense raw", "Current sense speed", and
    "Current sense processed" variables from the Motoron using a single
    command, so the values returned are all guaranteed to be part of the
    same measurement.

    This only works for the high-power Motorons.

    \sa get_current_sense_raw_and_speed(), get_current_sense_processed_and_speed()
    """
    buffer = self.get_variables(motor, MVAR_CURRENT_SENSE_RAW, 6)
    raw, speed, processed = struct.unpack('<HhH', buffer)
    return { 'raw': raw, 'speed': speed, 'processed': processed }

  def get_current_sense_raw_and_speed(self, motor):
    """
    This is like get_current_sense_reading() but it only reads the raw current
    sense measurement and the speed.

    This only works for the high-power Motorons.
    """
    buffer = self.get_variables(motor, MVAR_CURRENT_SENSE_RAW, 4)
    raw, speed = struct.unpack('<Hh', buffer)
    return { 'raw': raw, 'speed': speed }

  def get_current_sense_processed_and_speed(self, motor):
    """
    This is like get_current_sense_reading() but it only reads the processed
    current sense measurement and the speed.

    This only works for the high-power Motorons.
    """
    buffer = self.get_variables(motor, MVAR_CURRENT_SENSE_SPEED, 4)
    speed, processed = struct.unpack('<hH', buffer)
    return { 'speed': speed, 'processed': processed }

  def get_current_sense_raw(self, motor):
    """
    Reads the raw current sense measurement for the specified motor.

    This only works for the high-power Motorons.

    For more information, see the "Current sense raw" variable
    in the Motoron user's guide.

    \sa get_current_sense_reading()
    """
    return self.get_var_u16(motor, MVAR_CURRENT_SENSE_RAW)


  def get_current_sense_processed(self, motor):
    """
    Reads the processed current sense reading for the specified motor.

    This only works for the high-power Motorons.

    The units of this reading depend on the logic voltage of the Motoron
    and on the specific model of Motoron that you have, and you can use
    current_sense_units_milliamps() to calculate the units.

    The accuracy of this reading can be improved by measuring the current
    sense offset and setting it with set_current_sense_offset().
    See the "Current sense processed" variable in the Motoron user's guide for
    or the current_sense_calibrate example for more information.

    Note that this reading will be 0xFFFF if an overflow happens during the
    calculation due to very high current.

    \sa get_current_sense_processed_and_speed()
    """
    return self.get_var_u16(motor, MVAR_CURRENT_SENSE_PROCESSED)

  def get_current_sense_offset(self, motor):
    """
    Reads the current sense offset setting.

    This only works for the high-power Motorons.

    For more information, see the "Current sense offset" variable in the
    Motoron user's guide.

    \sa set_current_sense_offset()
    """
    return self.get_var_u8(motor, MVAR_CURRENT_SENSE_OFFSET)

  def get_current_sense_minimum_divisor(self, motor):
    """
    Reads the current sense minimum divisor setting and returns it as a speed
    between 0 and 800.

    This only works for the high-power Motorons.

    For more information, see the "Current sense minimum divisor" variable in
    the Motoron user's guide.

    \sa set_current_sense_minimum_divisor()
    """
    return self.get_var_u8(motor, MVAR_CURRENT_SENSE_MINIMUM_DIVISOR) << 2


  def set_variable(self, motor, offset, value):
    """
    Configures the Motoron using a "Set variable" command.

    This library has helper methods to set every variable, so you should
    not need to call this function directly.

    \param motor 0 to set a general variable, or a motor number to set
      motor-specific variables.
    \param offset The address of the variable to set (only certain offsets
      are allowed).
    \param value The value to set the variable to.

    \sa get_variables()
    """
    if value > 0x3FFF: value = 0x3FFF
    cmd = [
      CMD_SET_VARIABLE,
      motor & 0x1F,
      offset & 0x7F,
      value & 0x7F,
      (value >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_command_timeout_milliseconds(self, ms):
    """
    Sets the command timeout period, in milliseconds.

    For more information, see the "Command timeout" variable
    in the Motoron user's guide.

    \sa disable_command_timeout(), get_command_timeout_milliseconds()
    """
    # Divide by 4, but round up.
    timeout = math.ceil(ms / 4)
    self.set_variable(0, VAR_COMMAND_TIMEOUT, timeout)

  def set_error_response(self, response):
    """
    Sets the error response, which defines how the Motoron will
    stop its motors when an error is happening.

    The response parameter should be one of these constants from the motoron
    package:

    - motoron.ERROR_RESPONSE_COAST
    - motoron.ERROR_RESPONSE_BRAKE
    - motoron.ERROR_RESPONSE_COAST_NOW
    - motoron.ERROR_RESPONSE_BRAKE_NOW

    For more information, see the "Error response" variable in the Motoron
    user's guide.

    \sa get_error_response()
    """
    self.set_variable(0, VAR_ERROR_RESPONSE, response)

  def set_error_mask(self, mask):
    """
    Sets the "Error mask" variable, which defines which status flags are
    considered to be errors.

    For more information, see the "Error mask" variable in the Motoron
    user's guide.

    \sa get_error_mask(), get_status_flags()
    """
    self.set_variable(0, VAR_ERROR_MASK, mask)

  def disable_command_timeout(self):
    """
    This disables the Motoron's command timeout feature by resetting the
    the "Error mask" variable to its default value but with the command
    timeout bit cleared.

    By default, the Motoron's command timeout will occur if no valid commands
    are received in 1500 milliseconds, and the command timeout is treated as
    an error, so the motors will shut down.  You can use this function if you
    want to disable that feature.

    Note that this function overrides any previous values you set in the
    "Error mask" variable, so if you are using set_error_mask() in your program
    to configure which status flags are treated as errors, you do not need to
    use this function and you probably should not use this function.

    \sa set_command_timeout_milliseconds(), set_error_mask()
    """
    self.set_error_mask(MotoronBase.DEFAULT_ERROR_MASK & ~(1 << STATUS_FLAG_COMMAND_TIMEOUT))

  def set_pwm_mode(self, motor, mode):
    """
    Sets the PWM mode for the specified motor.

    The mode parameter should be one of the following these constants from
    the motoron package:

    - motoron.PWM_MODE_DEFAULT (20 kHz)
    - motoron.PWM_MODE_1_KHZ 1
    - motoron.PWM_MODE_2_KHZ 2
    - motoron.PWM_MODE_4_KHZ 3
    - motoron.PWM_MODE_5_KHZ 4
    - motoron.PWM_MODE_10_KHZ 5
    - motoron.PWM_MODE_20_KHZ 6
    - motoron.PWM_MODE_40_KHZ 7
    - motoron.PWM_MODE_80_KHZ 8

    For more information, see the "PWM mode" variable in the Motoron user's
    guide.

    \sa get_pwm_mode(self)
    """
    self.set_variable(motor, MVAR_PWM_MODE, mode)

  def set_max_acceleration_forward(self, motor, accel):
    """
    Sets the maximum acceleration of the specified motor for the forward
    direction.

    For more information, see the "Max acceleration forward" variable in the
    Motoron user's guide.

    \sa set_max_acceleration(), get_max_acceleration_forward()
    """
    self.set_variable(motor, MVAR_MAX_ACCEL_FORWARD, accel)

  def set_max_acceleration_reverse(self, motor, accel):
    """
    Sets the maximum acceleration of the specified motor for the reverse
    direction.

    For more information, see the "Max acceleration reverse" variable in the
    Motoron user's guide.

    \sa set_max_acceleration(), get_max_acceleration_reverse()
    """
    self.set_variable(motor, MVAR_MAX_ACCEL_REVERSE, accel)

  def set_max_acceleration(self, motor, accel):
    """
    Sets the maximum acceleration of the specified motor (both directions).

    If this function succeeds, it is equivalent to calling
    set_max_acceleration_forward() and set_max_acceleration_reverse().
    """
    self.set_max_acceleration_forward(motor, accel)
    self.set_max_acceleration_reverse(motor, accel)

  def set_max_deceleration_forward(self, motor, decel):
    """
    Sets the maximum deceleration of the specified motor for the forward
    direction.

    For more information, see the "Max deceleration forward" variable in the
    Motoron user's guide.

    \sa set_max_deceleration(), get_max_deceleration_forward()
    """
    self.set_variable(motor, MVAR_MAX_DECEL_FORWARD, decel)

  def set_max_deceleration_reverse(self, motor, decel):
    """
    Sets the maximum deceleration of the specified motor for the reverse
    direction.

    For more information, see the "Max deceleration reverse" variable in the
    Motoron user's guide.

    \sa set_max_deceleration(), get_max_deceleration_reverse()
    """
    self.set_variable(motor, MVAR_MAX_DECEL_REVERSE, decel)

  def set_max_deceleration(self, motor, decel):
    """
    Sets the maximum deceleration of the specified motor (both directions).

    If this function succeeds, it is equivalent to calling
    set_max_deceleration_forward() and set_max_deceleration_reverse().
    """
    self.set_max_deceleration_forward(motor, decel)
    self.set_max_deceleration_reverse(motor, decel)

  def set_starting_speed_forward(self, motor, speed):
    """
    Sets the starting speed of the specified motor for the forward
    direction.

    For more information, see the "Starting speed forward" variable in the
    Motoron user's guide.

    \sa set_starting_speed(), get_starting_speed_forward()
    """
    self.set_variable(motor, MVAR_STARTING_SPEED_FORWARD, speed)

  def set_starting_speed_reverse(self, motor, speed):
    """
    Sets the starting speed of the specified motor for the reverse
    direction.

    For more information, see the "Starting speed reverse" variable in the
    Motoron user's guide.

    \sa set_starting_speed(), get_starting_speed_reverse()
    """
    self.set_variable(motor, MVAR_STARTING_SPEED_REVERSE, speed)

  def set_starting_speed(self, motor, speed):
    """
    Sets the starting speed of the specified motor (both directions).

    If this function succeeds, it is equivalent to calling
    set_starting_speed_forward() and set_starting_speed_reverse().
    """
    self.set_starting_speed_forward(motor, speed)
    self.set_starting_speed_reverse(motor, speed)

  def set_direction_change_delay_forward(self, motor, duration):
    """
    Sets the direction change delay of the specified motor for the forward
    direction, in units of 10 ms.

    For more information, see the "Direction change delay forward" variable
    in the Motoron user's guide.

    \sa set_direction_change_delay(), get_direction_change_delay_forward()
    """
    self.set_variable(motor, MVAR_DIRECTION_CHANGE_DELAY_FORWARD, duration)

  def set_direction_change_delay_reverse(self, motor, duration):
    """
    Sets the direction change delay of the specified motor for the reverse
    direction, in units of 10 ms.

    For more information, see the "Direction change delay reverse" variable
    in the Motoron user's guide.

    \sa set_direction_change_delay(), get_direction_change_delay_reverse()
    """
    self.set_variable(motor, MVAR_DIRECTION_CHANGE_DELAY_REVERSE, duration)

  def set_direction_change_delay(self, motor, duration):
    """
    Sets the direction change delay of the specified motor (both directions),
    in units of 10 ms.

    If this function succeeds, it is equivalent to calling
    set_direction_change_delay_forward() and set_direction_change_delay_reverse().
    """
    self.set_direction_change_delay_forward(motor, duration)
    self.set_direction_change_delay_reverse(motor, duration)

  def set_current_limit(self, motor, limit):
    """
    Sets the current limit for the specified motor.

    This only works for the high-power Motorons.

    The units of the current limit depend on the type of Motoron you have
    and the logic voltage of your system.  See the "Current limit" variable
    in the Motoron user's guide for more information, or see
    calculate_current_limit().

    \sa get_current_limit()
    """
    self.set_variable(motor, MVAR_CURRENT_LIMIT, limit)

  def set_current_sense_offset(self, motor, offset):
    """
    Sets the current sense offset setting for the specified motor.

    This is one of the settings that determines how current sense
    readings are processed.  It is supposed to be the value returned by
    get_current_sense_raw() when motor power is supplied to the Motoron and
    it is driving its motor outputs at speed 0.

    The current_sense_calibrate example shows how to measure the current
    sense offsets and load them onto the Motoron using this function.

    If you do not care about measuring motor current, you do not need to
    set this variable.

    For more information, see the "Current sense offset" variable in the
    Motoron user's guide.

    This only works for the high-power Motorons.

    \sa get_current_sense_offset()
    """
    self.set_variable(motor, MVAR_CURRENT_SENSE_OFFSET, offset)

  def set_current_sense_minimum_divisor(self, motor, speed):
    """
    Sets the current sense minimum divisor setting for the specified motor,
    given a speed between 0 and 800.

    This is one of the settings that determines how current sense
    readings are processed.

    If you do not care about measuring motor current, you do not need to
    set this variable.

    For more information, see the "Current sense minimum divisor" variable in
    the Motoron user's guide.

    This only works for the high-power Motorons.

    \sa get_current_sense_minimum_divisor()
    """
    self.set_variable(motor, MVAR_CURRENT_SENSE_MINIMUM_DIVISOR, speed >> 2)

  def coast_now(self):
    """
    Sends a "Coast now" command to the Motoron, causing all of the motors to
    immediately start coasting.

    For more information, see the "Coast now" command in the Motoron
    user's guide.
    """
    cmd = [CMD_COAST_NOW]
    self._send_command(cmd)

  def clear_motor_fault(self, flags=0):
    """
    Sends a "Clear motor fault" command to the Motoron.

    If any of the Motoron's motors chips are currently experiencing a
    fault (error), or bit 0 of the flags argument is 1, this command makes
    the Motoron attempt to recover from the faults.

    For more information, see the "Clear motor fault" command in the Motoron
    user's guide.

    \sa clear_motor_fault_unconditional(), get_motor_faulting_flag()
    """
    cmd = [ CMD_CLEAR_MOTOR_FAULT, (flags & 0x7F) ]
    self._send_command(cmd)

  def clear_motor_fault_unconditional(self):
    """
    Sends a "Clear motor fault" command to the Motoron with the
    "unconditional" flag set, so the Motoron will attempt to recover
    from any motor faults even if no fault is currently occurring.

    This is a more robust version of clear_motor_fault().
    """
    self.clear_motor_fault(1 << CLEAR_MOTOR_FAULT_UNCONDITIONAL)

  def clear_latched_status_flags(self, flags):
    """
    Clears the specified flags in get_status_flags().

    For each bit in the flags argument that is 1, this command clears the
    corresponding bit in the "Status flags" variable, setting it to 0.

    For more information, see the "Clear latched status flags" command in the
    Motoron user's guide.

    \sa get_status_flags(), set_latched_status_flags()
    """
    cmd = [
      CMD_CLEAR_LATCHED_STATUS_FLAGS,
      flags & 0x7F,
      (flags >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def clear_reset_flag(self):
    """
    Clears the Motoron's reset flag.

    The reset flag is a latched status flag in get_status_flags() that is
    particularly important to clear: it gets set to 1 after the Motoron
    powers on or experiences a reset, and it is considered to be an error
    by default, so it prevents the motors from running.  Therefore, it is
    necessary to call this function (or clear_latched_status_flags()) to clear
    the Reset flag before you can get the motors running.

    We recommend that immediately after you clear the reset flag. you should
    configure the Motoron's motor settings and error response settings.
    That way, if the Motoron experiences an unexpected reset while your system
    is running, it will stop running its motors and it will not start them
    again until all the important settings have been configured.

    \sa clear_latched_status_flags()
    """
    self.clear_latched_status_flags(1 << STATUS_FLAG_RESET)

  def set_latched_status_flags(self, flags):
    """
    Sets the specified flags in get_status_flags().

    For each bit in the flags argument that is 1, this command sets the
    corresponding bit in the "Status flags" variable to 1.

    For more information, see the "Set latched status flags" command in the
    Motoron user's guide.

    \sa get_status_flags(), set_latched_status_flags()
    """
    cmd = [
      CMD_SET_LATCHED_STATUS_FLAGS,
      flags & 0x7F,
      (flags >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_speed(self, motor, speed):
    """
    Sets the target speed of the specified motor.

    The current speed will start moving to the specified target speed,
    obeying any acceleration and deceleration limits.

    The motor number should be between 1 and the number of motors supported
    by the Motoron.

    The speed should be between -800 and 800.  Values outside that range
    will be clipped to -800 or 800 by the Motoron firmware.

    For more information, see the "Set speed" command in the Motoron
    user's guide.

    \sa set_speed_now(), set_all_speeds()
    """
    cmd = [
      CMD_SET_SPEED,
      motor & 0x7F,
      speed & 0x7F,
      (speed >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_speed_now(self, motor, speed):
    """
    Sets the target and current speed of the specified motor, ignoring
    any acceleration and deceleration limits.

    For more information, see the "Set speed" command in the Motoron
    user's guide.

    \sa set_speed(), set_all_speeds_now()
    """
    cmd = [
      CMD_SET_SPEED_NOW,
      motor & 0x7F,
      speed & 0x7F,
      (speed >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_buffered_speed(self, motor, speed):
    """
    Sets the buffered speed of the specified motor.

    This command does not immediately cause any change to the motor: it
    stores a speed for the specified motor in the Motoron so it can be
    used by later commands.

    For more information, see the "Set speed" command in the Motoron
    user's guide.

    \sa set_speed(), set_all_buffered_speeds(),
      set_all_speeds_using_buffers(), set_all_speeds_now_using_buffers()
    """
    cmd = [
      CMD_SET_BUFFERED_SPEED,
      motor & 0x7F,
      speed & 0x7F,
      (speed >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_all_speeds(self, *speeds):
    """
    Sets the target speeds of all the motors at the same time.

    The number of speed arguments you provide to this function must be equal
    to the number of motor channels your Motoron has, or else this command
    might not work.

    This is equivalent to calling set_speed() once for each motor, but it is
    more efficient because all of the speeds are sent in the same command.

    There are a few different ways you can call this method (and the related
    methods that set speeds for all the motors):

    ```{.py}
    # with separate arguments
    mc.set_all_speeds(100, -200, 300)

    # with arguments unpacked from a list
    speeds = [-400, 500, -600]
    mc.set_all_speeds(*speeds)
    ```

    For more information, see the "Set all speeds" command in the Motoron
    user's guide.

    \sa set_speed(), set_all_speeds_now(), set_all_buffered_speeds()
    """
    cmd = [CMD_SET_ALL_SPEEDS]
    for speed in speeds:
      cmd += [
        speed & 0x7F,
        (speed >> 7) & 0x7F,
      ]
    self._send_command(cmd)

  def set_all_speeds_now(self, *speeds):
    """
    Sets the target and current speeds of all the motors at the same time.

    The number of speed arguments you provide to this function must be equal
    to the number of motor channels your Motoron has, or else this command
    might not work.

    This is equivalent to calling set_speed_now() once for each motor, but it is
    more efficient because all of the speeds are sent in the same command.

    For more information, see the "Set all speeds" command in the Motoron
    user's guide.

    \sa set_speed(), set_speed_now(), set_all_speeds()
    """
    cmd = [CMD_SET_ALL_SPEEDS_NOW]
    for speed in speeds:
      cmd += [
        speed & 0x7F,
        (speed >> 7) & 0x7F,
      ]
    self._send_command(cmd)

  def set_all_buffered_speeds(self, *speeds):
    """
    Sets the buffered speeds of all the motors.

    The number of speed arguments you provide to this function must be equal
    to the number of motor channels your Motoron has, or else this command
    might not work.

    This command does not immediately cause any change to the motors: it
    stores speed for each motor in the Motoron so they can be used by later
    commands.

    For more information, see the "Set all speeds" command in the Motoron
    user's guide.

    \sa set_speed(), set_buffered_speed(), set_all_speeds(),
      set_all_speeds_using_buffers(), set_all_speeds_now_using_buffers()
    """
    cmd = [CMD_SET_ALL_BUFFERED_SPEEDS]
    for speed in speeds:
      cmd += [
        speed & 0x7F,
        (speed >> 7) & 0x7F,
      ]
    self._send_command(cmd)

  def set_all_speeds_using_buffers(self):
    """
    Sets each motor's target speed equal to the buffered speed.

    This command is the same as set_all_speeds() except that the speeds are
    provided ahead of time using set_buffered_speed() or set_all_buffered_speeds().

    \sa set_all_speeds_now_using_buffers(), set_buffered_speed(),
      set_all_buffered_speeds()
    """
    cmd = [CMD_SET_ALL_SPEEDS_USING_BUFFERS]
    self._send_command(cmd)

  def set_all_speeds_now_using_buffers(self):
    """
    Sets each motor's target speed and current speed equal to the buffered
    speed.

    This command is the same as set_all_speeds_now() except that the speeds are
    provided ahead of time using set_buffered_speed() or set_all_buffered_speeds().

    \sa set_all_speeds_using_buffers(), set_buffered_speed(),
      set_all_buffered_speeds()
    """
    cmd = [CMD_SET_ALL_SPEEDS_NOW_USING_BUFFERS]
    self._send_command(cmd)

  def set_braking(self, motor, amount):
    """
    Commands the motor to brake, coast, or something in between.

    Sending this command causes the motor to decelerate to speed 0 obeying
    any relevant deceleration limits.  Once the current speed reaches 0, the
    motor will attempt to brake or coast as specified by this command, but
    due to hardware limitations it might not be able to.

    The motor number parameter should be between 1 and the number of motors
    supported by the Motoron.

    The amount parameter gets stored in the "Target brake amount" variable
    for the motor and should be between 0 (coasting) and 800 (braking).
    Values above 800 will be clipped to 800 by the Motoron firmware.

    See the "Set braking" command in the Motoron user's guide for more
    information.

    \sa set_braking_now(), get_target_brake_amount()
    """
    cmd = [
      CMD_SET_BRAKING,
      motor & 0x7F,
      amount & 0x7F,
      (amount >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_braking_now(self, motor, amount):
    """
    Commands the motor to brake, coast, or something in between.

    Sending this command causes the motor's current speed to change to 0.
    The motor will attempt to brake or coast as specified by this command,
    but due to hardware limitations it might not be able to.

    The motor number parameter should be between 1 and the number of motors
    supported by the Motoron.

    The amount parameter gets stored in the "Target brake amount" variable
    for the motor and should be between 0 (coasting) and 800 (braking).
    Values above 800 will be clipped to 800 by the Motoron firmware.

    See the "Set braking" command in the Motoron user's guide for more
    information.

    \sa set_braking(), get_target_brake_amount()
    """
    cmd = [
      CMD_SET_BRAKING_NOW,
      motor & 0x7F,
      amount & 0x7F,
      (amount >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def reset_command_timeout(self):
    """
    Resets the command timeout.

    This prevents the command timeout status flags from getting set for some
    time.  (The command timeout is also reset by every other Motoron command,
    as long as its parameters are valid.)

    For more information, see the "Reset command timeout" command in the
    Motoron user's guide.

    \sa disable_command_timeout(), set_command_timeout_milliseconds()
    """
    cmd = [CMD_RESET_COMMAND_TIMEOUT]
    self._send_command(cmd)

  def _send_command(self, cmd):
    send_crc = bool(self.protocol_options & (1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS))
    self._send_command_core(cmd, send_crc)

  def _send_command_and_read_response(self, cmd, response_length):
    self._send_command(cmd)
    return self._read_response(response_length)

def calculate_current_limit(milliamps, type, reference_mv, offset):
  """
  Calculates a current limit value that can be passed to the Motoron
  using set_current_limit().

  \param milliamps The desired current limit, in units of mA.
  \param type Specifies what type of Motoron you are using.  This should be one
    of the members of the motoron.CurrentSenseType enum.
  \param reference_mv The reference voltage (IOREF), in millivolts.
    For example, use 3300 for a 3.3 V system or 5000 for a 5 V system.
  \param offset The offset of the raw current sense signal for the Motoron
    channel.  This is the same measurement that you would put into the
    Motoron's "Current sense offset" variable using set_current_sense_offset(),
    so see the documentation of that function for more info.
    The offset is typically 10 for 5 V systems and 15 for 3.3 V systems,
    (50*1024/reference_mv) but it can vary widely.
  """
  if milliamps > 1000000: milliamps = 1000000
  limit = offset * 125 / 128 + milliamps * 20 / (reference_mv * (enum_value(type) & 3))
  if limit > 1000: limit = 1000
  return math.floor(limit)

def current_sense_units_milliamps(type, reference_mv):
  """
  Calculates the units for the Motoron's current sense reading returned by
  get_current_sense_processed(), in milliamps.

  To convert a reading from get_current_sense_processed() to milliamps
  multiply it by the value returned from this function.

  \param type Specifies what type of Motoron you are using.  This should be one
    of the members of the motoron.CurrentSenseType enum.
  \param reference_mv The reference voltage (IOREF), in millivolts.
    For example, use 3300 for a 3.3 V system or 5000 for a 5 V system.
  """
  return reference_mv * (enum_value(type) & 3) * 25 / 512

class MotoronI2C(MotoronBase):
  """
  Represents an I2C connection to a Pololu Motoron Motor Controller.
  """

  def __init__(self, *, bus=1, address=16):
    """
    Creates a new MotoronI2C object to communicate with the Motoron over I2C.

    \param bus Optional argument that specifies which I2C bus to use.
      This can be an integer, an SMBus object from the smbus2 package, or an
      object with an interface similar to SMBus.
      The default bus is 1, which corresponds to `/dev/i2c-1`.
    \param address Optional argument that specifies the 7-bit I2C address to
      use.  This must match the address that the Motoron is configured to use.
      The default address is 16.
    """
    super().__init__()

    self.set_bus(bus)

    ## The 7-bit I2C address used by this object. The default is 16.
    self.address = address

    """
    Configures this object to use the specified I2C bus object.

    The bus argument should be one of the following:
    - The number of an I2C bus to open with smbus2
      (e.g. 2 for `/dev/i2c-2`)
    - An SMBus object from smbus2.
    - A machine.I2C object from MicroPython.
    """
  def set_bus(self, bus):
    if isinstance(bus, int):
      import smbus2
      bus = smbus2.SMBus(bus)

    try:
      bus.i2c_rdwr
      type_is_smbus = True
    except AttributeError:
      type_is_smbus = False

    if type_is_smbus:
      self._send_command_core = self._smbus_send_command_core
      self._read_response = self._smbus_read_response
      import smbus2
      self._msg = smbus2.i2c_msg
    else:
      self._send_command_core = self._mpy_send_command_core
      self._read_response = self._mpy_read_response

    self.bus = bus

  def _smbus_send_command_core(self, cmd, send_crc):
    if send_crc:
      write = self._msg.write(self.address, cmd + [calculate_crc(cmd)])
    else:
      write = self._msg.write(self.address, cmd)
    self.bus.i2c_rdwr(write)

  def _smbus_read_response(self, length):
    # On some Raspberry Pis with buggy implementations of I2C clock stretching,
    # sleeping for 0.5 ms might be necessary in order to give the Motoron time
    # to prepare its response, so it does not need to stretch the clock.
    time.sleep(0.0005)

    crc_enabled = bool(self.protocol_options & (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))
    read = self._msg.read(self.address, length + crc_enabled)
    self.bus.i2c_rdwr(read)
    response = bytes(read)
    if crc_enabled:
      crc = response[-1]
      response = response[:-1]
      if crc != calculate_crc(response):
        raise RuntimeError('Incorrect CRC received.')
    return response

  def _mpy_send_command_core(self, cmd, send_crc):
    if send_crc:
      self.bus.writeto(self.address, bytes(cmd + [calculate_crc(cmd)]))
    else:
      self.bus.writeto(self.address, bytes(cmd))

  def _mpy_read_response(self, length):
    crc_enabled = bool(self.protocol_options & (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))
    response = self.bus.readfrom(self.address, length + crc_enabled)
    if crc_enabled:
      crc = response[-1]
      response = response[:-1]
      if crc != calculate_crc(response):
        raise RuntimeError('Incorrect CRC received.')
    return response


class MotoronSerial(MotoronBase):
  """
  Represents a serial connection to a Pololu Motoron Motor Controller.
  """

  def __init__(self, *, port=None, device_number=None):
    """
    Creates a new MotoronSerial object.

    The `deviceNumber` argument is optional.  If it is omitted or None,
    the object will use the compact protocol.

    The `port` argument specifies the serial port to use and is passed
    directly to set_port().
    """
    super().__init__()

    self.set_port(port)

    ## The device number that will be included in commands sent by this object.
    ## The default is None, which means to not send a device number and use the
    ## compact protocol instead.
    self.device_number = device_number

    ## The serial options used by this object.  This must match the serial
    ## options in the EEPROM of the Motoron you are communicating with.
    ## The default is 7-bit device numbers and 8-bit responses.
    ##
    ## The bits in this variable are defined by the
    ## motoron.COMMUNICATION_OPTION_* constants.  The bits that affect the
    ## behavior of this library are:
    ## - motoron.COMMUNICATION_OPTION_7BIT_RESPONSES
    ## - motoron.COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER
    self.communication_options = 0

  def set_port(self, port):
    """
    Configures this object to use the specified serial port object.

    The port argument should be one of the following:
    - The name of a serial port to open with pyserial
      (e.g. "COM6" or "/dev/ttyS0")
    - A Serial object from pyserial.
    - A machine.UART object from MicroPython.
    """
    if isinstance(port, str):
      import serial
      self.port = serial.Serial(port, 115200, timeout=0.1, write_timeout=0.1)
    else:
      ## The serial port used by this object.  See set_port().
      self.port = port

  def expect_7bit_responses(self):
    """
    Configures this object to work with Motorons that are configured to send
    7-bit serial responses.
    """
    self.communication_options |= (1 << COMMUNICATION_OPTION_7BIT_RESPONSES)

  def expect_8bit_responses(self):
    """
    Configures this object to work with Motorons that are configured to send
    responses in the normal 8-bit format.
    """
    self.communication_options &= ~(1 << COMMUNICATION_OPTION_7BIT_RESPONSES)

  def use_14bit_device_number(self):
    """
    Configures this object to send 14-bit device numbers when using the
    Pololu protocol, instead of the default 7-bit.
    """
    self.communication_options |= (1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER)

  def use_7bit_device_number(self):
    """
    Configures this object to send 7-bit device numbers, which is the default.
    """
    self.communication_options &= ~(1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER)

  def multi_device_error_check_start(self, starting_device_number, device_count):
    """
    Sends a "Multi-device error check" command but does not read any
    responses.

    Note: Before using this, most users should make sure the MotoronSerial
    object is configured to use the compact protocol: construct the object
    without specifying a device number, or set device_number to None.
    """
    if self.communication_options & (1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER):
      if device_count < 0 or device_count > 0x3FFF:
        raise RuntimeError('Invalid device count.')
      cmd = [
        CMD_MULTI_DEVICE_ERROR_CHECK,
        starting_device_number & 0x7F,
        starting_device_number >> 7 & 0x7F,
        device_count & 0x7F,
        device_count >> 7 & 0x7F,
      ]
    else:
      if device_count < 0 or device_count > 0x7F:
        raise RuntimeError('Invalid device count.')
      cmd = [
        CMD_MULTI_DEVICE_ERROR_CHECK,
        starting_device_number & 0x7F,
        device_count,
      ]

    self._send_command(cmd)
    self.port.flush()

  def multi_device_error_check(self, starting_device_number, device_count):
    """
    Sends a "Multi-device error check" command and reads the responses.

    This function assumes that each addressed Motoron can see the responses
    sent by the other Motorons (e.g. they are in a half-duplex RS-485 network).

    Returns the number of devices that indicated they have no errors.
    If the return value is less than device count, you can add the return
    value to the starting_device_number to get the device number of the
    first device where the check failed.  This device either did not
    respond or it responded with an indication that it has an error, or an
    unexpected byte was received for some reason.

    Note: Before using this, most users should make sure the MotoronSerial
    object is configured to use the compact protocol: construct the object
    without specifying a device number, or set device_number to None.
    """
    self.multi_device_error_check_start(starting_device_number, device_count)
    responses = self.port.read(device_count)
    for i, v in enumerate(responses):
      if v != ERROR_CHECK_CONTINUE: return i
    return len(responses)

  def multi_device_write(self, starting_device_number, device_count,
    command_byte, data):
    """
    Sends a "Multi-device write" command.

    Note: Before using this, most users should make sure the MotoronSerial
    object is configured to use the compact protocol: construct the object
    without specifying a device number, or call setDeviceNumber with an
    argument of 0xFFFF.
    """

    if bool(self.communication_options & (1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER)):
      if device_count < 0 or device_count > 0x3FFF:
        raise RuntimeError('Invalid device count.')
      cmd = [
        CMD_MULTI_DEVICE_WRITE,
        starting_device_number & 0x7F,
        starting_device_number >> 7 & 0x7F,
        device_count & 0x7F,
        device_count >> 7 & 0x7F,
      ]
    else:
      if device_count < 0 or device_count > 0x7F:
        raise RuntimeError('Invalid device count.')
      cmd = [
        CMD_MULTI_DEVICE_WRITE,
        starting_device_number & 0x7F,
        device_count,
      ]

    if data == None: data = []
    if len(data) % device_count:
      raise RuntimeError("Expected data length to be a multiple of " \
        f"{device_count}, got {len(data)}.")
    bytes_per_device = len(data) // device_count
    if bytes_per_device > 15: raise RuntimeError('Data too long.')

    cmd += [bytes_per_device, command_byte & 0x7F]
    cmd += data

    self._send_command(cmd)

  def _send_command_core(self, cmd, send_crc):
    if self.device_number != None:
      if self.communication_options & (1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER):
        cmd = [
          0xAA,
          self.device_number & 0x7F,
          self.device_number >> 7 & 0x7F,
          cmd[0] & 0x7F
        ] + cmd[1:]
      else:
        cmd = [
          0xAA,
          self.device_number & 0x7F,
          cmd[0] & 0x7F
        ] + cmd[1:]

    if send_crc: cmd += [calculate_crc(cmd)]

    self.port.write(bytes(cmd))

  def _read_response(self, length):
    crc_enabled = bool(self.protocol_options & (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))
    response_7bit = bool(self.communication_options & (1 << COMMUNICATION_OPTION_7BIT_RESPONSES))

    if response_7bit and length > 7:
      raise RuntimeError('The Motoron does not support response payloads ' \
        'longer than 7 bytes in 7-bit response mode.')

    self.port.flush()
    read_length = length + response_7bit + crc_enabled
    response = self.port.read(read_length)
    if response is None: response = b''
    if len(response) != read_length:
      raise RuntimeError(f"Expected to read {read_length} bytes, got {len(response)}.")

    if crc_enabled:
      crc = response[-1]
      response = response[:-1]
      if crc != calculate_crc(response):
        raise RuntimeError('Incorrect CRC received.')

    if response_7bit:
      msbs = response[-1]
      response = bytearray(response[:-1])
      for i in range(length):
        if msbs & 1: response[i] |= 0x80
        msbs >>= 1
      response = bytes(response)

    return response


# === ./src/old/animation_manager-old2.py ===
import time
import os
import pygame
from .sound_manager import SoundManger
from natsort import natsorted
global resx, resy, yoffset, imp, initPygameOnce
resx=1280
resy=800
yoffset=70
initPygameOnce=0

#imp = pygame.image.load('/home/pi/MiniMax/powerdown/001.jpg').convert()
def load_images(path):
    store =[]
    for file_name in os.listdir(path):
        temp=file_name
        store.append(temp)
    store=natsorted(store)  
    images = []
    image_array=[]
    for xyz in store:
        pic=path+xyz
        image_array.append(pic)
    for names in image_array:
        imagine = pygame.image.load(names).convert()
        images.append(imagine)
    return images
    
class AnimationManager:
    def __init__(self, config) -> None:
        pygame.init()
        self.display = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
        self.background = pygame.Surface(self.display.get_size())
        #self.clock = pygame.time.Clock()
        self.animate_delay = config.animate_delay
        #self.sound_manager = SoundManger
        self.channel = pygame.mixer.Channel(1)

    def animate(self, images):
        closedmouth = pygame.image.load('/home/pi/MiniMax/Animations/advised/outputFile_001.jpg').convert()
        loops = 0
        run = True
        length=len(images)
        print('starting the loop',length)
        while loops < length:      
            #frt=show_fps()
            #print(loops)
            intloops = int(loops)
            image = images[intloops]
            self.display.blit(image, (0, 0))
            pygame.display.flip()
            loops = loops +1.25
        #time.sleep(0.05)
        self.display.blit(closedmouth, (0, 0)) # end animation with closed mouth .jpg 
        pygame.display.flip()
       

# === ./src/robot/Robot.py ===
from src.action_managers.HeadMoveManager import HeadVelocityManager
from src.multithreading.PathFindingThread import PathFindingThread
from src.path_finding.LidarPlot import LidarPlot
from ..sensors.CameraSensor import CameraSensor

from ..sensors.DistanceSensor import DistanceSensor
from ..sensors.LidarSensor import LidarSensor
from ..types.CameraMode import CameraMode
from ..types.RobotModes import RobotMode
from ..sensors.KeyboardSensor import KeyboardSensor
from ..action_managers.FacialAnimationManager import FacialAnimationManager
from ..action_managers.SoundManager import SoundManager
from ..action_managers.SpeechManager import SpeechManager
from ..action_managers.DirectVelocityManager import DirectVelocityManager
from queue import LifoQueue


class Robot:
    """
    The main robot class.
    This class just holds all the Action managers and sensors the robot has.
    It also stores the current mode.

    Behaviors will have access to the robot class to do stuff with it
    """

    def __init__(
        self,
        keyboard_sensor: KeyboardSensor,
        facial_animation_manager: FacialAnimationManager,
        sound_manager: SoundManager,
        speech_manager: SpeechManager,
        velocity_manager: DirectVelocityManager,
        head_move_manager: HeadVelocityManager,
        distance_sensor: DistanceSensor,
        camera_sensor: CameraSensor,
        lidar_sensor: LidarSensor,
        errors_ocurred: list[str],
    ) -> None:
        """
        Initialises the robot with the relevant sensors and action managers
        """
        self.errors_ocurred = errors_ocurred
        self.keyboard_sensor = keyboard_sensor
        self.facial_animation_manager = facial_animation_manager
        self.sound_manager = sound_manager
        self.speech_manager = speech_manager
        self.velocity_manager = velocity_manager
        self.head_move_manager = head_move_manager

        self.distance_sensor = distance_sensor
        self.camera_sensor = camera_sensor

        self.lidar_sensor = lidar_sensor

        # robot starts in IDLE mode
        self.current_mode = RobotMode.IDLE

    def set_mode(self, mode: RobotMode):
        """
        Sets the mode of the robot
        """
        self.current_mode = mode

    def shutdown(self):
        # switch speed to 0 and turn brake back on
        self.velocity_manager.shutdown()

# === ./src/robot/__init__.py ===


# === ./src/robot/RobotFactory.py ===
import pygame
import yaml

from src.action_managers.HeadMoveManager import HeadVelocityManager

from ..sensors.CameraSensor import CameraSensor
from ..sensors.I2CSensor import I2CSensor

from ..sensors.DistanceSensor import DistanceSensor
from ..sensors.LidarSensor import LidarSensor
from ..sensors.KeyboardSensor import KeyboardSensor
from ..action_managers.FacialAnimationManager import FacialAnimationManager
from ..action_managers.SoundManager import SoundManager
from ..action_managers.SpeechManager import SpeechManager
from ..action_managers.DirectVelocityManager import DirectVelocityManager
from ..action_managers.DebugActionManager import DebugActionManager
from ..types.DirectionSensorLocation import DirectionSensorLocation

from .Robot import Robot


class RobotFactory:
    """
    Builds a Robot object from a config file
    """

    def __init__(self, config_filename: str) -> None:
        """
        Initiliases the factory

        arguments:
            - config_filename: the filename of the configurations file
        """
        # load the configs
        self.config = self.load_config_file(config_filename)
        self.errors_occurred=[]

    def load_config_file(self, file_name: str):
        """
        Loads the configuration file from disk
        """
        with open(file_name, "r") as file:
            return yaml.safe_load(file)

    def build_robot(self) -> Robot:
        """
        Builds the robot based on the config file
        """

        display = self.init_pygame(self.config["pygame"])

        # build action managers
        animimation_manager = self.build_facial_animation_manager(
            self.config["animation_manager"]
        )
        sound_manager = self.build_sound_manger(self.config["sound_manager"])
        speech_manager = self.build_speech_manager(self.config["speech_manager"])
        
        try:
            velocity_manager = self.build_velocity_manager(self.config["velocity_manager"])
        except:
            self.errors_occurred.append('Velocity Manager failed to initialise')
            velocity_manager=None


        head_movement_manager = HeadVelocityManager(11)
        # build sensors
        keyboard_sensor = self.build_keyboard_sensor()
        distance_sensor = self.build_distance_sensor(self.config["distance_sensors"])
        camera_sensor = self.build_camera_sensor()
        try:
            lidar_sensor = self.build_lidar_sensor()
        except:
            self.errors_occurred.append('Lidar failed to initialise')
            lidar_sensor=None
        # build robot
        return Robot(
            keyboard_sensor,
            animimation_manager,
            sound_manager,
            speech_manager,
            velocity_manager,
            head_movement_manager,
            distance_sensor,
            camera_sensor,
            lidar_sensor,
            self.errors_occurred
        )

    def build_lidar_sensor(self) -> LidarSensor:
        return LidarSensor()

    def build_keyboard_sensor(self) -> KeyboardSensor:
        """
        Builds the keyboard sensor
        """
        return KeyboardSensor()

    def init_pygame(self, config) -> pygame.Surface:
        """
        Initilises pygame according to configs
        """
        pygame.init()



        # initialise audio
        frequency = config["audio_frequency"]
        size = config["audio_size"]
        channels = config["audio_channels"]
        buffer = config["audio_buffer"]
        pygame.mixer.pre_init(frequency, size, channels, buffer)
        pygame.mixer.init()


    def build_facial_animation_manager(self, config) -> FacialAnimationManager:
        # if in manager is in debug mode return a debug manager
        if config["debug"]:
            return DebugActionManager("Facial Animation Manager")

        return FacialAnimationManager()

    def build_sound_manger(self, config) -> SoundManager:
        # if in manager is in debug mode return a debug manager
        if config["debug"]:
            return DebugActionManager("Sound Manager")

        return SoundManager(config["channel"])

    def build_speech_manager(self, config) -> SpeechManager:
        # if in manager is in debug mode return a debug manager
        if config["debug"]:
            return DebugActionManager("Speech Manager")

        rate = config["rate"]
        voice = config["voice"]
        volume = config["volume"]
        return SpeechManager(rate, voice, volume)

    def build_velocity_manager(self, config) -> DirectVelocityManager:
        # if in manager is in debug mode return a debug manager
        if config["debug"]:
            return DebugActionManager("Velocity Manager")

        return DirectVelocityManager(
            config["version"],
            config["maxaccel"],
            config["maxdeccel"],
            config["current_limit"],
            simulate=config.get("simulate", False)

        )

    def build_distance_sensor(self, config) -> DistanceSensor:
        """
        Builds the distance sensor
        """

        # builds the sensors from the config
        def build_sensors():
            sensors = {}
            for direction, i2c_config in config.items():
                sensor = I2CSensor(
                    int(i2c_config["ic2_adress"]), int(i2c_config["port"])
                )
                sensors[DirectionSensorLocation(direction)] = sensor
            return sensors

        sensors = [] #build_sensors()
        return DistanceSensor(sensors)

    def build_camera_sensor(self) -> CameraSensor:
        """
        Builds the camera sensor
        """
        return CameraSensor()


# === ./src/action_managers/DirectVelocityManager.py ===
import time
from ..types.MovementDirection import MovementDirection
from ..action_managers.VelocityManager import VelocityConfig
from ..action_managers.ActionManager import ActionManager
import Jetson.GPIO as GPIO
import motoron


class DirectVelocityManager(ActionManager):
    REFERENCE_MV = 3300
    PIN= 7
    TYPE = motoron.CurrentSenseType.MOTORON_24V16
    VIN_TYPE = motoron.VinSenseType.MOTORON_HP
    
    MAX_ACCELERATION = 50
    MAX_DECELERATION = 40
    CURRENT_LIMIT= 13000

# I have adjusted speed of all motors down, to slow down robot.
# This is to ensure that turning robot doesn't overshoot target.
    DIRECTION_TO_MOTOR_SPEED = {
        MovementDirection.NONE: (0, 0),
        MovementDirection.FORWARDS: (-200, -196),
        MovementDirection.RIGHT: (160, -160),
        MovementDirection.LEFT: (-160, 160),
        MovementDirection.FORWARDS_LEFT: (-270,-200),
        MovementDirection.BACKWARDS_LEFT: (300, 200),
        MovementDirection.BACKWARDS: (200, 200),
        MovementDirection.BACKWARDS_RIGHT: (-170, -205),
        MovementDirection.FORWARDS_RIGHT: (-170, -205),
    }

    

    def __init__(self, version, maxaccel, maxdecel, current_limit, simulate=False) -> None:
        self.name = "Direct Motor Control Velocity Manager"
        self.TYPE=motoron.CurrentSenseType.MOTORON_24V16
        self.CURRENT_LIMIT=current_limit
        self.MAX_ACCELERATION=maxaccel
        self.MAX_DECELERATION=maxdecel
        self.simulate=simulate
        GPIO.setmode(GPIO.BOARD)  
        GPIO.setup(self.PIN, GPIO.OUT)
        if not self.simulate:
            self.initialise_motor_control()
            self.turn_brake_off()
        
    

    def shutdown(self):

        if self.simulate:
            print('simulated shutdown and brake on')
            self.turn_brake_on() 
            return

        self.mc.set_speed(1, 0)
        self.mc.set_speed(2, 0)
        self.turn_brake_on()
        print('shutdown and brake on')

    def turn_brake_on(self):
        # brake line goes off to turn brake on (due to safety feature)
        GPIO.output(self.PIN, GPIO.LOW)  # Set the pin to LOW (brake on)
        print('brake on')
        
	
    def turn_brake_off(self):
        # brake line goes on to turn brake off (due to safety feature)
        GPIO.output(self.PIN, GPIO.HIGH)  # Set the pin to LOW (brake on)
        print('brake off')

    def initialise_motor_control(self):

        if not self.simulate:
            self.turn_brake_off() 
            #initialise the motor controller
            self.mc = motoron.MotoronI2C(bus=7)
            self.mc.reinitialize() 
            self.mc.disable_crc()
            self.mc.clear_reset_flag()
            self.mc.disable_command_timeout()

            # configure both motors 
            self.configure_motor(1)
            self.configure_motor(2) 
            print('init motoron')
        else:
            print('Simulate init motoron controller')

    def configure_motor(self, motor_id):
        
        print('config motoron')
        # clear motor status
        self.mc.clear_motor_fault()
        
        # set acceleration / deceleration maximums
        self.mc.set_max_acceleration(motor_id, self.MAX_ACCELERATION)
        self.mc.set_max_deceleration(motor_id, self.MAX_DECELERATION)

        # set currnet limit on motor by calculating each motor's offset first. Motors must be in zero state.
        # set motors to zero
        self.mc.set_braking(motor_id,0)
        self.mc.set_speed(motor_id, 0)
        current_offset = self.mc.get_current_sense_offset(motor_id)
        #print('The current offset for motor ',motor_id,' is ', current_offset)
        limit = motoron.calculate_current_limit(self.CURRENT_LIMIT, self.TYPE, self.REFERENCE_MV, current_offset)
        self.mc.set_current_limit(motor_id, limit)
        #print("The current limit " , int(limit))
        

    def perform_action(self, config: VelocityConfig):

        #print('perform action')
        direction = config.direction
        speed = config.speed
        motor_1, motor_2 = self.DIRECTION_TO_MOTOR_SPEED[direction]

        motor_1 = int(motor_1 * speed)
        motor_2 = int(motor_2 * speed)

        if motor_1 < 0:
            motor_1 = max(-800, motor_1)
        else:
            motor_1 = min(800, motor_1)

        if motor_2 < 0:
            motor_2 = max(-800, motor_2)
        else:
            motor_2 = min(800, motor_2)

        if self.simulate:
            if direction == MovementDirection.NONE:
                print("[Sim] Stopped")
            elif direction == MovementDirection.FORWARDS:
                print("[Sim] Moving forward")
            elif direction == MovementDirection.BACKWARDS:
                print("[Sim] Moving backward")
            elif direction == MovementDirection.LEFT:
                print("[Sim] Turning left")
            elif direction == MovementDirection.RIGHT:
                print("[Sim] Turning right")
            elif direction == MovementDirection.FORWARDS_LEFT:
                print("[Sim] Forward-left")
            elif direction == MovementDirection.FORWARDS_RIGHT:
                print("[Sim] Forward-right")
            elif direction == MovementDirection.BACKWARDS_LEFT:
                print("[Sim] Backward-left")
            elif direction == MovementDirection.BACKWARDS_RIGHT:
                print("[Sim] Backward-right")
            else:
                print(f"[Sim] Unknown direction: {direction}")
            return
        #print('perform action')
        # set left speed & braking when speed is 0
        #self.mc.set_braking(1,0)
        self.mc.set_speed(1, motor_1)
        #time.sleep(.01)
        #VOLTAGE_MV = self.mc.get_vin_voltage_mv(self.REFERENCE_MV, self.VIN_TYPE)
        #self.mc.set_current_sense_minimum_divisor(1,800)
        #AMPS_MV = self.mc.get_current_sense_processed(1)
        #print("current voltage motor 1 is ", int(VOLTAGE_MV))
        #print("current AMPS motor 1 is ", int(AMPS_MV*161))

        # set right speed & braking when speed is 0
        #self.mc.set_braking(2,0)
        self.mc.set_speed(2, motor_2)
        #time.sleep(.01)
        #VOLTAGE_MV = self.mc.get_vin_voltage_mv(self.REFERENCE_MV, self.VIN_TYPE)
        #self.mc.set_current_sense_minimum_divisor(2,800)
        #AMPS_MV = self.mc.get_current_sense_processed(2)
        #print("current voltage motor 2 is ", int(VOLTAGE_MV))
        #print("current AMPS motor 2 is ", int(AMPS_MV*161))
        


# === ./src/action_managers/ActionManager.py ===
from abc import ABC, abstractmethod
from typing import Generic, TypeVar

# the type of readings from this sensor
ActionConfig = TypeVar("ActionConfig")


class  ActionManager(ABC, Generic[ActionConfig]):
    """
    Base class for an Action Manager.
    An ActionManager handles a capability that the robot has.
    This is an abstract class that cannot be instantiated
    """

    def __init__(self, name) -> None:
        """ "
        Initialises the Action Manager

        arguments:
            - name: the name of the manager
        """
        self.name = name

    @abstractmethod
    def perform_action(self, config: ActionConfig):
        """
        Perform an action from this manager
        Children of this class must implement this method

        arguments:
            - config: The configuration for this action specific to this class
        """
        pass


# === ./src/action_managers/FacialAnimationManager.py ===
import os
import time

import pygame

from ..types.FacialAnimation import FacialAnimation
from .ActionManager import ActionManager


class FacialAnimationManager(ActionManager[FacialAnimation]):
    """
    Action Manager for Facial Animations
    """

    # constant variables
    ANIMATION_FOLDER = "Animations"
    DEFAULT_ANIMATION = FacialAnimation.advised

    def __init__(self) -> None:
        """
        Initialises the manager

        arguments:
            - display: the pygram display object
        """
        super().__init__("Facial Animation Manager")
        self.resting_face_img = self.load_default_face()
        self.resting_face_img =pygame.transform.scale(self.resting_face_img,(1920, 1080))    
                
           
        
        self.open_window()
        # show resting face
        self.display.blit(self.resting_face_img, (0, 0))
        pygame.display.flip()

    def close_window(self):
        pygame.display.quit()


    def open_window(self):
        display_flags = pygame.FULLSCREEN
        display = pygame.display.set_mode((0, 0), display_flags)
        self.display  = display

    def perform_action(self, config: FacialAnimation):
        """
        Implementation of the managers perform action function
        Will load the relevant images for the animation
        And animate the face
        """
        # load images
        animation_images = self.load_animation_images(config)

        # run animation
        for image in animation_images:
            image =pygame.transform.scale(image, (1920, 1080)) 
            self.display.blit(image, (0, 0))
            pygame.display.flip()
            # time.sleep(0.01)

        # end animation with resting face
        self.display.blit(self.resting_face_img, (0, 0))
        pygame.display.flip()

    def load_default_face(self):
        """
        Loads the default face image to be stored
        """
        animation_dir = self.get_animation_dir(self.DEFAULT_ANIMATION)
        image_filename = list(sorted(os.listdir(animation_dir)))[0]
        return self.load_image(animation_dir, image_filename)

    def get_animation_dir(self, animation: FacialAnimation) -> str:
        """
        Returns the directory of an animation type
        """
        return os.path.join(self.ANIMATION_FOLDER, animation.value)

    def load_image(self, animation_dir: str, image_filename: str):
        """
        Loads an animation image
        """
        return pygame.image.load(os.path.join(animation_dir, image_filename))

    def load_animation_images(self, animation: FacialAnimation):
        """
        Loads all the images for a particular animation
        """
        animation_dir = self.get_animation_dir(animation)
        animation_images = os.listdir(animation_dir)
        animation_images = sorted(animation_images)

        return [
            self.load_image(animation_dir, image_name)
            for image_name in animation_images[::1]
        ]


# === ./src/action_managers/SoundManager.py ===
import pygame

from ..types.Sound import Sound
from .ActionManager import ActionManager


class SoundManager(ActionManager[Sound]):
    """
    The Action Manager for playing sounds from file
    """

    def __init__(self, channel: int) -> None:
        """
        Initialises the manager.

        arguments:
            - channel: the audio channel to play on
        """
        self.channel = pygame.mixer.Channel(channel)

    def perform_action(self, config: Sound):
        """
        The manager's implementation of perform action.
        Will load a sound from file and play it.
        """
        sound = config.load_sound()
        self.channel.play(sound)


# === ./src/action_managers/HeadMoveManager.py ===
import time
import UltraBorg
from .ActionManager import ActionManager
from ..types.HeadMovementDirection import HeadMovementDirection
from py_trees.common import Status
import py_trees

class HeadVelocityManager(ActionManager[HeadMovementDirection]):
    """
    The Action Manager responsible for moving the robot.
    """

    # stores the x, y coordinates to move the servo for each direction of travel
    DIRECTION_TO_SERVO_VALUES = {
        HeadMovementDirection.NONE: (0),
        HeadMovementDirection.LEFT: (0.02),
        HeadMovementDirection.RIGHT: (-0.02),
    }

    def __init__(self, i2c_channel) -> None:
        """
        Initiliases the head velocity manager.

        args:
            - ic2_channel: The I2C Channel of the Head actuators
        """
        super().__init__("Velocity Manager")
        print("")
        self.board =  UltraBorg.UltraBorg()
        self.board.i2cAddress = int(i2c_channel)
        self.board.Init()
                

        # setting max left, right and start postions
        self.board.SetWithRetry(self.board.SetServoMaximum3, self.board.GetServoMaximum3, 5085, 5)
        self.board.SetWithRetry(self.board.SetServoMinimum3, self.board.GetServoMinimum3, 1550, 5)
        self.board.SetWithRetry(self.board.SetServoStartup3, self.board.GetServoStartup3, 3565, 5)

        self.center_head()
        self.current_head_position = 0
        
    def center_head(self):
        position = self.board.GetServoPosition3()
        
        if position is None:
            print("[WARNING] Head position is None, setting to 0.")
            position = 0  # fallback default value
        distance_from_zero = abs(0 - position)
        # continue centering logic
        increment=0.02
        print(position)
        while abs(position) >= 0.02:
            if distance_from_zero<0.2:
                increment=0.01
            else:
                increment=0.02

            if position > 0:
                step = -1*increment
            else:
                step = 1*increment

            self.board.SetServoPosition3(position + step)
            position = self.board.GetServoPosition3()
            time.sleep(0.1)



    def perform_action(self, direction: HeadMovementDirection):
        """
        Moves the robot in a direction by setting servo values
        """
        # get servo positions for direction
        step_amount= self.DIRECTION_TO_SERVO_VALUES[direction]
        new_position = self.current_head_position + step_amount
        
        if new_position > 0.98:
            return

        if new_position < -0.98:
            return
        
        # set servo positions
        self.current_head_position = new_position
        self.board.SetServoPosition3(new_position)

    def get_head_position(self):
        position = self.board.GetServoPosition3()
        return position

# === ./src/action_managers/VelocityManager.py ===
import UltraBorg
from .ActionManager import ActionManager
from ..types.MovementDirection import MovementDirection


class VelocityConfig:
    """
    The configuration for moving the robot.
    Behaviors will pass this to the Velocity Manager to move
    """

    def __init__(self, direction: MovementDirection, speed: int=1) -> None:
        self.direction = direction
        self.speed = speed


    def __str__(self):
        return f"Moving {self.direction.name} ({self.speed})"


class VelocityManager(ActionManager[VelocityConfig]):
    """
    The Action Manager responsible for moving the robot.
    """

    # stores the x, y coordinates to move the servo for each direction of travel
    DIRECTION_TO_SERVO_VALUES = {
        MovementDirection.NONE: (0, 0),
        MovementDirection.FORWARDS: (0.98, 0.24),
        MovementDirection.LEFT: (-0.26, -1.0),
        MovementDirection.RIGHT: (0.1, 0.8),
        MovementDirection.FORWARDS_LEFT: (-0.9, 0.1),
        MovementDirection.FORWARDS_RIGHT: (-0.9, -0.3),
        MovementDirection.BACKWARDS: (-0.95, -0.12),
        MovementDirection.BACKWARDS_LEFT: (0.98, -0.02),
        MovementDirection.BACKWARDS_RIGHT: (0.98, 0.4),
    }

    def __init__(self, i2c_channel) -> None:
        """
        Initiliases the velocity manager.

        args:
            - ic2_channel: The I2C Channel of the joystick actuators
        """
        super().__init__("Velocity Manager")
        self.board = UltraBorg_old.UltraBorg()
        self.board.i2cAddress = i2c_channel
        self.board.Init()

    def perform_action(self, config: VelocityConfig):
        """
        Moves the robot in a direction by setting servo values
        """
        # get servo positions for direction
        direction = config.direction
        x, y = self.DIRECTION_TO_SERVO_VALUES[direction]

        # set servo positions
        self.board.SetServoPosition1(x)
        self.board.SetServoPosition2(y)


# === ./src/action_managers/__init__.py ===


# === ./src/action_managers/DebugActionManager.py ===
from .ActionManager import ActionManager
from typing import TypeVar

ActionConfig = TypeVar("ActionConfig")


class DebugActionManager(ActionManager[ActionConfig]):
    """
    Implementation for a debug Action Manager
    This action manager just prints the action being taken instead of taking an action
    Useful to test with a raspberry pi missing actuators
    Will be used when the action_manager's debug: True is in config.
    """

    def perform_action(self, config: ActionConfig):
        print(f"{self.name} peforming action: {str(config)}")


# === ./src/action_managers/SpeechManager.py ===
from ..multithreading.TextToSpeechThread import TextToSpeechThread
from .ActionManager import ActionManager
import queue

class SpeechManager(ActionManager[str]):
    """
    The Manager for Text To Speech.
    This will run on a separate thread to prevent blocking.
    """

    def __init__(self, rate, voice, volume) -> None:
        """
        Initilises the manager

        arguments:
            - rate: the bit rate
            - voice: the voice to use
            - volumne: the volume to play at
        """
        super().__init__("Speech Manager")

        # build and start TTS thread
        self.speech_queue = queue.Queue()
        self.thread = TextToSpeechThread(self.speech_queue, rate, voice, volume)
        self.thread.start()

    def perform_action(self, sentence: str):
        """
        Converts text to speech and plays it.
        """
        self.speech_queue.put(sentence)


# === ./Jetson_simple_GPIO_output_pin7.py ===
#!/usr/bin/env python

# Copyright (c) 2019-2022, NVIDIA CORPORATION. All rights reserved.
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

import Jetson.GPIO as GPIO
import time

# Pin Definitions
output_pin = 7  # Jetson Board Pin 7


def main():
    # Pin Setup:
    GPIO.setmode(GPIO.BOARD)  # Jetson board numbering scheme
    # set pin as an output pin with optional initial state of HIGH
    GPIO.setup(output_pin, GPIO.OUT, initial=GPIO.HIGH)

    print("Starting demo now! Press CTRL+C to exit")
    curr_value = GPIO.HIGH
    try:
        while True:
            time.sleep(10)
            # Toggle the output every second
            print("Outputting {} to pin {}".format(curr_value, output_pin))
            GPIO.output(output_pin, curr_value)
            curr_value = GPIO.HIGH
    finally:
        GPIO.output(output_pin, 0)
        GPIO.cleanup()

if __name__ == '__main__':
    main()


# === ./stop_motors.py ===

import motoron
from gpiozero import LED

REFERENCE_MV = 3300
PIN_ADDRESS = 21
TYPE = motoron.CurrentSenseType.MOTORON_24V16
MAX_ACCELERATION = 40
MAX_DECELERATION = 20
CURRENT_LIMIT= 13000


def configure_motor(mc, motor_id):
    # set acceleration / deceleration maximums
    mc.set_max_acceleration(motor_id, MAX_ACCELERATION)
    mc.set_max_deceleration(motor_id, MAX_DECELERATION)

    # set currnt limit on motor
    current_offset = mc.get_current_sense_offset(motor_id)
    limit = motoron.calculate_current_limit(CURRENT_LIMIT, TYPE, REFERENCE_MV, current_offset)
    mc.set_current_limit(motor_id, limit)

def initialise_motor():
    # initialise the motor controller
    mc = motoron.MotoronI2C()
    mc.reinitialize()
    mc.disable_crc()
    mc.clear_reset_flag()
    mc.disable_command_timeout()

    configure_motor(mc, 1)
    configure_motor(mc, 2)

    return mc


mc = initialise_motor()

# set speed to 0
mc.set_speed(1, 0)
mc.set_speed(2, 0)


# switch brake ON (pin off)
brake_line = LED(PIN_ADDRESS)
brake_line.off()
        

# === ./motoron.py ===
# Copyright (C) Pololu Corporation.  See LICENSE.txt for details.

import math
import os
import struct
import time

try:
  from enum import Enum
  def enum_value(x): return x.value
except ImportError:
  Enum = object
  def enum_value(x): return x

from motoron_protocol import *

## \file motoron.py
##
## This is the main file for the Motoron Motor Controller Python library for
## Raspberry Pi.
##
## For more information about the library, see the main repository at:
## https://github.com/pololu/motoron-python

class CurrentSenseType(Enum):
  MOTORON_18V18 = 0b0001
  MOTORON_24V14 = 0b0101
  MOTORON_18V20 = 0b1010
  MOTORON_24V16 = 0b1101

class VinSenseType(Enum):
  MOTORON_256 =  0b0000  # M*256 Motorons
  MOTORON_HP  =  0b0010  # High-power Motorons
  MOTORON_550 =  0b0011  # M*550 Motorons

class MotoronBase():
  """
  Represents a connection to a Pololu Motoron Motoron Controller.
  """

  __DEFAULT_PROTOCOL_OPTIONS = (
    (1 << PROTOCOL_OPTION_I2C_GENERAL_CALL) |
    (1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS) |
    (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  ## The default value of the Motoron's "Error mask" variable.
  DEFAULT_ERROR_MASK = (
    (1 << STATUS_FLAG_COMMAND_TIMEOUT) |
    (1 << STATUS_FLAG_RESET))

  def __init__(self):
    ## The bits in this variable are defined by the
    ## motoron.PROTOCOL_OPTION_* constants.  See set_protocol_options().
    self.protocol_options = MotoronBase.__DEFAULT_PROTOCOL_OPTIONS

  def get_firmware_version(self):
    """
    Sends the "Get firmware version" command to get the device's firmware
    product ID and firmware version numbers.

    For more information, see the "Get firmware version"
    command in the Motoron user's guide.

    \return A dictionary in this format:
    ```{.py}
    {'product_id': 204, 'firmware_version': {'major': 1, 'minor': 0}}
    ```
    """
    cmd = [CMD_GET_FIRMWARE_VERSION]
    response = self._send_command_and_read_response(cmd, 4)
    product_id, minor, major = struct.unpack('<HBB', response)
    return {
      'product_id': product_id,
      'firmware_version': {'major': major, 'minor': minor}
    }

  def set_protocol_options(self, options):
    """
    Sends the "Set protocol options" command to the device to specify options
    related to how the device processes commands and sends responses.
    The options are also saved in this object and are used later
    when sending other commands or reading responses.

    When CRC for commands is enabled, this library generates the CRC
    byte and appends it to the end of each command it sends.  The Motoron
    checks it to help ensure the command was received correctly.

    When CRC for responses is enabled, this library reads the CRC byte sent
    by the Motoron in its responses and makes sure it is correct.  If the
    response CRC byte is incorrect, get_last_error() will return a non-zero
    error code after the command has been run.

    When the I2C general call address is enabled, the Motoron receives
    commands sent to address 0 in addition to its usual I2C address.
    The general call address is write-only; reading bytes from it is not
    supported.

    By default (in this libary and the Motoron itself), CRC for commands and
    responses is enabled, and the I2C general call address is enabled.

    This method always sends its command with a CRC byte, so it will work
    even if CRC was previously disabled but has been re-enabled on the device
    (e.g. due to a reset).

    The @p options argument should be 0 or combination of the following
    expressions made using the bitwise or operator (|):
    - (1 << motoron.PROTOCOL_OPTION_CRC_FOR_COMMANDS)
    - (1 << motoron.PROTOCOL_OPTION_CRC_FOR_RESPONSES)
    - (1 << motoron.PROTOCOL_OPTION_I2C_GENERAL_CALL)

    For more information, see the "Set protocol optons"
    command in the Motoron user's guide.

    @sa enable_crc(), disable_crc(),
      enable_crc_for_commands(), disable_crc_for_commands(),
      enable_crc_for_responses(), disable_crc_for_responses(),
      enable_i2c_general_call(), disable_i2c_general_call()
    """
    self.protocol_options = options
    cmd = [
      CMD_SET_PROTOCOL_OPTIONS,
      options & 0x7F,
      ~options & 0x7F
    ]
    self._send_command_core(cmd, True)

  def set_protocol_options_locally(self, options):
    """
    Sets the protocol options for this object, without sending a command to
    the Motoron.

    If the options you specify here do not match the actual configuration of
    the Motoron, future communication could fail.

    Most users should use set_protocol_options() instead of this.
    """
    self.protocol_options = options

  def enable_crc(self):
    """
    Enables CRC for commands and responses.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      | (1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS)
      | (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  def disable_crc(self):
    """
    Disables CRC for commands and responses.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      & ~(1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS)
      & ~(1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  def enable_crc_for_commands(self):
    """
    Enables CRC for commands.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      | (1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS))

  def disable_crc_for_commands(self):
    """
    Disables CRC for commands.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      & ~(1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS))

  def enable_crc_for_responses(self):
    """
    Enables CRC for responses.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      | (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  def disable_crc_for_responses(self):
    """
    Disables CRC for responses.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      & ~(1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))

  def enable_i2c_general_call(self):
    """
    Enables the I2C general call address.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      | (1 << PROTOCOL_OPTION_I2C_GENERAL_CALL))

  def disable_i2c_general_call(self):
    """
    Disables the I2C general call address.  See set_protocol_options().
    """
    self.set_protocol_options(self.protocol_options
      & ~(1 << PROTOCOL_OPTION_I2C_GENERAL_CALL))

  def read_eeprom(self, offset, length):
    """
    Reads the specified bytes from the Motoron's EEPROM memory.

    For more information, see the "Read EEPROM" command in the
    Motoron user's guide.
    """
    cmd = [
      CMD_READ_EEPROM,
      offset & 0x7F,
      length & 0x7F,
    ]
    return self._send_command_and_read_response(cmd, length)

  def read_eeprom_device_number(self):
    """
    Reads the EEPROM device number from the device.
    This is the I2C address that the device uses if it detects that JMP1
    is shorted to GND when it starts up.  It is stored in non-volatile
    EEPROM memory.
    """
    return self.read_eeprom(SETTING_DEVICE_NUMBER, 1)[0]

  def write_eeprom(self, offset, value):
    """
    Writes a value to one byte in the Motoron's EEPROM memory.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**

    For more information, see the "Write EEPROM" command in the
    Motoron user's guide.
    """
    cmd = [
      CMD_WRITE_EEPROM,
      offset & 0x7F,
      value & 0x7F,
      (value >> 7) & 1,
    ]
    cmd += [
      cmd[1] ^ 0x7F,
      cmd[2] ^ 0x7F,
      cmd[3] ^ 0x7F,
    ]
    self._send_command(cmd)
    time.sleep(0.006)

  def write_eeprom16(self, offset, value):
    """
    Writes a 2-byte value in the Motoron's EEPROM memory.

    This command only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron’s microcontroller is only rated for
    100,000 erase/write cycles.
    """
    self.write_eeprom(offset, value & 0xFF)
    self.write_eeprom(offset + 1, value >> 8 & 0xFF)

  def write_eeprom_device_number(self, number):
    """
    Writes to the EEPROM device number, changing it to the specified value.

    This command only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**
    """
    self.write_eeprom(SETTING_DEVICE_NUMBER, number & 0x7F)
    self.write_eeprom(SETTING_DEVICE_NUMBER + 1, number >> 7 & 0x7F)

  def write_eeprom_alternative_device_number(self, number):
    """
    Writes to the alternative device number stored in EEPROM, changing it to
    the specified value.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**

    @sa write_eeprom_disable_alternative_device_number()
    """
    self.write_eeprom(SETTING_ALTERNATIVE_DEVICE_NUMBER, (number & 0x7F) | 0x80)
    self.write_eeprom(SETTING_ALTERNATIVE_DEVICE_NUMBER + 1, number >> 7 & 0x7F)

  def write_eeprom_disable_alternative_device_number(self):
    """
    Writes to EEPROM to disable the alternative device number.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**

    @sa write_eeprom_alternative_device_number()
    """
    self.write_eeprom(SETTING_ALTERNATIVE_DEVICE_NUMBER, 0)
    self.write_eeprom(SETTING_ALTERNATIVE_DEVICE_NUMBER + 1, 0)

  def write_eeprom_communication_options(self, options):
    """
    Writes to the serial options byte stored in EEPROM, changing it to
    the specified value.

    The bits in this byte are defined by the
    MOTORON_COMMUNICATION_OPTION_* constants.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**
    """
    self.write_eeprom(SETTING_COMMUNICATION_OPTIONS, options)

  def write_eeprom_baud_rate(self, baud):
    """
    Writes to the baud rate stored in EEPROM, changing it to the
    specified value.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**
    """
    if (baud < MIN_BAUD_RATE): baud = MIN_BAUD_RATE
    if (baud > MAX_BAUD_RATE): baud = MAX_BAUD_RATE
    self.write_eeprom16(SETTING_BAUD_DIVIDER, round(16000000 / baud))

  def write_eeprom_response_delay(self, delay):
    """
    Writes to the serial response delay setting stored in EEPROM, changing
    it to the specified value, in units of microseconds.

    This function is only useful on Motorons with a serial interface,
    and only has an effect if JMP1 is shorted to GND.

    **Warning: Be careful not to write to the EEPROM in a fast loop. The
    EEPROM memory of the Motoron's microcontroller is only rated for
    100,000 erase/write cycles.**
    """
    self.write_eeprom(SETTING_RESPONSE_DELAY, delay)

  def reinitialize(self):
    """
    Sends a "Reinitialize" command to the Motoron, which resets most of the
    Motoron's variables to their default state.

    For more information, see the "Reinitialize" command in the Motoron
    user's guide.

    @sa reset()
    """
    # Always send the reinitialize command with a CRC byte to make it more reliable.
    cmd = [CMD_REINITIALIZE]
    self._send_command_core(cmd, True)
    self.protocol_options = MotoronBase.__DEFAULT_PROTOCOL_OPTIONS

  def reset(self, ignore_nack=True):
    """
    Sends a "Reset" command to the Motoron, which does a full hardware reset.

    This command is equivalent to briefly driving the Motoron's RST pin low.
    The Motoron's RST is briefly driven low by the Motoron itself as a
    result of this command.

    After running this command, we recommend waiting for at least 5
    milliseconds before you try to communicate with the Motoron.

    @param ignore_nack Optional argument: if `True` (the default), this method
      ignores a NACK error if it occurs on sending the Reset command. This is
      useful in case the Motoron has CRC off and executes the reset before it
      can ACK the CRC byte (which this method always sends to make it more
      reliable).

    @sa reinitialize()
    """
    # Always send the reset command with a CRC byte to make it more reliable.
    cmd = [CMD_RESET]
    try:
      self._send_command_core(cmd, True)
    except OSError as e:
      # Errno 5 (Input/output error) or 121 (Remote I/O error) indicates a
      # NACK of a data byte.  Ignore it if the ignore_nack argument is True.
      # In all other cases, re-raise the exception.
      if not (ignore_nack and (e.args[0] == 5 or e.args[0] == 121)): raise
    self.protocol_options = MotoronBase.__DEFAULT_PROTOCOL_OPTIONS

  def get_variables(self, motor, offset, length):
    """
    Reads information from the Motoron using a "Get variables" command.

    This library has helper methods to read every variable, but this method
    is useful if you want to get the raw bytes, or if you want to read
    multiple consecutive variables at the same time for efficiency.

    @param motor 0 to read general variables, or a motor number to read
      motor-specific variables.
    @param offset The location of the first byte to read.
    @param length How many bytes to read.
    """
    cmd = [
      CMD_GET_VARIABLES,
      motor & 0x7F,
      offset & 0x7F,
      length & 0x7F,
    ]
    return self._send_command_and_read_response(cmd, length)

  def get_var_u8(self, motor, offset):
    """
    Reads one byte from the Motoron using a "Get variables" command
    and returns the result as an unsigned 8-bit integer.

    @param motor 0 to read a general variable, or a motor number to read
      a motor-specific variable.
    @param offset The location of the byte to read.
    """
    return self.get_variables(motor, offset, 1)[0]

  def get_var_u16(self, motor, offset):
    """
    Reads two bytes from the Motoron using a "Get variables" command
    and returns the result as an unsigned 16-bit integer.

    @param motor 0 to read general variables, or a motor number to read
      motor-specific variables.
    @param offset The location of the first byte to read.
    """
    buffer = self.get_variables(motor, offset, 2)
    return struct.unpack('<H', buffer)[0]

  def get_var_s16(self, motor, offset):
    """
    Reads two bytes from the Motoron using a "Get variables" command
    and returns the result as a signed 16-bit integer.

    @param motor 0 to read general variables, or a motor number to read
      motor-specific variables.
    @param offset The location of the first byte to read.
    """
    buffer = self.get_variables(motor, offset, 2)
    return struct.unpack('<h', buffer)[0]

  def get_status_flags(self):
    """
    Reads the "Status flags" variable from the Motoron.

    The bits in this variable are defined by the STATUS_FLAGS_*
    constants in the motoron package:

    - motoron.STATUS_FLAG_PROTOCOL_ERROR
    - motoron.STATUS_FLAG_CRC_ERROR
    - motoron.STATUS_FLAG_COMMAND_TIMEOUT_LATCHED
    - motoron.STATUS_FLAG_MOTOR_FAULT_LATCHED
    - motoron.STATUS_FLAG_NO_POWER_LATCHED
    - motoron.STATUS_FLAG_RESET
    - motoron.STATUS_FLAG_COMMAND_TIMEOUT
    - motoron.STATUS_FLAG_MOTOR_FAULTING
    - motoron.STATUS_FLAG_NO_POWER
    - motoron.STATUS_FLAG_ERROR_ACTIVE
    - motoron.STATUS_FLAG_MOTOR_OUTPUT_ENABLED
    - motoron.STATUS_FLAG_MOTOR_DRIVING

    Here is some example code that uses bitwise operators to check
    whether there is currently a motor fault or a lack of power:

    ```{.py}
    mask = ((1 << motoron.STATUS_FLAG_NO_POWER) |
      (1 << motoron.STATUS_FLAG_MOTOR_FAULTING))
    if mc.get_status_flags() & mask: # do something
    ```

    This library has helper methods that make it easier if you just want to
    read a single bit:

    - get_protocol_error_flag()
    - get_crc_error_flag()
    - get_command_timeout_latched_flag()
    - get_motor_fault_latched_flag()
    - get_no_power_latched_flag()
    - get_reset_flag()
    - get_motor_faulting_flag()
    - get_no_power_flag()
    - get_error_active_flag()
    - get_motor_output_enabled_flag()
    - get_motor_driving_flag()

    The clear_latched_status_flags() method sets the specified set of latched
    status flags to 0.  The reinitialize() and reset() commands reset the
    latched status flags to their default values.

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return self.get_var_u16(0, VAR_STATUS_FLAGS)

  def get_protocol_error_flag(self):
    """
    Returns the "Protocol error" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_PROTOCOL_ERROR))

  def get_crc_error_flag(self):
    """
    Returns the "CRC error" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_CRC_ERROR))

  def get_command_timeout_latched_flag(self):
    """
    Returns the "Command timeout latched" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_COMMAND_TIMEOUT_LATCHED))

  def get_motor_fault_latched_flag(self):
    """
    Returns the "Motor fault latched" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_MOTOR_FAULT_LATCHED))

  def get_no_power_latched_flag(self):
    """
    Returns the "No power latched" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_NO_POWER_LATCHED))

  def get_reset_flag(self):
    """
    Returns the "Reset" bit from get_status_flags().

    This bit is set to 1 when the Motoron powers on, its processor is
    reset (e.g. by reset()), or it receives a reinitialize() command.
    It can be cleared using clear_reset_flag() or clear_latched_status_flags().

    By default, the Motoron is configured to treat this bit as an error,
    so you will need to clear it before you can turn on the motors.

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_RESET))

  def get_motor_faulting_flag(self):
    """
    Returns the "Motor faulting" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_MOTOR_FAULTING))

  def get_no_power_flag(self):
    """
    Returns the "No power" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_NO_POWER))

  def get_error_active_flag(self):
    """
    Returns the "Error active" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_ERROR_ACTIVE))

  def get_motor_output_enabled_flag(self):
    """
    Returns the "Motor output enabled" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_MOTOR_OUTPUT_ENABLED))

  def get_motor_driving_flag(self):
    """
    Returns the "Motor driving" bit from get_status_flags().

    For more information, see the "Status flags" variable in the Motoron
    user's guide.
    """
    return bool(self.get_status_flags() & (1 << STATUS_FLAG_MOTOR_DRIVING))

  def get_vin_voltage(self):
    """
    Reads voltage on the Motoron's VIN pin, in raw device units.

    For more information, see the "VIN voltage" variable in the Motoron
    user's guide.

    @sa get_vin_voltage_mv()
    """
    return self.get_var_u16(0, VAR_VIN_VOLTAGE)

  def get_vin_voltage_mv(self, reference_mv=3300, type=VinSenseType.MOTORON_256):
    """
    Reads the voltage on the Motoron's VIN pin and converts it to millivolts.

    For more information, see the "VIN voltage" variable in the Motoron
    user's guide.

    @param reference_mv The logic voltage of the Motoron, in millivolts.
      This is assumed to be 3300 by default.
    @param type Specifies what type of Motoron you are using.  This should be one
      of the members of the motoron.VinSenseType enum.

    @sa get_vin_voltage()
    """
    scale = 459 if enum_value(type) & 1 else 1047
    return self.get_vin_voltage() * reference_mv / 1024 * scale / 47

  def get_command_timeout_milliseconds(self):
    """
    Reads the "Command timeout" variable and converts it to milliseconds.

    For more information, see the "Command timeout" variable in the Motoron
    user's guide.

    @sa set_command_timeout_milliseconds()
    """
    return self.get_var_u16(0, VAR_COMMAND_TIMEOUT) * 4

  def get_error_response(self):
    """
    Reads the "Error response" variable, which defines how the Motoron will
    stop its motors when an error is happening.

    For more information, see the "Error response" variable in the Motoron
    user's guide.

    @sa set_error_response()
    """
    return self.get_var_u8(0, VAR_ERROR_RESPONSE)

  def get_error_mask(self):
    """
    Reads the "Error mask" variable, which defines which status flags are
    considered to be errors.

    For more information, see the "Error mask" variable in the Motoron
    user's guide.

    @sa set_error_mask()
    """
    return self.get_var_u16(0, VAR_ERROR_MASK)

  def get_jumper_state(self):
    """
    Reads the "Jumper state" variable.

    For more information, see the "Jumper state" variable in the Motoron
    user's guide
    """
    return self.get_var_u8(0, VAR_JUMPER_STATE)

  def get_target_speed(self, motor):
    """
    Reads the target speed of the specified motor, which is the speed at
    which the motor has been commanded to move.

    For more information, see the "Target speed" variable in the Motoron
    user's guide.

    @sa set_speed(), set_all_speeds(), set_all_speeds_using_buffers()
    """
    return self.get_var_s16(motor, MVAR_TARGET_SPEED)

  def get_target_brake_amount(self, motor):
    """
    Reads the target brake amount for the specified motor.

    For more information, see the "Target speed" variable in the Motoron
    user's guide.

    @sa set_target_brake_amount()
    """
    return self.get_var_u16(motor, MVAR_TARGET_BRAKE_AMOUNT)

  def get_current_speed(self, motor):
    """
    Reads the current speed of the specified motor, which is the speed that
    the Motoron is currently trying to apply to the motor.

    For more information, see the "Target speed" variable in the Motoron
    user's guide.

    @sa set_speed_now(), set_all_speeds_now(), set_all_speeds_now_using_buffers()
    """
    return self.get_var_s16(motor, MVAR_CURRENT_SPEED)

  def get_buffered_speed(self, motor):
    """
    Reads the buffered speed of the specified motor.

    For more information, see the "Buffered speed" variable in the Motoron
    user's guide.

    @sa set_buffered_speed(), set_all_buffered_speeds()
    """
    return self.get_var_s16(motor, MVAR_BUFFERED_SPEED)

  def get_pwm_mode(self, motor):
    """
    Reads the PWM mode of the specified motor.

    For more information, see the "PWM mode" variable in the Motoron
    user's guide.

    @sa set_pwm_mode()
    """
    return self.get_var_u8(motor, MVAR_PWM_MODE)

  def get_max_acceleration_forward(self, motor):
    """
    Reads the maximum acceleration of the specified motor for the forward
    direction.

    For more information, see the "Max acceleration forward" variable in the
    Motoron user's guide.

    @sa set_max_acceleration(), set_max_acceleration_forward()
    """
    return self.get_var_u16(motor, MVAR_MAX_ACCEL_FORWARD)

  def get_max_acceleration_reverse(self, motor):
    """
    Reads the maximum acceleration of the specified motor for the reverse
    direction.

    For more information, see the "Max acceleration reverse" variable in the
    Motoron user's guide.

    @sa set_max_acceleration(), set_max_acceleration_reverse()
    """
    return self.get_var_u16(motor, MVAR_MAX_ACCEL_REVERSE)

  def get_max_deceleration_forward(self, motor):
    """
    Reads the maximum deceleration of the specified motor for the forward
    direction.

    For more information, see the "Max deceleration forward" variable in the
    Motoron user's guide.

    @sa set_max_deceleration(), set_max_deceleration_forward()
    """
    return self.get_var_u16(motor, MVAR_MAX_DECEL_FORWARD)

  def get_max_deceleration_reverse(self, motor):
    """
    Reads the maximum deceleration of the specified motor for the reverse
    direction.

    For more information, see the "Max deceleration reverse" variable in the
    Motoron user's guide.

    @sa set_max_deceleration(), set_max_deceleration_reverse()
    """
    return self.get_var_u16(motor, MVAR_MAX_DECEL_REVERSE)


# @cond

  # This function is used by Pololu for testing.
  def get_max_deceleration_temporary(self, motor):
    return self.get_var_u16(motor, MVAR_MAX_DECEL_TMP)

# \endcond

  def get_starting_speed_forward(self, motor):
    """
    Reads the starting speed for the specified motor in the forward direction.

    For more information, see the "Starting speed forward" variable in the
    Motoron user's guide.

    @sa set_starting_speed(), set_starting_speed_forward()
    """
    return self.get_var_u16(motor, MVAR_STARTING_SPEED_FORWARD)

  def get_starting_speed_reverse(self, motor):
    """
    Reads the starting speed for the specified motor in the reverse direction.

    For more information, see the "Starting speed reverse" variable in the
    Motoron user's guide.

    @sa set_starting_speed(), set_starting_speed_reverse()
    """
    return self.get_var_u16(motor, MVAR_STARTING_SPEED_REVERSE)

  def get_direction_change_delay_forward(self, motor):
    """
    Reads the direction change delay for the specified motor in the
    forward direction.

    For more information, see the "Direction change delay forward" variable
    in the Motoron user's guide.

    @sa set_direction_change_delay(), set_direction_change_delay_forward()
    """
    return self.get_var_u8(motor, MVAR_DIRECTION_CHANGE_DELAY_FORWARD)

  def get_direction_change_delay_reverse(self, motor):
    """
    Reads the direction change delay for the specified motor in the
    reverse direction.

    For more information, see the "Direction change delay reverse" variable
    in the Motoron user's guide.

    @sa set_direction_change_delay(), set_direction_change_delay_reverse()
    """
    return self.get_var_u8(motor, MVAR_DIRECTION_CHANGE_DELAY_REVERSE)

  def get_current_limit(self, motor):
    """
    Reads the current limit for the specified motor.

    This only works for the high-power Motorons.

    For more information, see the "Current limit" variable in the Motoron user's
    guide.

    @sa set_current_limit()
    """
    return self.get_var_u16(motor, MVAR_CURRENT_LIMIT)

  def get_current_sense_reading(self, motor):
    """
    Reads all the results from the last current sense measurement for the
    specified motor.

    This function reads the "Current sense raw", "Current sense speed", and
    "Current sense processed" variables from the Motoron using a single
    command, so the values returned are all guaranteed to be part of the
    same measurement.

    This only works for the high-power Motorons.

    @sa get_current_sense_raw_and_speed(), get_current_sense_processed_and_speed()
    """
    buffer = self.get_variables(motor, MVAR_CURRENT_SENSE_RAW, 6)
    raw, speed, processed = struct.unpack('<HhH', buffer)
    return { 'raw': raw, 'speed': speed, 'processed': processed }

  def get_current_sense_raw_and_speed(self, motor):
    """
    This is like get_current_sense_reading() but it only reads the raw current
    sense measurement and the speed.

    This only works for the high-power Motorons.
    """
    buffer = self.get_variables(motor, MVAR_CURRENT_SENSE_RAW, 4)
    raw, speed = struct.unpack('<Hh', buffer)
    return { 'raw': raw, 'speed': speed }

  def get_current_sense_processed_and_speed(self, motor):
    """
    This is like get_current_sense_reading() but it only reads the processed
    current sense measurement and the speed.

    This only works for the high-power Motorons.
    """
    buffer = self.get_variables(motor, MVAR_CURRENT_SENSE_SPEED, 4)
    speed, processed = struct.unpack('<hH', buffer)
    return { 'speed': speed, 'processed': processed }

  def get_current_sense_raw(self, motor):
    """
    Reads the raw current sense measurement for the specified motor.

    This only works for the high-power Motorons.

    For more information, see the "Current sense raw" variable
    in the Motoron user's guide.

    @sa get_current_sense_reading()
    """
    return self.get_var_u16(motor, MVAR_CURRENT_SENSE_RAW)


  def get_current_sense_processed(self, motor):
    """
    Reads the processed current sense reading for the specified motor.

    This only works for the high-power Motorons.

    The units of this reading depend on the logic voltage of the Motoron
    and on the specific model of Motoron that you have, and you can use
    current_sense_units_milliamps() to calculate the units.

    The accuracy of this reading can be improved by measuring the current
    sense offset and setting it with set_current_sense_offset().
    See the "Current sense processed" variable in the Motoron user's guide for
    or the current_sense_calibrate example for more information.

    Note that this reading will be 0xFFFF if an overflow happens during the
    calculation due to very high current.

    @sa get_current_sense_processed_and_speed()
    """
    return self.get_var_u16(motor, MVAR_CURRENT_SENSE_PROCESSED)

  def get_current_sense_offset(self, motor):
    """
    Reads the current sense offset setting.

    This only works for the high-power Motorons.

    For more information, see the "Current sense offset" variable in the
    Motoron user's guide.

    @sa set_current_sense_offset()
    """
    return self.get_var_u8(motor, MVAR_CURRENT_SENSE_OFFSET)

  def get_current_sense_minimum_divisor(self, motor):
    """
    Reads the current sense minimum divisor setting and returns it as a speed
    between 0 and 800.

    This only works for the high-power Motorons.

    For more information, see the "Current sense minimum divisor" variable in
    the Motoron user's guide.

    @sa set_current_sense_minimum_divisor()
    """
    return self.get_var_u8(motor, MVAR_CURRENT_SENSE_MINIMUM_DIVISOR) << 2


  def set_variable(self, motor, offset, value):
    """
    Configures the Motoron using a "Set variable" command.

    This library has helper methods to set every variable, so you should
    not need to call this function directly.

    @param motor 0 to set a general variable, or a motor number to set
      motor-specific variables.
    @param offset The address of the variable to set (only certain offsets
      are allowed).
    @param value The value to set the variable to.

    @sa get_variables()
    """
    if value > 0x3FFF: value = 0x3FFF
    cmd = [
      CMD_SET_VARIABLE,
      motor & 0x1F,
      offset & 0x7F,
      value & 0x7F,
      (value >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_command_timeout_milliseconds(self, ms):
    """
    Sets the command timeout period, in milliseconds.

    For more information, see the "Command timeout" variable
    in the Motoron user's guide.

    @sa disable_command_timeout(), get_command_timeout_milliseconds()
    """
    # Divide by 4, but round up.
    timeout = math.ceil(ms / 4)
    self.set_variable(0, VAR_COMMAND_TIMEOUT, timeout)

  def set_error_response(self, response):
    """
    Sets the error response, which defines how the Motoron will
    stop its motors when an error is happening.

    The response parameter should be one of these constants from the motoron
    package:

    - motoron.ERROR_RESPONSE_COAST
    - motoron.ERROR_RESPONSE_BRAKE
    - motoron.ERROR_RESPONSE_COAST_NOW
    - motoron.ERROR_RESPONSE_BRAKE_NOW

    For more information, see the "Error response" variable in the Motoron
    user's guide.

    @sa get_error_response()
    """
    self.set_variable(0, VAR_ERROR_RESPONSE, response)

  def set_error_mask(self, mask):
    """
    Sets the "Error mask" variable, which defines which status flags are
    considered to be errors.

    For more information, see the "Error mask" variable in the Motoron
    user's guide.

    @sa get_error_mask(), get_status_flags()
    """
    self.set_variable(0, VAR_ERROR_MASK, mask)

  def disable_command_timeout(self):
    """
    This disables the Motoron's command timeout feature by resetting the
    the "Error mask" variable to its default value but with the command
    timeout bit cleared.

    By default, the Motoron's command timeout will occur if no valid commands
    are received in 1500 milliseconds, and the command timeout is treated as
    an error, so the motors will shut down.  You can use this function if you
    want to disable that feature.

    Note that this function overrides any previous values you set in the
    "Error mask" variable, so if you are using set_error_mask() in your program
    to configure which status flags are treated as errors, you do not need to
    use this function and you probably should not use this function.

    @sa set_command_timeout_milliseconds(), set_error_mask()
    """
    self.set_error_mask(MotoronBase.DEFAULT_ERROR_MASK & ~(1 << STATUS_FLAG_COMMAND_TIMEOUT))

  def set_pwm_mode(self, motor, mode):
    """
    Sets the PWM mode for the specified motor.

    The mode parameter should be one of the following these constants from
    the motoron package:

    - motoron.PWM_MODE_DEFAULT (20 kHz)
    - motoron.PWM_MODE_1_KHZ 1
    - motoron.PWM_MODE_2_KHZ 2
    - motoron.PWM_MODE_4_KHZ 3
    - motoron.PWM_MODE_5_KHZ 4
    - motoron.PWM_MODE_10_KHZ 5
    - motoron.PWM_MODE_20_KHZ 6
    - motoron.PWM_MODE_40_KHZ 7
    - motoron.PWM_MODE_80_KHZ 8

    For more information, see the "PWM mode" variable in the Motoron user's
    guide.

    @sa get_pwm_mode(self)
    """
    self.set_variable(motor, MVAR_PWM_MODE, mode)

  def set_max_acceleration_forward(self, motor, accel):
    """
    Sets the maximum acceleration of the specified motor for the forward
    direction.

    For more information, see the "Max acceleration forward" variable in the
    Motoron user's guide.

    @sa set_max_acceleration(), get_max_acceleration_forward()
    """
    self.set_variable(motor, MVAR_MAX_ACCEL_FORWARD, accel)

  def set_max_acceleration_reverse(self, motor, accel):
    """
    Sets the maximum acceleration of the specified motor for the reverse
    direction.

    For more information, see the "Max acceleration reverse" variable in the
    Motoron user's guide.

    @sa set_max_acceleration(), get_max_acceleration_reverse()
    """
    self.set_variable(motor, MVAR_MAX_ACCEL_REVERSE, accel)

  def set_max_acceleration(self, motor, accel):
    """
    Sets the maximum acceleration of the specified motor (both directions).

    If this function succeeds, it is equivalent to calling
    set_max_acceleration_forward() and set_max_acceleration_reverse().
    """
    self.set_max_acceleration_forward(motor, accel)
    self.set_max_acceleration_reverse(motor, accel)

  def set_max_deceleration_forward(self, motor, decel):
    """
    Sets the maximum deceleration of the specified motor for the forward
    direction.

    For more information, see the "Max deceleration forward" variable in the
    Motoron user's guide.

    @sa set_max_deceleration(), get_max_deceleration_forward()
    """
    self.set_variable(motor, MVAR_MAX_DECEL_FORWARD, decel)

  def set_max_deceleration_reverse(self, motor, decel):
    """
    Sets the maximum deceleration of the specified motor for the reverse
    direction.

    For more information, see the "Max deceleration reverse" variable in the
    Motoron user's guide.

    @sa set_max_deceleration(), get_max_deceleration_reverse()
    """
    self.set_variable(motor, MVAR_MAX_DECEL_REVERSE, decel)

  def set_max_deceleration(self, motor, decel):
    """
    Sets the maximum deceleration of the specified motor (both directions).

    If this function succeeds, it is equivalent to calling
    set_max_deceleration_forward() and set_max_deceleration_reverse().
    """
    self.set_max_deceleration_forward(motor, decel)
    self.set_max_deceleration_reverse(motor, decel)

  def set_starting_speed_forward(self, motor, speed):
    """
    Sets the starting speed of the specified motor for the forward
    direction.

    For more information, see the "Starting speed forward" variable in the
    Motoron user's guide.

    @sa set_starting_speed(), get_starting_speed_forward()
    """
    self.set_variable(motor, MVAR_STARTING_SPEED_FORWARD, speed)

  def set_starting_speed_reverse(self, motor, speed):
    """
    Sets the starting speed of the specified motor for the reverse
    direction.

    For more information, see the "Starting speed reverse" variable in the
    Motoron user's guide.

    @sa set_starting_speed(), get_starting_speed_reverse()
    """
    self.set_variable(motor, MVAR_STARTING_SPEED_REVERSE, speed)

  def set_starting_speed(self, motor, speed):
    """
    Sets the starting speed of the specified motor (both directions).

    If this function succeeds, it is equivalent to calling
    set_starting_speed_forward() and set_starting_speed_reverse().
    """
    self.set_starting_speed_forward(motor, speed)
    self.set_starting_speed_reverse(motor, speed)

  def set_direction_change_delay_forward(self, motor, duration):
    """
    Sets the direction change delay of the specified motor for the forward
    direction, in units of 10 ms.

    For more information, see the "Direction change delay forward" variable
    in the Motoron user's guide.

    @sa set_direction_change_delay(), get_direction_change_delay_forward()
    """
    self.set_variable(motor, MVAR_DIRECTION_CHANGE_DELAY_FORWARD, duration)

  def set_direction_change_delay_reverse(self, motor, duration):
    """
    Sets the direction change delay of the specified motor for the reverse
    direction, in units of 10 ms.

    For more information, see the "Direction change delay reverse" variable
    in the Motoron user's guide.

    @sa set_direction_change_delay(), get_direction_change_delay_reverse()
    """
    self.set_variable(motor, MVAR_DIRECTION_CHANGE_DELAY_REVERSE, duration)

  def set_direction_change_delay(self, motor, duration):
    """
    Sets the direction change delay of the specified motor (both directions),
    in units of 10 ms.

    If this function succeeds, it is equivalent to calling
    set_direction_change_delay_forward() and set_direction_change_delay_reverse().
    """
    self.set_direction_change_delay_forward(motor, duration)
    self.set_direction_change_delay_reverse(motor, duration)

  def set_current_limit(self, motor, limit):
    """
    Sets the current limit for the specified motor.

    This only works for the high-power Motorons.

    The units of the current limit depend on the type of Motoron you have
    and the logic voltage of your system.  See the "Current limit" variable
    in the Motoron user's guide for more information, or see
    calculate_current_limit().

    @sa get_current_limit()
    """
    self.set_variable(motor, MVAR_CURRENT_LIMIT, limit)

  def set_current_sense_offset(self, motor, offset):
    """
    Sets the current sense offset setting for the specified motor.

    This is one of the settings that determines how current sense
    readings are processed.  It is supposed to be the value returned by
    get_current_sense_raw() when motor power is supplied to the Motoron and
    it is driving its motor outputs at speed 0.

    The current_sense_calibrate example shows how to measure the current
    sense offsets and load them onto the Motoron using this function.

    If you do not care about measuring motor current, you do not need to
    set this variable.

    For more information, see the "Current sense offset" variable in the
    Motoron user's guide.

    This only works for the high-power Motorons.

    @sa get_current_sense_offset()
    """
    self.set_variable(motor, MVAR_CURRENT_SENSE_OFFSET, offset)

  def set_current_sense_minimum_divisor(self, motor, speed):
    """
    Sets the current sense minimum divisor setting for the specified motor,
    given a speed between 0 and 800.

    This is one of the settings that determines how current sense
    readings are processed.

    If you do not care about measuring motor current, you do not need to
    set this variable.

    For more information, see the "Current sense minimum divisor" variable in
    the Motoron user's guide.

    This only works for the high-power Motorons.

    @sa get_current_sense_minimum_divisor()
    """
    self.set_variable(motor, MVAR_CURRENT_SENSE_MINIMUM_DIVISOR, speed >> 2)

  def coast_now(self):
    """
    Sends a "Coast now" command to the Motoron, causing all of the motors to
    immediately start coasting.

    For more information, see the "Coast now" command in the Motoron
    user's guide.
    """
    cmd = [CMD_COAST_NOW]
    self._send_command(cmd)

  def clear_motor_fault(self, flags=0):
    """
    Sends a "Clear motor fault" command to the Motoron.

    If any of the Motoron's motors chips are currently experiencing a
    fault (error), or bit 0 of the flags argument is 1, this command makes
    the Motoron attempt to recover from the faults.

    For more information, see the "Clear motor fault" command in the Motoron
    user's guide.

    @sa clear_motor_fault_unconditional(), get_motor_faulting_flag()
    """
    cmd = [ CMD_CLEAR_MOTOR_FAULT, (flags & 0x7F) ]
    self._send_command(cmd)

  def clear_motor_fault_unconditional(self):
    """
    Sends a "Clear motor fault" command to the Motoron with the
    "unconditional" flag set, so the Motoron will attempt to recover
    from any motor faults even if no fault is currently occurring.

    This is a more robust version of clear_motor_fault().
    """
    self.clear_motor_fault(1 << CLEAR_MOTOR_FAULT_UNCONDITIONAL)

  def clear_latched_status_flags(self, flags):
    """
    Clears the specified flags in get_status_flags().

    For each bit in the flags argument that is 1, this command clears the
    corresponding bit in the "Status flags" variable, setting it to 0.

    For more information, see the "Clear latched status flags" command in the
    Motoron user's guide.

    @sa get_status_flags(), set_latched_status_flags()
    """
    cmd = [
      CMD_CLEAR_LATCHED_STATUS_FLAGS,
      flags & 0x7F,
      (flags >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def clear_reset_flag(self):
    """
    Clears the Motoron's reset flag.

    The reset flag is a latched status flag in get_status_flags() that is
    particularly important to clear: it gets set to 1 after the Motoron
    powers on or experiences a reset, and it is considered to be an error
    by default, so it prevents the motors from running.  Therefore, it is
    necessary to call this function (or clear_latched_status_flags()) to clear
    the Reset flag before you can get the motors running.

    We recommend that immediately after you clear the reset flag. you should
    configure the Motoron's motor settings and error response settings.
    That way, if the Motoron experiences an unexpected reset while your system
    is running, it will stop running its motors and it will not start them
    again until all the important settings have been configured.

    @sa clear_latched_status_flags()
    """
    self.clear_latched_status_flags(1 << STATUS_FLAG_RESET)

  def set_latched_status_flags(self, flags):
    """
    Sets the specified flags in get_status_flags().

    For each bit in the flags argument that is 1, this command sets the
    corresponding bit in the "Status flags" variable to 1.

    For more information, see the "Set latched status flags" command in the
    Motoron user's guide.

    @sa get_status_flags(), set_latched_status_flags()
    """
    cmd = [
      CMD_SET_LATCHED_STATUS_FLAGS,
      flags & 0x7F,
      (flags >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_speed(self, motor, speed):
    """
    Sets the target speed of the specified motor.

    The current speed will start moving to the specified target speed,
    obeying any acceleration and deceleration limits.

    The motor number should be between 1 and the number of motors supported
    by the Motoron.

    The speed should be between -800 and 800.  Values outside that range
    will be clipped to -800 or 800 by the Motoron firmware.

    For more information, see the "Set speed" command in the Motoron
    user's guide.

    @sa set_speed_now(), set_all_speeds()
    """
    cmd = [
      CMD_SET_SPEED,
      motor & 0x7F,
      speed & 0x7F,
      (speed >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_speed_now(self, motor, speed):
    """
    Sets the target and current speed of the specified motor, ignoring
    any acceleration and deceleration limits.

    For more information, see the "Set speed" command in the Motoron
    user's guide.

    @sa set_speed(), set_all_speeds_now()
    """
    cmd = [
      CMD_SET_SPEED_NOW,
      motor & 0x7F,
      speed & 0x7F,
      (speed >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_buffered_speed(self, motor, speed):
    """
    Sets the buffered speed of the specified motor.

    This command does not immediately cause any change to the motor: it
    stores a speed for the specified motor in the Motoron so it can be
    used by later commands.

    For more information, see the "Set speed" command in the Motoron
    user's guide.

    @sa set_speed(), set_all_buffered_speeds(),
      set_all_speeds_using_buffers(), set_all_speeds_now_using_buffers()
    """
    cmd = [
      CMD_SET_BUFFERED_SPEED,
      motor & 0x7F,
      speed & 0x7F,
      (speed >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_all_speeds(self, *speeds):
    """
    Sets the target speeds of all the motors at the same time.

    The number of speed arguments you provide to this function must be equal
    to the number of motor channels your Motoron has, or else this command
    might not work.

    This is equivalent to calling set_speed() once for each motor, but it is
    more efficient because all of the speeds are sent in the same command.

    There are a few different ways you can call this method (and the related
    methods that set speeds for all the motors):

    ```{.py}
    # with separate arguments
    mc.set_all_speeds(100, -200, 300)

    # with arguments unpacked from a list
    speeds = [-400, 500, -600]
    mc.set_all_speeds(*speeds)
    ```

    For more information, see the "Set all speeds" command in the Motoron
    user's guide.

    @sa set_speed(), set_all_speeds_now(), set_all_buffered_speeds()
    """
    cmd = [CMD_SET_ALL_SPEEDS]
    for speed in speeds:
      cmd += [
        speed & 0x7F,
        (speed >> 7) & 0x7F,
      ]
    self._send_command(cmd)

  def set_all_speeds_now(self, *speeds):
    """
    Sets the target and current speeds of all the motors at the same time.

    The number of speed arguments you provide to this function must be equal
    to the number of motor channels your Motoron has, or else this command
    might not work.

    This is equivalent to calling set_speed_now() once for each motor, but it is
    more efficient because all of the speeds are sent in the same command.

    For more information, see the "Set all speeds" command in the Motoron
    user's guide.

    @sa set_speed(), set_speed_now(), set_all_speeds()
    """
    cmd = [CMD_SET_ALL_SPEEDS_NOW]
    for speed in speeds:
      cmd += [
        speed & 0x7F,
        (speed >> 7) & 0x7F,
      ]
    self._send_command(cmd)

  def set_all_buffered_speeds(self, *speeds):
    """
    Sets the buffered speeds of all the motors.

    The number of speed arguments you provide to this function must be equal
    to the number of motor channels your Motoron has, or else this command
    might not work.

    This command does not immediately cause any change to the motors: it
    stores speed for each motor in the Motoron so they can be used by later
    commands.

    For more information, see the "Set all speeds" command in the Motoron
    user's guide.

    @sa set_speed(), set_buffered_speed(), set_all_speeds(),
      set_all_speeds_using_buffers(), set_all_speeds_now_using_buffers()
    """
    cmd = [CMD_SET_ALL_BUFFERED_SPEEDS]
    for speed in speeds:
      cmd += [
        speed & 0x7F,
        (speed >> 7) & 0x7F,
      ]
    self._send_command(cmd)

  def set_all_speeds_using_buffers(self):
    """
    Sets each motor's target speed equal to the buffered speed.

    This command is the same as set_all_speeds() except that the speeds are
    provided ahead of time using set_buffered_speed() or set_all_buffered_speeds().

    @sa set_all_speeds_now_using_buffers(), set_buffered_speed(),
      set_all_buffered_speeds()
    """
    cmd = [CMD_SET_ALL_SPEEDS_USING_BUFFERS]
    self._send_command(cmd)

  def set_all_speeds_now_using_buffers(self):
    """
    Sets each motor's target speed and current speed equal to the buffered
    speed.

    This command is the same as set_all_speeds_now() except that the speeds are
    provided ahead of time using set_buffered_speed() or set_all_buffered_speeds().

    @sa set_all_speeds_using_buffers(), set_buffered_speed(),
      set_all_buffered_speeds()
    """
    cmd = [CMD_SET_ALL_SPEEDS_NOW_USING_BUFFERS]
    self._send_command(cmd)

  def set_braking(self, motor, amount):
    """
    Commands the motor to brake, coast, or something in between.

    Sending this command causes the motor to decelerate to speed 0 obeying
    any relevant deceleration limits.  Once the current speed reaches 0, the
    motor will attempt to brake or coast as specified by this command, but
    due to hardware limitations it might not be able to.

    The motor number parameter should be between 1 and the number of motors
    supported by the Motoron.

    The amount parameter gets stored in the "Target brake amount" variable
    for the motor and should be between 0 (coasting) and 800 (braking).
    Values above 800 will be clipped to 800 by the Motoron firmware.

    See the "Set braking" command in the Motoron user's guide for more
    information.

    @sa set_braking_now(), get_target_brake_amount()
    """
    cmd = [
      CMD_SET_BRAKING,
      motor & 0x7F,
      amount & 0x7F,
      (amount >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def set_braking_now(self, motor, amount):
    """
    Commands the motor to brake, coast, or something in between.

    Sending this command causes the motor's current speed to change to 0.
    The motor will attempt to brake or coast as specified by this command,
    but due to hardware limitations it might not be able to.

    The motor number parameter should be between 1 and the number of motors
    supported by the Motoron.

    The amount parameter gets stored in the "Target brake amount" variable
    for the motor and should be between 0 (coasting) and 800 (braking).
    Values above 800 will be clipped to 800 by the Motoron firmware.

    See the "Set braking" command in the Motoron user's guide for more
    information.

    @sa set_braking(), get_target_brake_amount()
    """
    cmd = [
      CMD_SET_BRAKING_NOW,
      motor & 0x7F,
      amount & 0x7F,
      (amount >> 7) & 0x7F,
    ]
    self._send_command(cmd)

  def reset_command_timeout(self):
    """
    Resets the command timeout.

    This prevents the command timeout status flags from getting set for some
    time.  (The command timeout is also reset by every other Motoron command,
    as long as its parameters are valid.)

    For more information, see the "Reset command timeout" command in the
    Motoron user's guide.

    @sa disable_command_timeout(), set_command_timeout_milliseconds()
    """
    cmd = [CMD_RESET_COMMAND_TIMEOUT]
    self._send_command(cmd)

  def _send_command(self, cmd):
    send_crc = bool(self.protocol_options & (1 << PROTOCOL_OPTION_CRC_FOR_COMMANDS))
    self._send_command_core(cmd, send_crc)

  def _send_command_and_read_response(self, cmd, response_length):
    self._send_command(cmd)
    return self._read_response(response_length)

def calculate_current_limit(milliamps, type, reference_mv, offset):
  """
  Calculates a current limit value that can be passed to the Motoron
  using set_current_limit().

  @param milliamps The desired current limit, in units of mA.
  @param type Specifies what type of Motoron you are using.  This should be one
    of the members of the motoron.CurrentSenseType enum.
  @param reference_mv The reference voltage (IOREF), in millivolts.
    For example, use 3300 for a 3.3 V system or 5000 for a 5 V system.
  @param offset The offset of the raw current sense signal for the Motoron
    channel.  This is the same measurement that you would put into the
    Motoron's "Current sense offset" variable using set_current_sense_offset(),
    so see the documentation of that function for more info.
    The offset is typically 10 for 5 V systems and 15 for 3.3 V systems,
    (50*1024/reference_mv) but it can vary widely.
  """
  if milliamps > 1000000: milliamps = 1000000
  limit = offset * 125 / 128 + milliamps * 20 / (reference_mv * (enum_value(type) & 3))
  if limit > 1000: limit = 1000
  return math.floor(limit)

def current_sense_units_milliamps(type, reference_mv):
  """
  Calculates the units for the Motoron's current sense reading returned by
  get_current_sense_processed(), in milliamps.

  To convert a reading from get_current_sense_processed() to milliamps
  multiply it by the value returned from this function.

  @param type Specifies what type of Motoron you are using.  This should be one
    of the members of the motoron.CurrentSenseType enum.
  @param reference_mv The reference voltage (IOREF), in millivolts.
    For example, use 3300 for a 3.3 V system or 5000 for a 5 V system.
  """
  return reference_mv * (enum_value(type) & 3) * 25 / 512

class MotoronI2C(MotoronBase):
  """
  Represents an I2C connection to a Pololu Motoron Motor Controller.
  """

  def __init__(self, *, bus=1, address=16):
    """
    Creates a new MotoronI2C object to communicate with the Motoron over I2C.

    @param bus Optional argument that specifies which I2C bus to use.
      This can be an integer, an SMBus object from the smbus2 package, or an
      object with an interface similar to SMBus.
      The default bus is 1, which corresponds to `/dev/i2c-1`.
    @param address Optional argument that specifies the 7-bit I2C address to
      use.  This must match the address that the Motoron is configured to use.
      The default address is 16.
    """
    super().__init__()

    self.set_bus(bus)

    ## The 7-bit I2C address used by this object. The default is 16.
    self.address = address

    """
    Configures this object to use the specified I2C bus object.

    The bus argument should be one of the following:
    - The number of an I2C bus to open with smbus2
      (e.g. 2 for `/dev/i2c-2`)
    - An SMBus object from smbus2.
    - A machine.I2C object from MicroPython.
    """
  def set_bus(self, bus):
    if isinstance(bus, int):
      import smbus2
      bus = smbus2.SMBus(bus)

    try:
      bus.i2c_rdwr
      type_is_smbus = True
    except AttributeError:
      type_is_smbus = False

    if type_is_smbus:
      self._send_command_core = self._smbus_send_command_core
      self._read_response = self._smbus_read_response
      import smbus2
      self._msg = smbus2.i2c_msg
    else:
      self._send_command_core = self._mpy_send_command_core
      self._read_response = self._mpy_read_response

    self.bus = bus

  def _smbus_send_command_core(self, cmd, send_crc):
    if send_crc:
      write = self._msg.write(self.address, cmd + [calculate_crc(cmd)])
    else:
      write = self._msg.write(self.address, cmd)
    self.bus.i2c_rdwr(write)

  def _smbus_read_response(self, length):
    # On some Raspberry Pis with buggy implementations of I2C clock stretching,
    # sleeping for 0.5 ms might be necessary in order to give the Motoron time
    # to prepare its response, so it does not need to stretch the clock.
    time.sleep(0.0005)

    crc_enabled = bool(self.protocol_options & (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))
    read = self._msg.read(self.address, length + crc_enabled)
    self.bus.i2c_rdwr(read)
    response = bytes(read)
    if crc_enabled:
      crc = response[-1]
      response = response[:-1]
      if crc != calculate_crc(response):
        raise RuntimeError('Incorrect CRC received.')
    return response

  def _mpy_send_command_core(self, cmd, send_crc):
    if send_crc:
      self.bus.writeto(self.address, bytes(cmd + [calculate_crc(cmd)]))
    else:
      self.bus.writeto(self.address, bytes(cmd))

  def _mpy_read_response(self, length):
    crc_enabled = bool(self.protocol_options & (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))
    response = self.bus.readfrom(self.address, length + crc_enabled)
    if crc_enabled:
      crc = response[-1]
      response = response[:-1]
      if crc != calculate_crc(response):
        raise RuntimeError('Incorrect CRC received.')
    return response


class MotoronSerial(MotoronBase):
  """
  Represents a serial connection to a Pololu Motoron Motor Controller.
  """

  def __init__(self, *, port=None, device_number=None):
    """
    Creates a new MotoronSerial object.

    The `deviceNumber` argument is optional.  If it is omitted or None,
    the object will use the compact protocol.

    The `port` argument specifies the serial port to use and is passed
    directly to set_port().
    """
    super().__init__()

    self.set_port(port)

    ## The device number that will be included in commands sent by this object.
    ## The default is None, which means to not send a device number and use the
    ## compact protocol instead.
    self.device_number = device_number

    ## The serial options used by this object.  This must match the serial
    ## options in the EEPROM of the Motoron you are communicating with.
    ## The default is 7-bit device numbers and 8-bit responses.
    ##
    ## The bits in this variable are defined by the
    ## motoron.COMMUNICATION_OPTION_* constants.  The bits that affect the
    ## behavior of this library are:
    ## - motoron.COMMUNICATION_OPTION_7BIT_RESPONSES
    ## - motoron.COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER
    self.communication_options = 0

  def set_port(self, port):
    """
    Configures this object to use the specified serial port object.

    The port argument should be one of the following:
    - The name of a serial port to open with pyserial
      (e.g. "COM6" or "/dev/ttyS0")
    - A Serial object from pyserial.
    - A machine.UART object from MicroPython.
    """
    if isinstance(port, str):
      import serial
      self.port = serial.Serial(port, 115200, timeout=0.1, write_timeout=0.1)
    else:
      ## The serial port used by this object.  See set_port().
      self.port = port

  def expect_7bit_responses(self):
    """
    Configures this object to work with Motorons that are configured to send
    7-bit serial responses.
    """
    self.communication_options |= (1 << COMMUNICATION_OPTION_7BIT_RESPONSES)

  def expect_8bit_responses(self):
    """
    Configures this object to work with Motorons that are configured to send
    responses in the normal 8-bit format.
    """
    self.communication_options &= ~(1 << COMMUNICATION_OPTION_7BIT_RESPONSES)

  def use_14bit_device_number(self):
    """
    Configures this object to send 14-bit device numbers when using the
    Pololu protocol, instead of the default 7-bit.
    """
    self.communication_options |= (1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER)

  def use_7bit_device_number(self):
    """
    Configures this object to send 7-bit device numbers, which is the default.
    """
    self.communication_options &= ~(1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER)

  def multi_device_error_check_start(self, starting_device_number, device_count):
    """
    Sends a "Multi-device error check" command but does not read any
    responses.

    Note: Before using this, most users should make sure the MotoronSerial
    object is configured to use the compact protocol: construct the object
    without specifying a device number, or set device_number to None.
    """
    if self.communication_options & (1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER):
      if device_count < 0 or device_count > 0x3FFF:
        raise RuntimeError('Invalid device count.')
      cmd = [
        CMD_MULTI_DEVICE_ERROR_CHECK,
        starting_device_number & 0x7F,
        starting_device_number >> 7 & 0x7F,
        device_count & 0x7F,
        device_count >> 7 & 0x7F,
      ]
    else:
      if device_count < 0 or device_count > 0x7F:
        raise RuntimeError('Invalid device count.')
      cmd = [
        CMD_MULTI_DEVICE_ERROR_CHECK,
        starting_device_number & 0x7F,
        device_count,
      ]

    self._send_command(cmd)
    self.port.flush()

  def multi_device_error_check(self, starting_device_number, device_count):
    """
    Sends a "Multi-device error check" command and reads the responses.

    This function assumes that each addressed Motoron can see the responses
    sent by the other Motorons (e.g. they are in a half-duplex RS-485 network).

    Returns the number of devices that indicated they have no errors.
    If the return value is less than device count, you can add the return
    value to the starting_device_number to get the device number of the
    first device where the check failed.  This device either did not
    respond or it responded with an indication that it has an error, or an
    unexpected byte was received for some reason.

    Note: Before using this, most users should make sure the MotoronSerial
    object is configured to use the compact protocol: construct the object
    without specifying a device number, or set device_number to None.
    """
    self.multi_device_error_check_start(starting_device_number, device_count)
    responses = self.port.read(device_count)
    for i, v in enumerate(responses):
      if v != ERROR_CHECK_CONTINUE: return i
    return len(responses)

  def multi_device_write(self, starting_device_number, device_count,
    command_byte, data):
    """
    Sends a "Multi-device write" command.

    Note: Before using this, most users should make sure the MotoronSerial
    object is configured to use the compact protocol: construct the object
    without specifying a device number, or call setDeviceNumber with an
    argument of 0xFFFF.
    """

    if bool(self.communication_options & (1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER)):
      if device_count < 0 or device_count > 0x3FFF:
        raise RuntimeError('Invalid device count.')
      cmd = [
        CMD_MULTI_DEVICE_WRITE,
        starting_device_number & 0x7F,
        starting_device_number >> 7 & 0x7F,
        device_count & 0x7F,
        device_count >> 7 & 0x7F,
      ]
    else:
      if device_count < 0 or device_count > 0x7F:
        raise RuntimeError('Invalid device count.')
      cmd = [
        CMD_MULTI_DEVICE_WRITE,
        starting_device_number & 0x7F,
        device_count,
      ]

    if data == None: data = []
    if len(data) % device_count:
      raise RuntimeError("Expected data length to be a multiple of " \
        f"{device_count}, got {len(data)}.")
    bytes_per_device = len(data) // device_count
    if bytes_per_device > 15: raise RuntimeError('Data too long.')

    cmd += [bytes_per_device, command_byte & 0x7F]
    cmd += data

    self._send_command(cmd)

  def _send_command_core(self, cmd, send_crc):
    if self.device_number != None:
      if self.communication_options & (1 << COMMUNICATION_OPTION_14BIT_DEVICE_NUMBER):
        cmd = [
          0xAA,
          self.device_number & 0x7F,
          self.device_number >> 7 & 0x7F,
          cmd[0] & 0x7F
        ] + cmd[1:]
      else:
        cmd = [
          0xAA,
          self.device_number & 0x7F,
          cmd[0] & 0x7F
        ] + cmd[1:]

    if send_crc: cmd += [calculate_crc(cmd)]

    self.port.write(bytes(cmd))

  def _read_response(self, length):
    crc_enabled = bool(self.protocol_options & (1 << PROTOCOL_OPTION_CRC_FOR_RESPONSES))
    response_7bit = bool(self.communication_options & (1 << COMMUNICATION_OPTION_7BIT_RESPONSES))

    if response_7bit and length > 7:
      raise RuntimeError('The Motoron does not support response payloads ' \
        'longer than 7 bytes in 7-bit response mode.')

    self.port.flush()
    read_length = length + response_7bit + crc_enabled
    response = self.port.read(read_length)
    if response is None: response = b''
    if len(response) != read_length:
      raise RuntimeError(f"Expected to read {read_length} bytes, got {len(response)}.")

    if crc_enabled:
      crc = response[-1]
      response = response[:-1]
      if crc != calculate_crc(response):
        raise RuntimeError('Incorrect CRC received.')

    if response_7bit:
      msbs = response[-1]
      response = bytearray(response[:-1])
      for i in range(length):
        if msbs & 1: response[i] |= 0x80
        msbs >>= 1
      response = bytes(response)

    return response


